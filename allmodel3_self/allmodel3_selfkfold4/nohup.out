WARNING:tensorflow:From all_three_lstm.py:92: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:94: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:95: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From all_three_lstm.py:95: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From all_three_lstm.py:97: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From all_three_lstm.py:98: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
loading data...
train: 6417 test: 1636
load data time: 194.24818325042725
Start train!
Training iter #400:   Batch Loss = 26.762325, Accuracy = 0.10999999940395355
PERFORMANCE ON TEST SET: Batch Loss = 26.328044891357422, Accuracy = 0.15464547276496887
Training iter #2000:   Batch Loss = 25.280451, Accuracy = 0.3174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 24.937955856323242, Accuracy = 0.3270171284675598
Training iter #4000:   Batch Loss = 23.573935, Accuracy = 0.4074999988079071
PERFORMANCE ON TEST SET: Batch Loss = 23.22935676574707, Accuracy = 0.41014671325683594
Training iter #6000:   Batch Loss = 21.799427, Accuracy = 0.4050000011920929
PERFORMANCE ON TEST SET: Batch Loss = 21.585182189941406, Accuracy = 0.4352078139781952
Training iter #8000:   Batch Loss = 20.803677, Accuracy = 0.48750001192092896
PERFORMANCE ON TEST SET: Batch Loss = 20.562475204467773, Accuracy = 0.48471882939338684
Training iter #10000:   Batch Loss = 19.191811, Accuracy = 0.5649999976158142
PERFORMANCE ON TEST SET: Batch Loss = 19.368770599365234, Accuracy = 0.5458435416221619
Training iter #12000:   Batch Loss = 18.430759, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 18.490158081054688, Accuracy = 0.5971882343292236
Training iter #14000:   Batch Loss = 17.639078, Accuracy = 0.6225000023841858
PERFORMANCE ON TEST SET: Batch Loss = 17.885204315185547, Accuracy = 0.604523241519928
Training iter #16000:   Batch Loss = 17.848667, Accuracy = 0.5774999856948853
PERFORMANCE ON TEST SET: Batch Loss = 17.299816131591797, Accuracy = 0.6088019609451294
Training iter #18000:   Batch Loss = 16.509830, Accuracy = 0.6524999737739563
PERFORMANCE ON TEST SET: Batch Loss = 16.507888793945312, Accuracy = 0.6497554779052734
Training iter #20000:   Batch Loss = 15.606103, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 16.164688110351562, Accuracy = 0.6546455025672913
Training iter #22000:   Batch Loss = 15.948130, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 16.11054801940918, Accuracy = 0.6619804501533508
Training iter #24000:   Batch Loss = 15.848965, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 15.502188682556152, Accuracy = 0.6729828715324402
Training iter #26000:   Batch Loss = 15.215677, Accuracy = 0.6949999928474426
PERFORMANCE ON TEST SET: Batch Loss = 15.17109489440918, Accuracy = 0.6900978088378906
Training iter #28000:   Batch Loss = 14.734803, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 15.118448257446289, Accuracy = 0.6852078437805176
Training iter #30000:   Batch Loss = 14.796499, Accuracy = 0.7124999761581421
PERFORMANCE ON TEST SET: Batch Loss = 14.867565155029297, Accuracy = 0.6858190894126892
Training iter #32000:   Batch Loss = 14.430565, Accuracy = 0.6899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 14.626888275146484, Accuracy = 0.7017114758491516
Training iter #34000:   Batch Loss = 14.062129, Accuracy = 0.6470588445663452
PERFORMANCE ON TEST SET: Batch Loss = 14.420217514038086, Accuracy = 0.70110023021698
Training iter #36000:   Batch Loss = 13.756964, Accuracy = 0.6949999928474426
PERFORMANCE ON TEST SET: Batch Loss = 14.143674850463867, Accuracy = 0.7029339671134949
Training iter #38000:   Batch Loss = 13.271673, Accuracy = 0.7450000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.864831924438477, Accuracy = 0.7212713956832886
Training iter #40000:   Batch Loss = 13.171120, Accuracy = 0.75
PERFORMANCE ON TEST SET: Batch Loss = 13.731526374816895, Accuracy = 0.7353300452232361
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-100
Training iter #42000:   Batch Loss = 13.497439, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 14.03358268737793, Accuracy = 0.7114914655685425
Training iter #44000:   Batch Loss = 13.459018, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.71644401550293, Accuracy = 0.7194376587867737
Training iter #46000:   Batch Loss = 13.044778, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.450246810913086, Accuracy = 0.7316625714302063
Training iter #48000:   Batch Loss = 13.207197, Accuracy = 0.7275000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.369296073913574, Accuracy = 0.7334963083267212
Training iter #50000:   Batch Loss = 12.524283, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.286332130432129, Accuracy = 0.7365525960922241
Training iter #52000:   Batch Loss = 12.537417, Accuracy = 0.7875000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.943721771240234, Accuracy = 0.7512224912643433
Training iter #54000:   Batch Loss = 12.682392, Accuracy = 0.7749999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.986565589904785, Accuracy = 0.75
Training iter #56000:   Batch Loss = 12.553777, Accuracy = 0.7875000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.059327125549316, Accuracy = 0.7524449825286865
Training iter #58000:   Batch Loss = 12.512335, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.02681827545166, Accuracy = 0.7493887543678284
Training iter #60000:   Batch Loss = 13.344604, Accuracy = 0.7724999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.273244857788086, Accuracy = 0.7597799301147461
Training iter #62000:   Batch Loss = 12.385975, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 13.370046615600586, Accuracy = 0.7463325262069702
Training iter #64000:   Batch Loss = 12.224907, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.299020767211914, Accuracy = 0.7542787194252014
Training iter #66000:   Batch Loss = 12.743303, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.94780158996582, Accuracy = 0.7707824110984802
Training iter #68000:   Batch Loss = 14.071615, Accuracy = 0.6470588445663452
PERFORMANCE ON TEST SET: Batch Loss = 13.296358108520508, Accuracy = 0.7707824110984802
Training iter #70000:   Batch Loss = 12.701655, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.228389739990234, Accuracy = 0.761613667011261
Training iter #72000:   Batch Loss = 12.335629, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.942251205444336, Accuracy = 0.7652812004089355
Training iter #74000:   Batch Loss = 12.146797, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.704574584960938, Accuracy = 0.7762836217880249
Training iter #76000:   Batch Loss = 13.063261, Accuracy = 0.7774999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.081535339355469, Accuracy = 0.7713936567306519
Training iter #78000:   Batch Loss = 12.184602, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.783025741577148, Accuracy = 0.769559919834137
Training iter #80000:   Batch Loss = 11.902878, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.627774238586426, Accuracy = 0.7732273936271667
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-200
Training iter #82000:   Batch Loss = 12.090951, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.852116584777832, Accuracy = 0.7481662631034851
Training iter #84000:   Batch Loss = 11.932911, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 12.901931762695312, Accuracy = 0.7756723761558533
Training iter #86000:   Batch Loss = 12.428253, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.622943878173828, Accuracy = 0.7817848324775696
Training iter #88000:   Batch Loss = 11.870451, Accuracy = 0.8025000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.315227508544922, Accuracy = 0.7891197800636292
Training iter #90000:   Batch Loss = 11.736079, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.5137300491333, Accuracy = 0.7836185693740845
Training iter #92000:   Batch Loss = 11.713163, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.249088287353516, Accuracy = 0.7793398499488831
Training iter #94000:   Batch Loss = 11.764663, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.201951026916504, Accuracy = 0.7866747975349426
Training iter #96000:   Batch Loss = 11.884942, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.150793075561523, Accuracy = 0.794009804725647
Training iter #98000:   Batch Loss = 11.408875, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.92344856262207, Accuracy = 0.7897310256958008
Training iter #100000:   Batch Loss = 11.308652, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.921010971069336, Accuracy = 0.8007335066795349
Training iter #102000:   Batch Loss = 12.162589, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 12.022064208984375, Accuracy = 0.805623471736908
Training iter #104000:   Batch Loss = 11.759706, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.504558563232422, Accuracy = 0.7891197800636292
Training iter #106000:   Batch Loss = 11.398254, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.04728889465332, Accuracy = 0.7927873134613037
Training iter #108000:   Batch Loss = 11.190395, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.85344123840332, Accuracy = 0.7976772785186768
Training iter #110000:   Batch Loss = 11.296888, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.000988960266113, Accuracy = 0.7982885241508484
Training iter #112000:   Batch Loss = 11.076889, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.736666679382324, Accuracy = 0.7903422713279724
Training iter #114000:   Batch Loss = 10.689256, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.522591590881348, Accuracy = 0.8068459630012512
Training iter #116000:   Batch Loss = 11.120483, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.724505424499512, Accuracy = 0.790953516960144
Training iter #118000:   Batch Loss = 11.427248, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.422201156616211, Accuracy = 0.790953516960144
Training iter #120000:   Batch Loss = 11.662854, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.19692325592041, Accuracy = 0.7933985590934753
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-300
Training iter #122000:   Batch Loss = 11.292574, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.833317756652832, Accuracy = 0.8001222610473633
Training iter #124000:   Batch Loss = 11.600484, Accuracy = 0.8100000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.158791542053223, Accuracy = 0.7952322959899902
Training iter #126000:   Batch Loss = 11.145450, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.816390991210938, Accuracy = 0.794009804725647
Training iter #128000:   Batch Loss = 10.990662, Accuracy = 0.8349999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.703168869018555, Accuracy = 0.79889976978302
Training iter #130000:   Batch Loss = 11.975733, Accuracy = 0.8324999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.241785049438477, Accuracy = 0.7903422713279724
Training iter #132000:   Batch Loss = 11.288693, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.781118392944336, Accuracy = 0.7982885241508484
Training iter #134000:   Batch Loss = 10.893307, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.624086380004883, Accuracy = 0.8001222610473633
Training iter #136000:   Batch Loss = 10.265726, Accuracy = 0.7058823704719543
PERFORMANCE ON TEST SET: Batch Loss = 11.612838745117188, Accuracy = 0.8062347173690796
Training iter #138000:   Batch Loss = 11.346220, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.724175453186035, Accuracy = 0.817237138748169
Training iter #140000:   Batch Loss = 11.046996, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.023505210876465, Accuracy = 0.8062347173690796
Training iter #142000:   Batch Loss = 10.904070, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.744166374206543, Accuracy = 0.8099021911621094
Training iter #144000:   Batch Loss = 11.261935, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.755539894104004, Accuracy = 0.8068459630012512
Training iter #146000:   Batch Loss = 10.323431, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.592693328857422, Accuracy = 0.815403401851654
Training iter #148000:   Batch Loss = 10.517231, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.500319480895996, Accuracy = 0.8160146474838257
Training iter #150000:   Batch Loss = 10.700594, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.627017974853516, Accuracy = 0.8123471736907959
Training iter #152000:   Batch Loss = 10.994146, Accuracy = 0.8349999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.396403312683105, Accuracy = 0.8123471736907959
Training iter #154000:   Batch Loss = 10.631899, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.503059387207031, Accuracy = 0.8196821808815002
Training iter #156000:   Batch Loss = 10.700657, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.606117248535156, Accuracy = 0.8190708756446838
Training iter #158000:   Batch Loss = 10.738750, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.58930778503418, Accuracy = 0.8215159177780151
Training iter #160000:   Batch Loss = 10.761724, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.342554092407227, Accuracy = 0.8202934265136719
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-400
Training iter #162000:   Batch Loss = 10.570096, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.192184448242188, Accuracy = 0.8251833915710449
Training iter #164000:   Batch Loss = 10.116197, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.332613945007324, Accuracy = 0.8141809105873108
Training iter #166000:   Batch Loss = 10.638670, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.165553092956543, Accuracy = 0.8123471736907959
Training iter #168000:   Batch Loss = 10.660857, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.014877319335938, Accuracy = 0.8245721459388733
Training iter #170000:   Batch Loss = 9.421984, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.817007064819336, Accuracy = 0.8270171284675598
Training iter #172000:   Batch Loss = 10.804205, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.538277626037598, Accuracy = 0.8270171284675598
Training iter #174000:   Batch Loss = 11.390995, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.675773620605469, Accuracy = 0.8264058828353882
Training iter #176000:   Batch Loss = 11.460411, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.52271556854248, Accuracy = 0.8264058828353882
Training iter #178000:   Batch Loss = 11.757281, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.006409645080566, Accuracy = 0.8239609003067017
Training iter #180000:   Batch Loss = 13.961159, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 14.588067054748535, Accuracy = 0.7762836217880249
Training iter #182000:   Batch Loss = 13.154222, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.832418441772461, Accuracy = 0.815403401851654
Training iter #184000:   Batch Loss = 12.247183, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.68256950378418, Accuracy = 0.8044009804725647
Training iter #186000:   Batch Loss = 12.928879, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.230464935302734, Accuracy = 0.8019559979438782
Training iter #188000:   Batch Loss = 12.445862, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.8070707321167, Accuracy = 0.8129584193229675
Training iter #190000:   Batch Loss = 11.846964, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.467418670654297, Accuracy = 0.8209046721458435
Training iter #192000:   Batch Loss = 12.008942, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.5446195602417, Accuracy = 0.8141809105873108
Training iter #194000:   Batch Loss = 11.637112, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.307601928710938, Accuracy = 0.8135696649551392
Training iter #196000:   Batch Loss = 11.288067, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.126385688781738, Accuracy = 0.8190708756446838
Training iter #198000:   Batch Loss = 11.135626, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.134604454040527, Accuracy = 0.8257946372032166
Training iter #200000:   Batch Loss = 11.625941, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.158113479614258, Accuracy = 0.8190708756446838
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-500
Training iter #202000:   Batch Loss = 11.462973, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.218016624450684, Accuracy = 0.8092909455299377
Training iter #204000:   Batch Loss = 8.998932, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 12.066287994384766, Accuracy = 0.8190708756446838
Training iter #206000:   Batch Loss = 11.548998, Accuracy = 0.8349999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.95291805267334, Accuracy = 0.8141809105873108
Training iter #208000:   Batch Loss = 10.725578, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.839204788208008, Accuracy = 0.8264058828353882
Training iter #210000:   Batch Loss = 11.124683, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.886384963989258, Accuracy = 0.8111246824264526
Training iter #212000:   Batch Loss = 10.994698, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.260501861572266, Accuracy = 0.8141809105873108
Training iter #214000:   Batch Loss = 11.618494, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.0756196975708, Accuracy = 0.8239609003067017
Training iter #216000:   Batch Loss = 11.583266, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.015958786010742, Accuracy = 0.8190708756446838
Training iter #218000:   Batch Loss = 11.548747, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.501092910766602, Accuracy = 0.8160146474838257
Training iter #220000:   Batch Loss = 11.107326, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.064132690429688, Accuracy = 0.8227384090423584
Training iter #222000:   Batch Loss = 10.953802, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.765913009643555, Accuracy = 0.8209046721458435
Training iter #224000:   Batch Loss = 11.256577, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.689123153686523, Accuracy = 0.8276283740997314
Training iter #226000:   Batch Loss = 10.431873, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.93067455291748, Accuracy = 0.8184596300125122
Training iter #228000:   Batch Loss = 10.685544, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.779230117797852, Accuracy = 0.8245721459388733
Training iter #230000:   Batch Loss = 10.564812, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.466506004333496, Accuracy = 0.8398532867431641
Training iter #232000:   Batch Loss = 11.043108, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.772821426391602, Accuracy = 0.8325183391571045
Training iter #234000:   Batch Loss = 10.708670, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.617395401000977, Accuracy = 0.8221271634101868
Training iter #236000:   Batch Loss = 10.119459, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.437750816345215, Accuracy = 0.8331295847892761
Training iter #238000:   Batch Loss = 8.586218, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.498544692993164, Accuracy = 0.8282396197319031
Training iter #240000:   Batch Loss = 10.289706, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.443634033203125, Accuracy = 0.8312958478927612
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-600
Training iter #242000:   Batch Loss = 10.114155, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.211050987243652, Accuracy = 0.8380195498466492
Training iter #244000:   Batch Loss = 9.870908, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.332316398620605, Accuracy = 0.8398532867431641
Training iter #246000:   Batch Loss = 10.806686, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.402498245239258, Accuracy = 0.8361858129501343
Training iter #248000:   Batch Loss = 10.389746, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.345076560974121, Accuracy = 0.8404645323753357
Training iter #250000:   Batch Loss = 10.147800, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.169737815856934, Accuracy = 0.8453544974327087
Training iter #252000:   Batch Loss = 10.210298, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.844486236572266, Accuracy = 0.8178483843803406
Training iter #254000:   Batch Loss = 10.565701, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.874896049499512, Accuracy = 0.8215159177780151
Training iter #256000:   Batch Loss = 10.995637, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.49109172821045, Accuracy = 0.8447432518005371
Training iter #258000:   Batch Loss = 10.898472, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.458094596862793, Accuracy = 0.8514670133590698
Training iter #260000:   Batch Loss = 10.644438, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.587156295776367, Accuracy = 0.8325183391571045
Training iter #262000:   Batch Loss = 10.771046, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.488709449768066, Accuracy = 0.8367970585823059
Training iter #264000:   Batch Loss = 10.041376, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.377564430236816, Accuracy = 0.8337408304214478
Training iter #266000:   Batch Loss = 10.190736, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.518623352050781, Accuracy = 0.8380195498466492
Training iter #268000:   Batch Loss = 10.093358, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.321660995483398, Accuracy = 0.8355745673179626
Training iter #270000:   Batch Loss = 10.498308, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.215420722961426, Accuracy = 0.8355745673179626
Training iter #272000:   Batch Loss = 10.099354, Accuracy = 0.8235294222831726
PERFORMANCE ON TEST SET: Batch Loss = 11.222558975219727, Accuracy = 0.8386307954788208
Training iter #274000:   Batch Loss = 9.486489, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.141956329345703, Accuracy = 0.8435207605361938
Training iter #276000:   Batch Loss = 10.508454, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.181474685668945, Accuracy = 0.8435207605361938
Training iter #278000:   Batch Loss = 10.175177, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.122196197509766, Accuracy = 0.8392420411109924
Training iter #280000:   Batch Loss = 10.083981, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.010568618774414, Accuracy = 0.8386307954788208
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-700
Training iter #282000:   Batch Loss = 9.524439, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.95711612701416, Accuracy = 0.8441320061683655
Training iter #284000:   Batch Loss = 10.236834, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.998944282531738, Accuracy = 0.8447432518005371
Training iter #286000:   Batch Loss = 9.962083, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.466553688049316, Accuracy = 0.8398532867431641
Training iter #288000:   Batch Loss = 10.078562, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.320003509521484, Accuracy = 0.8447432518005371
Training iter #290000:   Batch Loss = 10.000636, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.254639625549316, Accuracy = 0.8355745673179626
Training iter #292000:   Batch Loss = 10.026713, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.765493392944336, Accuracy = 0.8581907153129578
Training iter #294000:   Batch Loss = 10.043362, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.994454383850098, Accuracy = 0.8404645323753357
Training iter #296000:   Batch Loss = 10.328909, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.95927619934082, Accuracy = 0.854523241519928
Training iter #298000:   Batch Loss = 10.026600, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.831252098083496, Accuracy = 0.8508557677268982
Training iter #300000:   Batch Loss = 9.906764, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.317930221557617, Accuracy = 0.8374083042144775
Training iter #302000:   Batch Loss = 10.103817, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.115708351135254, Accuracy = 0.8410757780075073
Training iter #304000:   Batch Loss = 9.788399, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.800118446350098, Accuracy = 0.8361858129501343
Training iter #306000:   Batch Loss = 10.897901, Accuracy = 0.8235294222831726
PERFORMANCE ON TEST SET: Batch Loss = 10.955421447753906, Accuracy = 0.830073356628418
Training iter #308000:   Batch Loss = 9.911707, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.006089210510254, Accuracy = 0.8435207605361938
Training iter #310000:   Batch Loss = 9.231400, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.807358741760254, Accuracy = 0.8563569784164429
Training iter #312000:   Batch Loss = 10.022589, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.720059394836426, Accuracy = 0.8496332764625549
Training iter #314000:   Batch Loss = 9.822556, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.009578704833984, Accuracy = 0.8502445220947266
Training iter #316000:   Batch Loss = 9.783102, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.85995101928711, Accuracy = 0.8508557677268982
Training iter #318000:   Batch Loss = 9.949632, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.774274826049805, Accuracy = 0.854523241519928
Training iter #320000:   Batch Loss = 9.502095, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.110702514648438, Accuracy = 0.8551344871520996
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-800
Training iter #322000:   Batch Loss = 9.761486, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.862215042114258, Accuracy = 0.8508557677268982
Training iter #324000:   Batch Loss = 9.590477, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.726949691772461, Accuracy = 0.8520782589912415
Training iter #326000:   Batch Loss = 9.842851, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.530562400817871, Accuracy = 0.8575794696807861
Training iter #328000:   Batch Loss = 9.483265, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.58989143371582, Accuracy = 0.8588019609451294
Training iter #330000:   Batch Loss = 9.383080, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.511177062988281, Accuracy = 0.8557457327842712
Training iter #332000:   Batch Loss = 9.196305, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.56867790222168, Accuracy = 0.8618581891059875
Training iter #334000:   Batch Loss = 10.190491, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.224699020385742, Accuracy = 0.8429095149040222
Training iter #336000:   Batch Loss = 9.935119, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.848891258239746, Accuracy = 0.8502445220947266
Training iter #338000:   Batch Loss = 9.450499, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.709423065185547, Accuracy = 0.8643031716346741
Training iter #340000:   Batch Loss = 8.619093, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 10.725809097290039, Accuracy = 0.8526895046234131
Training iter #342000:   Batch Loss = 10.084462, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.8805513381958, Accuracy = 0.8471882343292236
Training iter #344000:   Batch Loss = 9.999091, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.551026344299316, Accuracy = 0.854523241519928
Training iter #346000:   Batch Loss = 9.558274, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.526506423950195, Accuracy = 0.8643031716346741
Training iter #348000:   Batch Loss = 9.297988, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.885231018066406, Accuracy = 0.8581907153129578
Training iter #350000:   Batch Loss = 9.858217, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.6763334274292, Accuracy = 0.8612469434738159
Training iter #352000:   Batch Loss = 9.764753, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.163833618164062, Accuracy = 0.8575794696807861
Training iter #354000:   Batch Loss = 10.154033, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.860658645629883, Accuracy = 0.8429095149040222
Training iter #356000:   Batch Loss = 10.760037, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.684272766113281, Accuracy = 0.8398532867431641
Training iter #358000:   Batch Loss = 11.011884, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.098337173461914, Accuracy = 0.8123471736907959
Training iter #360000:   Batch Loss = 11.859133, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 12.94306755065918, Accuracy = 0.8422982692718506
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-900
Training iter #362000:   Batch Loss = 11.713900, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.99899959564209, Accuracy = 0.8367970585823059
Training iter #364000:   Batch Loss = 12.053057, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.871886253356934, Accuracy = 0.8422982692718506
Training iter #366000:   Batch Loss = 12.692903, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.584061622619629, Accuracy = 0.8386307954788208
Training iter #368000:   Batch Loss = 11.918669, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.208807945251465, Accuracy = 0.8343520760536194
Training iter #370000:   Batch Loss = 11.988811, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.702218055725098, Accuracy = 0.8343520760536194
Training iter #372000:   Batch Loss = 11.080062, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.217700004577637, Accuracy = 0.8484107851982117
Training iter #374000:   Batch Loss = 13.517562, Accuracy = 0.8235294222831726
PERFORMANCE ON TEST SET: Batch Loss = 12.38613510131836, Accuracy = 0.8496332764625549
Training iter #376000:   Batch Loss = 11.416018, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.227275848388672, Accuracy = 0.8160146474838257
Training iter #378000:   Batch Loss = 11.898064, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.690373420715332, Accuracy = 0.8288508653640747
Training iter #380000:   Batch Loss = 11.981982, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.490803718566895, Accuracy = 0.8343520760536194
Training iter #382000:   Batch Loss = 11.504394, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.383983612060547, Accuracy = 0.8288508653640747
Training iter #384000:   Batch Loss = 10.952934, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.104229927062988, Accuracy = 0.8422982692718506
Training iter #386000:   Batch Loss = 11.343254, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.085670471191406, Accuracy = 0.8367970585823059
Training iter #388000:   Batch Loss = 11.052884, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.914024353027344, Accuracy = 0.8410757780075073
Training iter #390000:   Batch Loss = 10.952066, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.665142059326172, Accuracy = 0.8453544974327087
Training iter #392000:   Batch Loss = 10.617538, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.55806827545166, Accuracy = 0.8429095149040222
Training iter #394000:   Batch Loss = 10.714877, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.500810623168945, Accuracy = 0.846576988697052
Training iter #396000:   Batch Loss = 11.058668, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.260589599609375, Accuracy = 0.8343520760536194
Training iter #398000:   Batch Loss = 11.326181, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.95561695098877, Accuracy = 0.8422982692718506
Training iter #400000:   Batch Loss = 11.052169, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.84585189819336, Accuracy = 0.8447432518005371
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1000
Training iter #402000:   Batch Loss = 11.716876, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.265169143676758, Accuracy = 0.8355745673179626
Training iter #404000:   Batch Loss = 11.951682, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.8436279296875, Accuracy = 0.8453544974327087
Training iter #406000:   Batch Loss = 11.021420, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.31525993347168, Accuracy = 0.834963321685791
Training iter #408000:   Batch Loss = 11.690771, Accuracy = 0.7647058963775635
PERFORMANCE ON TEST SET: Batch Loss = 12.425602912902832, Accuracy = 0.84779953956604
Training iter #410000:   Batch Loss = 11.559494, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.15062141418457, Accuracy = 0.8404645323753357
Training iter #412000:   Batch Loss = 11.108831, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.957698822021484, Accuracy = 0.854523241519928
Training iter #414000:   Batch Loss = 10.875711, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.80223560333252, Accuracy = 0.8459657430648804
Training iter #416000:   Batch Loss = 11.209370, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.041969299316406, Accuracy = 0.834963321685791
Training iter #418000:   Batch Loss = 10.529587, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.03956127166748, Accuracy = 0.8441320061683655
Training iter #420000:   Batch Loss = 10.621851, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.728066444396973, Accuracy = 0.8471882343292236
Training iter #422000:   Batch Loss = 10.520973, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.712510108947754, Accuracy = 0.8374083042144775
Training iter #424000:   Batch Loss = 10.657795, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.770176887512207, Accuracy = 0.8441320061683655
Training iter #426000:   Batch Loss = 10.645950, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.539860725402832, Accuracy = 0.8471882343292236
Training iter #428000:   Batch Loss = 10.750668, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.163890838623047, Accuracy = 0.8447432518005371
Training iter #430000:   Batch Loss = 11.197044, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.089215278625488, Accuracy = 0.8533007502555847
Training iter #432000:   Batch Loss = 10.835660, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.806853294372559, Accuracy = 0.8404645323753357
Training iter #434000:   Batch Loss = 10.963190, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.654359817504883, Accuracy = 0.8514670133590698
Training iter #436000:   Batch Loss = 11.818809, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.219327926635742, Accuracy = 0.8441320061683655
Training iter #438000:   Batch Loss = 10.618016, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.06139850616455, Accuracy = 0.841687023639679
Training iter #440000:   Batch Loss = 10.600401, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.608222007751465, Accuracy = 0.8520782589912415
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1100
Training iter #442000:   Batch Loss = 9.384981, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.50295639038086, Accuracy = 0.846576988697052
Training iter #444000:   Batch Loss = 13.267668, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 14.747471809387207, Accuracy = 0.8264058828353882
Training iter #446000:   Batch Loss = 13.219611, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 14.676017761230469, Accuracy = 0.8319070935249329
Training iter #448000:   Batch Loss = 14.058785, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 14.836307525634766, Accuracy = 0.8502445220947266
Training iter #450000:   Batch Loss = 12.633361, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 14.103759765625, Accuracy = 0.8539119958877563
Training iter #452000:   Batch Loss = 12.793558, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.933427810668945, Accuracy = 0.8471882343292236
Training iter #454000:   Batch Loss = 12.443486, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.63983154296875, Accuracy = 0.8514670133590698
Training iter #456000:   Batch Loss = 12.941862, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.562814712524414, Accuracy = 0.854523241519928
Training iter #458000:   Batch Loss = 12.439798, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.369078636169434, Accuracy = 0.8533007502555847
Training iter #460000:   Batch Loss = 12.119479, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.233539581298828, Accuracy = 0.8588019609451294
Training iter #462000:   Batch Loss = 11.877300, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.043249130249023, Accuracy = 0.8588019609451294
Training iter #464000:   Batch Loss = 12.082130, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.085500717163086, Accuracy = 0.8453544974327087
Training iter #466000:   Batch Loss = 12.204994, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.824362754821777, Accuracy = 0.8630806803703308
Training iter #468000:   Batch Loss = 11.747978, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.731786727905273, Accuracy = 0.8490220308303833
Training iter #470000:   Batch Loss = 11.733504, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.21175765991211, Accuracy = 0.8459657430648804
Training iter #472000:   Batch Loss = 11.654880, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.9291410446167, Accuracy = 0.8551344871520996
Training iter #474000:   Batch Loss = 11.237995, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.645751953125, Accuracy = 0.8588019609451294
Training iter #476000:   Batch Loss = 11.388762, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 12.754974365234375, Accuracy = 0.8563569784164429
Training iter #478000:   Batch Loss = 11.703862, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.817255020141602, Accuracy = 0.8490220308303833
Training iter #480000:   Batch Loss = 11.777239, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.581610679626465, Accuracy = 0.8539119958877563
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1200
Training iter #482000:   Batch Loss = 11.895727, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.45298957824707, Accuracy = 0.8624694347381592
Training iter #484000:   Batch Loss = 11.303030, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.638405799865723, Accuracy = 0.8502445220947266
Training iter #486000:   Batch Loss = 13.124036, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 14.457488059997559, Accuracy = 0.8404645323753357WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.

Training iter #488000:   Batch Loss = 12.199099, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.45722770690918, Accuracy = 0.8361858129501343
Training iter #490000:   Batch Loss = 12.388443, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.374467849731445, Accuracy = 0.8520782589912415
Training iter #492000:   Batch Loss = 12.012287, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.142393112182617, Accuracy = 0.8490220308303833
Training iter #494000:   Batch Loss = 12.004465, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.140494346618652, Accuracy = 0.8539119958877563
Training iter #496000:   Batch Loss = 11.632866, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.944146156311035, Accuracy = 0.8618581891059875
Training iter #498000:   Batch Loss = 11.991529, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.189321517944336, Accuracy = 0.8520782589912415
Training iter #500000:   Batch Loss = 12.680910, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.769890785217285, Accuracy = 0.8447432518005371
Training iter #502000:   Batch Loss = 12.186260, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.274127006530762, Accuracy = 0.8496332764625549
Training iter #504000:   Batch Loss = 12.122978, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.339293479919434, Accuracy = 0.8557457327842712
Training iter #506000:   Batch Loss = 12.230518, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.020146369934082, Accuracy = 0.8563569784164429
Training iter #508000:   Batch Loss = 11.970582, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.807049751281738, Accuracy = 0.8453544974327087
Training iter #510000:   Batch Loss = 14.274367, Accuracy = 0.6470588445663452
PERFORMANCE ON TEST SET: Batch Loss = 13.266108512878418, Accuracy = 0.854523241519928
Training iter #512000:   Batch Loss = 12.142150, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.203566551208496, Accuracy = 0.8337408304214478
Training iter #514000:   Batch Loss = 11.877685, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.877620697021484, Accuracy = 0.846576988697052
Training iter #516000:   Batch Loss = 11.941057, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.666765213012695, Accuracy = 0.8484107851982117
Training iter #518000:   Batch Loss = 11.327874, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.903196334838867, Accuracy = 0.8520782589912415
Training iter #520000:   Batch Loss = 11.764933, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.54069709777832, Accuracy = 0.8374083042144775
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1300
Training iter #522000:   Batch Loss = 11.591934, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.47610092163086, Accuracy = 0.8490220308303833
Training iter #524000:   Batch Loss = 11.716759, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.616865158081055, Accuracy = 0.8343520760536194
Training iter #526000:   Batch Loss = 11.048041, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.483726501464844, Accuracy = 0.8435207605361938
Training iter #528000:   Batch Loss = 11.164331, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.268770217895508, Accuracy = 0.8520782589912415
Training iter #530000:   Batch Loss = 10.711389, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.288664817810059, Accuracy = 0.8551344871520996
Training iter #532000:   Batch Loss = 11.690096, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.139751434326172, Accuracy = 0.841687023639679
Training iter #534000:   Batch Loss = 12.014716, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.544290542602539, Accuracy = 0.84779953956604
Training iter #536000:   Batch Loss = 11.772058, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.372370719909668, Accuracy = 0.8508557677268982
Training iter #538000:   Batch Loss = 10.988166, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.424068450927734, Accuracy = 0.8484107851982117
Training iter #540000:   Batch Loss = 11.150507, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.151910781860352, Accuracy = 0.8539119958877563
Training iter #542000:   Batch Loss = 10.580607, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.18345832824707, Accuracy = 0.8422982692718506
Training iter #544000:   Batch Loss = 9.989295, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 12.284733772277832, Accuracy = 0.8508557677268982
Training iter #546000:   Batch Loss = 11.712968, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.474180221557617, Accuracy = 0.8435207605361938
Training iter #548000:   Batch Loss = 10.961999, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.173832893371582, Accuracy = 0.8490220308303833
Training iter #550000:   Batch Loss = 10.930046, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.942873001098633, Accuracy = 0.8575794696807861
Training iter #552000:   Batch Loss = 11.307051, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.504741668701172, Accuracy = 0.8496332764625549
Training iter #554000:   Batch Loss = 11.376119, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.390111923217773, Accuracy = 0.8514670133590698
Training iter #556000:   Batch Loss = 11.091702, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.18146800994873, Accuracy = 0.8557457327842712
Training iter #558000:   Batch Loss = 10.322462, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.105550765991211, Accuracy = 0.8557457327842712
Training iter #560000:   Batch Loss = 11.464244, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.164705276489258, Accuracy = 0.8520782589912415
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1400
Training iter #562000:   Batch Loss = 10.482344, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.979276657104492, Accuracy = 0.8502445220947266
Training iter #564000:   Batch Loss = 10.539833, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.185461044311523, Accuracy = 0.8588019609451294
Training iter #566000:   Batch Loss = 10.818390, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.21544075012207, Accuracy = 0.8563569784164429
Training iter #568000:   Batch Loss = 11.091090, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.154458999633789, Accuracy = 0.8471882343292236
Training iter #570000:   Batch Loss = 10.757746, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.11741828918457, Accuracy = 0.8490220308303833
Training iter #572000:   Batch Loss = 10.753096, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.060748100280762, Accuracy = 0.8471882343292236
Training iter #574000:   Batch Loss = 10.680475, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.196380615234375, Accuracy = 0.8526895046234131
Training iter #576000:   Batch Loss = 10.039614, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.876659393310547, Accuracy = 0.841687023639679
Training iter #578000:   Batch Loss = 14.445185, Accuracy = 0.7058823704719543
PERFORMANCE ON TEST SET: Batch Loss = 12.027809143066406, Accuracy = 0.8575794696807861
Training iter #580000:   Batch Loss = 11.284242, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.658156394958496, Accuracy = 0.8508557677268982
Training iter #582000:   Batch Loss = 11.338078, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.554037094116211, Accuracy = 0.8636919260025024
Training iter #584000:   Batch Loss = 11.083857, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.171302795410156, Accuracy = 0.8612469434738159
Training iter #586000:   Batch Loss = 12.122674, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.125829696655273, Accuracy = 0.8520782589912415
Training iter #588000:   Batch Loss = 11.441625, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.492328643798828, Accuracy = 0.84779953956604
Training iter #590000:   Batch Loss = 11.589739, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.371919631958008, Accuracy = 0.8459657430648804
Training iter #592000:   Batch Loss = 11.630945, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.410247802734375, Accuracy = 0.8459657430648804
Training iter #594000:   Batch Loss = 10.993506, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.209558486938477, Accuracy = 0.8533007502555847
Training iter #596000:   Batch Loss = 10.821993, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.964770317077637, Accuracy = 0.854523241519928
Training iter #598000:   Batch Loss = 10.695354, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.791145324707031, Accuracy = 0.866136908531189
Training iter #600000:   Batch Loss = 10.447512, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.787189483642578, Accuracy = 0.8618581891059875
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1500
Training iter #602000:   Batch Loss = 10.583655, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.88648796081543, Accuracy = 0.8649144172668457
Training iter #604000:   Batch Loss = 10.764471, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.877654075622559, Accuracy = 0.8606356978416443
Training iter #606000:   Batch Loss = 11.199048, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.546468734741211, Accuracy = 0.8643031716346741
Training iter #608000:   Batch Loss = 10.773487, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.385355949401855, Accuracy = 0.8624694347381592
Training iter #610000:   Batch Loss = 10.642542, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.313079833984375, Accuracy = 0.8618581891059875
Training iter #612000:   Batch Loss = 9.809896, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.972725868225098, Accuracy = 0.8679706454277039
Training iter #614000:   Batch Loss = 11.138020, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.062843322753906, Accuracy = 0.871026873588562
Training iter #616000:   Batch Loss = 10.547742, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.879670143127441, Accuracy = 0.8722493648529053
Training iter #618000:   Batch Loss = 10.676436, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.769582748413086, Accuracy = 0.8581907153129578
Training iter #620000:   Batch Loss = 11.044136, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.955899238586426, Accuracy = 0.8526895046234131
Training iter #622000:   Batch Loss = 10.431128, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.855598449707031, Accuracy = 0.8606356978416443
Training iter #624000:   Batch Loss = 10.288208, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.701916694641113, Accuracy = 0.8533007502555847
Training iter #626000:   Batch Loss = 10.484741, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.829562187194824, Accuracy = 0.8618581891059875
Training iter #628000:   Batch Loss = 10.544129, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.862871170043945, Accuracy = 0.8655256628990173
Training iter #630000:   Batch Loss = 9.975326, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.699860572814941, Accuracy = 0.871026873588562
Training iter #632000:   Batch Loss = 10.733817, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.760391235351562, Accuracy = 0.8667481541633606
Training iter #634000:   Batch Loss = 12.149328, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 14.005266189575195, Accuracy = 0.8410757780075073
Training iter #636000:   Batch Loss = 12.505966, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.630086898803711, Accuracy = 0.8539119958877563
Training iter #638000:   Batch Loss = 11.585942, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.88511848449707, Accuracy = 0.8600244522094727
Training iter #640000:   Batch Loss = 12.089119, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.960245132446289, Accuracy = 0.8447432518005371
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1600
Training iter #642000:   Batch Loss = 12.015580, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.02954387664795, Accuracy = 0.8575794696807861
Training iter #644000:   Batch Loss = 12.185906, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.505518913269043, Accuracy = 0.8520782589912415
Training iter #646000:   Batch Loss = 12.117987, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 13.66164493560791, Accuracy = 0.8649144172668457
Training iter #648000:   Batch Loss = 12.540631, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.640190124511719, Accuracy = 0.8496332764625549
Training iter #650000:   Batch Loss = 12.285240, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.64362621307373, Accuracy = 0.8575794696807861
Training iter #652000:   Batch Loss = 13.490856, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 14.497411727905273, Accuracy = 0.8520782589912415
Training iter #654000:   Batch Loss = 13.577676, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 14.157621383666992, Accuracy = 0.8514670133590698
Training iter #656000:   Batch Loss = 12.993486, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 14.128897666931152, Accuracy = 0.8526895046234131
Training iter #658000:   Batch Loss = 12.822134, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.928502082824707, Accuracy = 0.8624694347381592
Training iter #660000:   Batch Loss = 12.379315, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.839019775390625, Accuracy = 0.8575794696807861
Training iter #662000:   Batch Loss = 12.706208, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.485698699951172, Accuracy = 0.8563569784164429
Training iter #664000:   Batch Loss = 11.952353, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.289237976074219, Accuracy = 0.8667481541633606
Training iter #666000:   Batch Loss = 12.060028, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.10795783996582, Accuracy = 0.866136908531189
Training iter #668000:   Batch Loss = 12.328128, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.367950439453125, Accuracy = 0.8649144172668457
Training iter #670000:   Batch Loss = 12.308764, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.042394638061523, Accuracy = 0.8575794696807861
Training iter #672000:   Batch Loss = 11.788568, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.961986541748047, Accuracy = 0.8649144172668457
Training iter #674000:   Batch Loss = 12.954401, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 14.21920394897461, Accuracy = 0.84779953956604
Training iter #676000:   Batch Loss = 12.598662, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.572389602661133, Accuracy = 0.854523241519928
Training iter #678000:   Batch Loss = 12.226232, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.336226463317871, Accuracy = 0.8569682240486145
Training iter #680000:   Batch Loss = 13.228155, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 13.370335578918457, Accuracy = 0.8618581891059875
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1700
Training iter #682000:   Batch Loss = 11.927780, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.164365768432617, Accuracy = 0.8679706454277039
Training iter #684000:   Batch Loss = 12.541765, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.672711372375488, Accuracy = 0.8520782589912415
Training iter #686000:   Batch Loss = 14.197281, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 14.469886779785156, Accuracy = 0.8276283740997314
Training iter #688000:   Batch Loss = 14.038827, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 15.010298728942871, Accuracy = 0.8251833915710449
Training iter #690000:   Batch Loss = 13.606266, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 14.573338508605957, Accuracy = 0.82334965467453
Training iter #692000:   Batch Loss = 13.519611, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.095932960510254, Accuracy = 0.8215159177780151
Training iter #694000:   Batch Loss = 15.227068, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 16.214080810546875, Accuracy = 0.7946210503578186
Training iter #696000:   Batch Loss = 14.266822, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 15.400727272033691, Accuracy = 0.8202934265136719
Training iter #698000:   Batch Loss = 14.507054, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 14.92324447631836, Accuracy = 0.8227384090423584
Training iter #700000:   Batch Loss = 13.981243, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 14.472290992736816, Accuracy = 0.8227384090423584
Training iter #702000:   Batch Loss = 14.154136, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 15.00819206237793, Accuracy = 0.8141809105873108
Training iter #704000:   Batch Loss = 13.561088, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 14.39632797241211, Accuracy = 0.8270171284675598
Training iter #706000:   Batch Loss = 14.044544, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 14.35413932800293, Accuracy = 0.8343520760536194
Training iter #708000:   Batch Loss = 14.394104, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 15.141546249389648, Accuracy = 0.8080684542655945
Training iter #710000:   Batch Loss = 13.899809, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 14.622224807739258, Accuracy = 0.834963321685791
Training iter #712000:   Batch Loss = 13.866731, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 14.438980102539062, Accuracy = 0.8221271634101868
Training iter #714000:   Batch Loss = 14.775640, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 14.421006202697754, Accuracy = 0.8282396197319031
Training iter #716000:   Batch Loss = 13.020438, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 14.483318328857422, Accuracy = 0.8190708756446838
Training iter #718000:   Batch Loss = 13.534156, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 14.228900909423828, Accuracy = 0.8331295847892761
Training iter #720000:   Batch Loss = 13.257177, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.970817565917969, Accuracy = 0.8361858129501343
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1800
Training iter #722000:   Batch Loss = 13.615154, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 14.184880256652832, Accuracy = 0.8251833915710449
Training iter #724000:   Batch Loss = 13.073898, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.902912139892578, Accuracy = 0.8447432518005371
Training iter #726000:   Batch Loss = 12.852493, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.719743728637695, Accuracy = 0.8453544974327087
Training iter #728000:   Batch Loss = 12.964055, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.718414306640625, Accuracy = 0.8398532867431641
Training iter #730000:   Batch Loss = 12.416107, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.638790130615234, Accuracy = 0.8380195498466492
Training iter #732000:   Batch Loss = 12.058578, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.55029296875, Accuracy = 0.8447432518005371
Training iter #734000:   Batch Loss = 13.068846, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.809457778930664, Accuracy = 0.830073356628418
Training iter #736000:   Batch Loss = 12.936283, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 14.203380584716797, Accuracy = 0.8319070935249329
Training iter #738000:   Batch Loss = 12.675492, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.981768608093262, Accuracy = 0.8392420411109924
Training iter #740000:   Batch Loss = 12.658983, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.68558120727539, Accuracy = 0.8557457327842712
Training iter #742000:   Batch Loss = 13.186653, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.946516036987305, Accuracy = 0.815403401851654
Training iter #744000:   Batch Loss = 12.716850, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.668950080871582, Accuracy = 0.8398532867431641
Training iter #746000:   Batch Loss = 12.834871, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.449834823608398, Accuracy = 0.8453544974327087
Training iter #748000:   Batch Loss = 15.038237, Accuracy = 0.7058823704719543
PERFORMANCE ON TEST SET: Batch Loss = 13.540696144104004, Accuracy = 0.84779953956604
Training iter #750000:   Batch Loss = 12.761766, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.523395538330078, Accuracy = 0.8319070935249329
Training iter #752000:   Batch Loss = 11.916403, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.446748733520508, Accuracy = 0.8484107851982117
Training iter #754000:   Batch Loss = 12.170533, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.226521492004395, Accuracy = 0.8557457327842712
Training iter #756000:   Batch Loss = 11.808030, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.131791114807129, Accuracy = 0.8520782589912415
Training iter #758000:   Batch Loss = 12.126998, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.176390647888184, Accuracy = 0.8508557677268982
Training iter #760000:   Batch Loss = 11.606069, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.990942001342773, Accuracy = 0.8643031716346741
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-1900
Training iter #762000:   Batch Loss = 11.676003, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.09228801727295, Accuracy = 0.8569682240486145
Training iter #764000:   Batch Loss = 12.485549, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.330727577209473, Accuracy = 0.8435207605361938
Training iter #766000:   Batch Loss = 12.451029, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.111961364746094, Accuracy = 0.8569682240486145
Training iter #768000:   Batch Loss = 11.452634, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.94891357421875, Accuracy = 0.8575794696807861
Training iter #770000:   Batch Loss = 12.245386, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.333576202392578, Accuracy = 0.8447432518005371
Training iter #772000:   Batch Loss = 11.738052, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.143356323242188, Accuracy = 0.8398532867431641
Training iter #774000:   Batch Loss = 11.835933, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.167417526245117, Accuracy = 0.8539119958877563
Training iter #776000:   Batch Loss = 11.964258, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.187719345092773, Accuracy = 0.841687023639679
Training iter #778000:   Batch Loss = 11.921968, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.998832702636719, Accuracy = 0.8422982692718506
Training iter #780000:   Batch Loss = 11.406337, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.844650268554688, Accuracy = 0.8508557677268982
Training iter #782000:   Batch Loss = 13.595675, Accuracy = 0.6470588445663452
PERFORMANCE ON TEST SET: Batch Loss = 12.75611400604248, Accuracy = 0.8496332764625549
Training iter #784000:   Batch Loss = 11.507857, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.904399871826172, Accuracy = 0.8459657430648804
Training iter #786000:   Batch Loss = 11.733218, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.545218467712402, Accuracy = 0.8514670133590698
Training iter #788000:   Batch Loss = 11.774837, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.619462966918945, Accuracy = 0.8496332764625549
Training iter #790000:   Batch Loss = 12.234329, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.70585823059082, Accuracy = 0.8520782589912415
Training iter #792000:   Batch Loss = 11.726478, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.561577796936035, Accuracy = 0.8508557677268982
Training iter #794000:   Batch Loss = 10.844132, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.46213150024414, Accuracy = 0.8612469434738159
Training iter #796000:   Batch Loss = 10.855960, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.405590057373047, Accuracy = 0.8624694347381592
Training iter #798000:   Batch Loss = 11.227965, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.283206939697266, Accuracy = 0.859413206577301
Training iter #800000:   Batch Loss = 11.236426, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.176972389221191, Accuracy = 0.8643031716346741
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2000
Training iter #802000:   Batch Loss = 10.935511, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.069531440734863, Accuracy = 0.871026873588562
Training iter #804000:   Batch Loss = 11.660041, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.603572845458984, Accuracy = 0.846576988697052
Training iter #806000:   Batch Loss = 11.105098, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.416173934936523, Accuracy = 0.8606356978416443
Training iter #808000:   Batch Loss = 10.851124, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.34878158569336, Accuracy = 0.8459657430648804
Training iter #810000:   Batch Loss = 11.319811, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.669265747070312, Accuracy = 0.8496332764625549
Training iter #812000:   Batch Loss = 11.193989, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.677311897277832, Accuracy = 0.8581907153129578
Training iter #814000:   Batch Loss = 10.914907, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.414520263671875, Accuracy = 0.8557457327842712
Training iter #816000:   Batch Loss = 10.883027, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 12.440661430358887, Accuracy = 0.8655256628990173
Training iter #818000:   Batch Loss = 10.837670, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.215049743652344, Accuracy = 0.8539119958877563
Training iter #820000:   Batch Loss = 10.950291, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.242982864379883, Accuracy = 0.8630806803703308
Training iter #822000:   Batch Loss = 11.595460, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.261812210083008, Accuracy = 0.8673593997955322
Training iter #824000:   Batch Loss = 11.349026, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.352897644042969, Accuracy = 0.8612469434738159
Training iter #826000:   Batch Loss = 11.648357, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.195014953613281, Accuracy = 0.8618581891059875
Training iter #828000:   Batch Loss = 11.142357, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.293571472167969, Accuracy = 0.8569682240486145
Training iter #830000:   Batch Loss = 11.150158, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.61070442199707, Accuracy = 0.846576988697052
Training iter #832000:   Batch Loss = 10.844835, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.431702613830566, Accuracy = 0.84779953956604
Training iter #834000:   Batch Loss = 11.045403, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.25583267211914, Accuracy = 0.8655256628990173
Training iter #836000:   Batch Loss = 11.306677, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.809539794921875, Accuracy = 0.866136908531189
Training iter #838000:   Batch Loss = 11.260237, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.795904159545898, Accuracy = 0.8673593997955322
Training iter #840000:   Batch Loss = 11.571094, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.225037574768066, Accuracy = 0.8575794696807861
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2100
Training iter #842000:   Batch Loss = 12.261860, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.286245346069336, Accuracy = 0.8746943473815918
Training iter #844000:   Batch Loss = 11.643614, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.186784744262695, Accuracy = 0.8606356978416443
Training iter #846000:   Batch Loss = 11.581322, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.824769973754883, Accuracy = 0.8600244522094727
Training iter #848000:   Batch Loss = 11.299257, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.562390327453613, Accuracy = 0.8667481541633606
Training iter #850000:   Batch Loss = 9.698658, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 12.534255981445312, Accuracy = 0.8691931366920471
Training iter #852000:   Batch Loss = 11.292232, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.459274291992188, Accuracy = 0.8643031716346741
Training iter #854000:   Batch Loss = 10.805666, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.384028434753418, Accuracy = 0.8698043823242188
Training iter #856000:   Batch Loss = 10.898609, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.293081283569336, Accuracy = 0.8740831017494202
Training iter #858000:   Batch Loss = 11.084019, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.309682846069336, Accuracy = 0.8673593997955322
Training iter #860000:   Batch Loss = 10.917416, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.172977447509766, Accuracy = 0.8667481541633606
Training iter #862000:   Batch Loss = 10.528069, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.98823356628418, Accuracy = 0.871026873588562
Training iter #864000:   Batch Loss = 10.508785, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.23231315612793, Accuracy = 0.8667481541633606
Training iter #866000:   Batch Loss = 10.946331, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.176213264465332, Accuracy = 0.8691931366920471
Training iter #868000:   Batch Loss = 11.243079, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.455172538757324, Accuracy = 0.8539119958877563
Training iter #870000:   Batch Loss = 11.140141, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.36544418334961, Accuracy = 0.8734718561172485
Training iter #872000:   Batch Loss = 11.564707, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.3967924118042, Accuracy = 0.8655256628990173
Training iter #874000:   Batch Loss = 12.356368, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.19562816619873, Accuracy = 0.8539119958877563
Training iter #876000:   Batch Loss = 11.405806, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.917457580566406, Accuracy = 0.8502445220947266
Training iter #878000:   Batch Loss = 12.068665, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.168179512023926, Accuracy = 0.84779953956604
Training iter #880000:   Batch Loss = 12.916314, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.53486442565918, Accuracy = 0.8471882343292236
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2200
Training iter #882000:   Batch Loss = 12.574339, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.35194206237793, Accuracy = 0.8581907153129578
Training iter #884000:   Batch Loss = 13.130463, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 13.127591133117676, Accuracy = 0.8526895046234131
Training iter #886000:   Batch Loss = 12.122207, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.047540664672852, Accuracy = 0.8600244522094727
Training iter #888000:   Batch Loss = 12.002825, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.783553123474121, Accuracy = 0.859413206577301
Training iter #890000:   Batch Loss = 11.404224, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.72551155090332, Accuracy = 0.8691931366920471
Training iter #892000:   Batch Loss = 11.154714, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.7637300491333, Accuracy = 0.8630806803703308
Training iter #894000:   Batch Loss = 11.286213, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.673276901245117, Accuracy = 0.8643031716346741
Training iter #896000:   Batch Loss = 11.634022, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.743326187133789, Accuracy = 0.8630806803703308
Training iter #898000:   Batch Loss = 11.525261, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.911380767822266, Accuracy = 0.841687023639679
Training iter #900000:   Batch Loss = 11.526842, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.607932090759277, Accuracy = 0.871026873588562
Training iter #902000:   Batch Loss = 11.375574, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.543778419494629, Accuracy = 0.8643031716346741
Training iter #904000:   Batch Loss = 11.407130, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.619991302490234, Accuracy = 0.8600244522094727
Training iter #906000:   Batch Loss = 11.414843, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.718534469604492, Accuracy = 0.8520782589912415
Training iter #908000:   Batch Loss = 12.145617, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.04410457611084, Accuracy = 0.8581907153129578
Training iter #910000:   Batch Loss = 12.094118, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.57666301727295, Accuracy = 0.8533007502555847
Training iter #912000:   Batch Loss = 13.563949, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 14.068878173828125, Accuracy = 0.8612469434738159
Training iter #914000:   Batch Loss = 12.309922, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.470680236816406, Accuracy = 0.8685818910598755
Training iter #916000:   Batch Loss = 12.362002, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.275690078735352, Accuracy = 0.8557457327842712
Training iter #918000:   Batch Loss = 13.291212, Accuracy = 0.8235294222831726
PERFORMANCE ON TEST SET: Batch Loss = 12.951845169067383, Accuracy = 0.8685818910598755
Training iter #920000:   Batch Loss = 11.762379, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 13.578200340270996, Accuracy = 0.8557457327842712
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2300
Training iter #922000:   Batch Loss = 12.436644, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.592432975769043, Accuracy = 0.84779953956604
Training iter #924000:   Batch Loss = 11.684258, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.145597457885742, Accuracy = 0.8496332764625549
Training iter #926000:   Batch Loss = 11.988695, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.42231559753418, Accuracy = 0.8496332764625549
Training iter #928000:   Batch Loss = 12.038979, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.150431632995605, Accuracy = 0.8588019609451294
Training iter #930000:   Batch Loss = 11.368700, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.804810523986816, Accuracy = 0.8557457327842712
Training iter #932000:   Batch Loss = 12.162031, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.7677001953125, Accuracy = 0.8484107851982117
Training iter #934000:   Batch Loss = 11.492746, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.07673454284668, Accuracy = 0.8673593997955322
Training iter #936000:   Batch Loss = 11.416103, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.948241233825684, Accuracy = 0.8618581891059875
Training iter #938000:   Batch Loss = 11.410120, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.580240249633789, Accuracy = 0.8685818910598755
Training iter #940000:   Batch Loss = 12.290912, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.867142677307129, Accuracy = 0.8673593997955322
Training iter #942000:   Batch Loss = 11.465830, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.5831937789917, Accuracy = 0.8704156279563904
Training iter #944000:   Batch Loss = 11.254827, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.477999687194824, Accuracy = 0.8783618807792664
Training iter #946000:   Batch Loss = 11.607632, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.542584419250488, Accuracy = 0.8612469434738159
Training iter #948000:   Batch Loss = 10.666189, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.73733139038086, Accuracy = 0.8649144172668457
Training iter #950000:   Batch Loss = 10.776106, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.637548446655273, Accuracy = 0.8783618807792664
Training iter #952000:   Batch Loss = 12.067659, Accuracy = 0.8235294222831726
PERFORMANCE ON TEST SET: Batch Loss = 12.62634563446045, Accuracy = 0.8667481541633606
Training iter #954000:   Batch Loss = 11.643925, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.950592041015625, Accuracy = 0.859413206577301
Training iter #956000:   Batch Loss = 11.141719, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.6001615524292, Accuracy = 0.8643031716346741
Training iter #958000:   Batch Loss = 11.265534, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.47970962524414, Accuracy = 0.8606356978416443
Training iter #960000:   Batch Loss = 11.126838, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.571002006530762, Accuracy = 0.8569682240486145
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2400
Training iter #962000:   Batch Loss = 11.729859, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.399721145629883, Accuracy = 0.8612469434738159
Training iter #964000:   Batch Loss = 10.566763, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.262983322143555, Accuracy = 0.8643031716346741
Training iter #966000:   Batch Loss = 10.854155, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.447498321533203, Accuracy = 0.8514670133590698
Training iter #968000:   Batch Loss = 11.398592, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.44998836517334, Accuracy = 0.8551344871520996
Training iter #970000:   Batch Loss = 10.934213, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.267786979675293, Accuracy = 0.8667481541633606
Training iter #972000:   Batch Loss = 10.980918, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.047615051269531, Accuracy = 0.8624694347381592
Training iter #974000:   Batch Loss = 11.544293, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.655498504638672, Accuracy = 0.8563569784164429
Training iter #976000:   Batch Loss = 11.150499, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.205787658691406, Accuracy = 0.8643031716346741
Training iter #978000:   Batch Loss = 10.782913, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.18225383758545, Accuracy = 0.8612469434738159
Training iter #980000:   Batch Loss = 10.923021, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.436820030212402, Accuracy = 0.8600244522094727
Training iter #982000:   Batch Loss = 10.902302, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.300939559936523, Accuracy = 0.8618581891059875
Training iter #984000:   Batch Loss = 10.566406, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.216423988342285, Accuracy = 0.8624694347381592
Training iter #986000:   Batch Loss = 11.823604, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 12.101080894470215, Accuracy = 0.8685818910598755
Training iter #988000:   Batch Loss = 10.608223, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.105175971984863, Accuracy = 0.8606356978416443
Training iter #990000:   Batch Loss = 10.742556, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.014655113220215, Accuracy = 0.8734718561172485
Training iter #992000:   Batch Loss = 11.066985, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.844940185546875, Accuracy = 0.8643031716346741
Training iter #994000:   Batch Loss = 10.994043, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.062859535217285, Accuracy = 0.8698043823242188
Training iter #996000:   Batch Loss = 10.466359, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.991682052612305, Accuracy = 0.8673593997955322
Training iter #998000:   Batch Loss = 11.033384, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.203429222106934, Accuracy = 0.8753056526184082
Training iter #1000000:   Batch Loss = 10.322824, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.856324195861816, Accuracy = 0.871026873588562
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2500
Training iter #1002000:   Batch Loss = 10.935057, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.907602310180664, Accuracy = 0.8722493648529053
Training iter #1004000:   Batch Loss = 10.662669, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.910102844238281, Accuracy = 0.8679706454277039
Training iter #1006000:   Batch Loss = 10.513412, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.81451416015625, Accuracy = 0.8777506351470947
Training iter #1008000:   Batch Loss = 10.482101, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.911985397338867, Accuracy = 0.8771393895149231
Training iter #1010000:   Batch Loss = 10.249139, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.74513053894043, Accuracy = 0.8667481541633606
Training iter #1012000:   Batch Loss = 10.264422, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.711453437805176, Accuracy = 0.8728606104850769
Training iter #1014000:   Batch Loss = 10.628702, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.710082054138184, Accuracy = 0.8704156279563904
Training iter #1016000:   Batch Loss = 10.291335, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.643518447875977, Accuracy = 0.8618581891059875
Training iter #1018000:   Batch Loss = 10.631330, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.48282241821289, Accuracy = 0.8771393895149231
Training iter #1020000:   Batch Loss = 10.426821, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.668243408203125, Accuracy = 0.8820293545722961
Training iter #1022000:   Batch Loss = 10.703638, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.949749946594238, Accuracy = 0.8753056526184082
Training iter #1024000:   Batch Loss = 10.405652, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.777442932128906, Accuracy = 0.878973126411438
Training iter #1026000:   Batch Loss = 10.310753, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.588048934936523, Accuracy = 0.8801956176757812
Training iter #1028000:   Batch Loss = 10.043612, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.878264427185059, Accuracy = 0.866136908531189
Training iter #1030000:   Batch Loss = 10.065403, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.827381134033203, Accuracy = 0.8643031716346741
Training iter #1032000:   Batch Loss = 10.325042, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.66369915008545, Accuracy = 0.8759168982505798
Training iter #1034000:   Batch Loss = 10.133120, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.624262809753418, Accuracy = 0.8734718561172485
Training iter #1036000:   Batch Loss = 10.080695, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.68312931060791, Accuracy = 0.878973126411438
Training iter #1038000:   Batch Loss = 10.088339, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.561735153198242, Accuracy = 0.8740831017494202
Training iter #1040000:   Batch Loss = 10.332376, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.578909873962402, Accuracy = 0.8740831017494202
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2600
Training iter #1042000:   Batch Loss = 10.411360, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.827829360961914, Accuracy = 0.8679706454277039
Training iter #1044000:   Batch Loss = 10.779284, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.702414512634277, Accuracy = 0.8753056526184082
Training iter #1046000:   Batch Loss = 10.046342, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.603498458862305, Accuracy = 0.8844743371009827
Training iter #1048000:   Batch Loss = 9.940205, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.834844589233398, Accuracy = 0.8795843720436096
Training iter #1050000:   Batch Loss = 10.173740, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.827001571655273, Accuracy = 0.8734718561172485
Training iter #1052000:   Batch Loss = 10.147789, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.672834396362305, Accuracy = 0.8832518458366394
Training iter #1054000:   Batch Loss = 10.156161, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.920679092407227, Accuracy = 0.8814181089401245
Training iter #1056000:   Batch Loss = 9.759587, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.823233604431152, Accuracy = 0.878973126411438
Training iter #1058000:   Batch Loss = 9.871517, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.583282470703125, Accuracy = 0.8722493648529053
Training iter #1060000:   Batch Loss = 10.350420, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.70214557647705, Accuracy = 0.8771393895149231
Training iter #1062000:   Batch Loss = 10.717725, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.12592887878418, Accuracy = 0.8746943473815918
Training iter #1064000:   Batch Loss = 10.954272, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.346328735351562, Accuracy = 0.8679706454277039
Training iter #1066000:   Batch Loss = 10.454835, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.619743347167969, Accuracy = 0.8722493648529053
Training iter #1068000:   Batch Loss = 10.397095, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.081254959106445, Accuracy = 0.8759168982505798
Training iter #1070000:   Batch Loss = 10.828674, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.935169219970703, Accuracy = 0.866136908531189
Training iter #1072000:   Batch Loss = 10.826098, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.728120803833008, Accuracy = 0.8746943473815918
Training iter #1074000:   Batch Loss = 10.055597, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.65298080444336, Accuracy = 0.8685818910598755
Training iter #1076000:   Batch Loss = 10.495053, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.824136734008789, Accuracy = 0.8746943473815918
Training iter #1078000:   Batch Loss = 10.184769, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.81559944152832, Accuracy = 0.8636919260025024
Training iter #1080000:   Batch Loss = 10.417013, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.608264923095703, Accuracy = 0.8734718561172485
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2700
Training iter #1082000:   Batch Loss = 10.269397, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.557474136352539, Accuracy = 0.8679706454277039
Training iter #1084000:   Batch Loss = 9.639891, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.346970558166504, Accuracy = 0.8759168982505798
Training iter #1086000:   Batch Loss = 10.126535, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.375422477722168, Accuracy = 0.8673593997955322
Training iter #1088000:   Batch Loss = 9.138324, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.337457656860352, Accuracy = 0.8783618807792664
Training iter #1090000:   Batch Loss = 10.307451, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.43362045288086, Accuracy = 0.8765281438827515
Training iter #1092000:   Batch Loss = 10.422392, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.698030471801758, Accuracy = 0.8808068633079529
Training iter #1094000:   Batch Loss = 10.941746, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.033120155334473, Accuracy = 0.8875305652618408
Training iter #1096000:   Batch Loss = 10.705431, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.885425567626953, Accuracy = 0.8814181089401245
Training iter #1098000:   Batch Loss = 10.639058, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.03969955444336, Accuracy = 0.8722493648529053
Training iter #1100000:   Batch Loss = 10.442657, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.890753746032715, Accuracy = 0.8624694347381592
Training iter #1102000:   Batch Loss = 10.494768, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.988574981689453, Accuracy = 0.8771393895149231
Training iter #1104000:   Batch Loss = 10.232556, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.812105178833008, Accuracy = 0.8649144172668457
Training iter #1106000:   Batch Loss = 10.423185, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.673831939697266, Accuracy = 0.8771393895149231
Training iter #1108000:   Batch Loss = 10.563066, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.775779724121094, Accuracy = 0.871026873588562
Training iter #1110000:   Batch Loss = 10.143526, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.114387512207031, Accuracy = 0.8569682240486145
Training iter #1112000:   Batch Loss = 10.447430, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.850970268249512, Accuracy = 0.8606356978416443
Training iter #1114000:   Batch Loss = 10.655778, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.56607437133789, Accuracy = 0.8588019609451294
Training iter #1116000:   Batch Loss = 10.430958, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.893020629882812, Accuracy = 0.866136908531189
Training iter #1118000:   Batch Loss = 10.884315, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.547454833984375, Accuracy = 0.871026873588562
Training iter #1120000:   Batch Loss = 11.245757, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.127748489379883, Accuracy = 0.871026873588562
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2800
Training iter #1122000:   Batch Loss = 9.459678, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 12.064929008483887, Accuracy = 0.871026873588562
Training iter #1124000:   Batch Loss = 10.714243, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.985184669494629, Accuracy = 0.8649144172668457
Training iter #1126000:   Batch Loss = 10.410258, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.992212295532227, Accuracy = 0.8667481541633606
Training iter #1128000:   Batch Loss = 10.464547, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.871353149414062, Accuracy = 0.8716381192207336
Training iter #1130000:   Batch Loss = 10.580448, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.743844985961914, Accuracy = 0.8722493648529053
Training iter #1132000:   Batch Loss = 10.169280, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.647455215454102, Accuracy = 0.8685818910598755
Training iter #1134000:   Batch Loss = 10.213554, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.48817253112793, Accuracy = 0.8783618807792664
Training iter #1136000:   Batch Loss = 9.754968, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.528066635131836, Accuracy = 0.8740831017494202
Training iter #1138000:   Batch Loss = 10.072106, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.419421195983887, Accuracy = 0.8832518458366394
Training iter #1140000:   Batch Loss = 9.579981, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.211385726928711, Accuracy = 0.8863080739974976
Training iter #1142000:   Batch Loss = 9.406836, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.359685897827148, Accuracy = 0.8820293545722961
Training iter #1144000:   Batch Loss = 9.849437, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.640250205993652, Accuracy = 0.8746943473815918
Training iter #1146000:   Batch Loss = 10.167107, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.52054214477539, Accuracy = 0.878973126411438
Training iter #1148000:   Batch Loss = 10.329449, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.41844367980957, Accuracy = 0.8746943473815918
Training iter #1150000:   Batch Loss = 10.073172, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.359994888305664, Accuracy = 0.8777506351470947
Training iter #1152000:   Batch Loss = 10.031473, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.384248733520508, Accuracy = 0.8795843720436096
Training iter #1154000:   Batch Loss = 9.848127, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.265869140625, Accuracy = 0.8826406002044678
Training iter #1156000:   Batch Loss = 10.482696, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.868908882141113, Accuracy = 0.8795843720436096
Training iter #1158000:   Batch Loss = 10.915718, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.081271171569824, Accuracy = 0.8691931366920471
Training iter #1160000:   Batch Loss = 10.205925, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.810737609863281, Accuracy = 0.8691931366920471
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-2900
Training iter #1162000:   Batch Loss = 10.295658, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.701123237609863, Accuracy = 0.8728606104850769
Training iter #1164000:   Batch Loss = 10.153392, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.094612121582031, Accuracy = 0.8691931366920471
Training iter #1166000:   Batch Loss = 10.688165, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.214982032775879, Accuracy = 0.859413206577301
Training iter #1168000:   Batch Loss = 9.949078, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.747350692749023, Accuracy = 0.8734718561172485
Training iter #1170000:   Batch Loss = 10.004284, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.090932846069336, Accuracy = 0.866136908531189
Training iter #1172000:   Batch Loss = 10.476795, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.851468086242676, Accuracy = 0.8588019609451294
Training iter #1174000:   Batch Loss = 10.047428, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.847335815429688, Accuracy = 0.8618581891059875
Training iter #1176000:   Batch Loss = 10.462343, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.709598541259766, Accuracy = 0.8759168982505798
Training iter #1178000:   Batch Loss = 10.145432, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.633962631225586, Accuracy = 0.8734718561172485
Training iter #1180000:   Batch Loss = 9.854606, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.64155387878418, Accuracy = 0.8734718561172485
Training iter #1182000:   Batch Loss = 10.384461, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.64830207824707, Accuracy = 0.8759168982505798
Training iter #1184000:   Batch Loss = 10.343348, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.05926513671875, Accuracy = 0.8636919260025024
Training iter #1186000:   Batch Loss = 9.910985, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.574563980102539, Accuracy = 0.8667481541633606
Training iter #1188000:   Batch Loss = 10.245085, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.381122589111328, Accuracy = 0.8795843720436096
Training iter #1190000:   Batch Loss = 9.610948, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 11.488995552062988, Accuracy = 0.8716381192207336
Training iter #1192000:   Batch Loss = 10.097509, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.367156982421875, Accuracy = 0.8753056526184082
Training iter #1194000:   Batch Loss = 9.999937, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.359456062316895, Accuracy = 0.8704156279563904
Training iter #1196000:   Batch Loss = 9.898144, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.373936653137207, Accuracy = 0.878973126411438
Training iter #1198000:   Batch Loss = 9.942171, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.791854858398438, Accuracy = 0.8643031716346741
Training iter #1200000:   Batch Loss = 9.930060, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.628314971923828, Accuracy = 0.8691931366920471
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3000
Training iter #1202000:   Batch Loss = 9.694319, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.33372974395752, Accuracy = 0.8783618807792664
Training iter #1204000:   Batch Loss = 9.718674, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.476215362548828, Accuracy = 0.8698043823242188
Training iter #1206000:   Batch Loss = 10.105606, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.909226417541504, Accuracy = 0.8753056526184082
Training iter #1208000:   Batch Loss = 10.296217, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.633795738220215, Accuracy = 0.8801956176757812
Training iter #1210000:   Batch Loss = 9.711443, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.782278060913086, Accuracy = 0.8673593997955322
Training iter #1212000:   Batch Loss = 9.858517, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.445623397827148, Accuracy = 0.8691931366920471
Training iter #1214000:   Batch Loss = 9.497097, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.342997550964355, Accuracy = 0.8783618807792664
Training iter #1216000:   Batch Loss = 9.828823, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.503437042236328, Accuracy = 0.8667481541633606
Training iter #1218000:   Batch Loss = 9.672487, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.586443901062012, Accuracy = 0.878973126411438
Training iter #1220000:   Batch Loss = 9.898505, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.460905075073242, Accuracy = 0.8808068633079529
Training iter #1222000:   Batch Loss = 9.982582, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.371931076049805, Accuracy = 0.8740831017494202
Training iter #1224000:   Batch Loss = 9.957241, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.379538536071777, Accuracy = 0.8685818910598755
Training iter #1226000:   Batch Loss = 9.847292, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.650012016296387, Accuracy = 0.8691931366920471
Training iter #1228000:   Batch Loss = 10.175118, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.437835693359375, Accuracy = 0.8753056526184082
Training iter #1230000:   Batch Loss = 9.737518, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.623199462890625, Accuracy = 0.8759168982505798
Training iter #1232000:   Batch Loss = 10.352320, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.6996488571167, Accuracy = 0.871026873588562
Training iter #1234000:   Batch Loss = 10.006048, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.337262153625488, Accuracy = 0.8759168982505798
Training iter #1236000:   Batch Loss = 10.037836, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.393401145935059, Accuracy = 0.871026873588562
Training iter #1238000:   Batch Loss = 9.637179, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.544537544250488, Accuracy = 0.8618581891059875
Training iter #1240000:   Batch Loss = 9.999864, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.3942232131958, Accuracy = 0.8765281438827515
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3100
Training iter #1242000:   Batch Loss = 9.449894, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.1602144241333, Accuracy = 0.878973126411438
Training iter #1244000:   Batch Loss = 9.377525, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.172369003295898, Accuracy = 0.8844743371009827
Training iter #1246000:   Batch Loss = 9.564490, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.173377990722656, Accuracy = 0.8850855827331543
Training iter #1248000:   Batch Loss = 9.365244, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.01754379272461, Accuracy = 0.8869193196296692
Training iter #1250000:   Batch Loss = 9.552196, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.942827224731445, Accuracy = 0.8814181089401245
Training iter #1252000:   Batch Loss = 8.891184, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.067822456359863, Accuracy = 0.8728606104850769
Training iter #1254000:   Batch Loss = 9.139471, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.881916046142578, Accuracy = 0.8814181089401245
Training iter #1256000:   Batch Loss = 9.255148, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.938453674316406, Accuracy = 0.8777506351470947
Training iter #1258000:   Batch Loss = 9.675888, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 10.923507690429688, Accuracy = 0.883863091468811
Training iter #1260000:   Batch Loss = 10.375383, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.995770454406738, Accuracy = 0.8728606104850769
Training iter #1262000:   Batch Loss = 10.234316, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.448150634765625, Accuracy = 0.871026873588562
Training iter #1264000:   Batch Loss = 10.612204, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.06776237487793, Accuracy = 0.8612469434738159
Training iter #1266000:   Batch Loss = 9.854698, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.791952133178711, Accuracy = 0.8685818910598755
Training iter #1268000:   Batch Loss = 10.008486, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.826467514038086, Accuracy = 0.8649144172668457
Training iter #1270000:   Batch Loss = 9.660109, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.368149757385254, Accuracy = 0.8795843720436096
Training iter #1272000:   Batch Loss = 9.459866, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.411748886108398, Accuracy = 0.8728606104850769
Training iter #1274000:   Batch Loss = 9.309649, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.221384048461914, Accuracy = 0.8801956176757812
Training iter #1276000:   Batch Loss = 9.934971, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.09248161315918, Accuracy = 0.8777506351470947
Training iter #1278000:   Batch Loss = 9.810759, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.460582733154297, Accuracy = 0.878973126411438
Training iter #1280000:   Batch Loss = 9.341053, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.246244430541992, Accuracy = 0.8771393895149231
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3200
Training iter #1282000:   Batch Loss = 9.548198, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.129918098449707, Accuracy = 0.8930317759513855
Training iter #1284000:   Batch Loss = 9.025519, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.968036651611328, Accuracy = 0.8844743371009827
Training iter #1286000:   Batch Loss = 10.067390, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.402070999145508, Accuracy = 0.8808068633079529
Training iter #1288000:   Batch Loss = 9.741207, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.097010612487793, Accuracy = 0.8856968283653259
Training iter #1290000:   Batch Loss = 9.278242, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.126269340515137, Accuracy = 0.8801956176757812
Training iter #1292000:   Batch Loss = 9.368338, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.060168266296387, Accuracy = 0.878973126411438
Training iter #1294000:   Batch Loss = 8.943932, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.144365310668945, Accuracy = 0.8740831017494202
Training iter #1296000:   Batch Loss = 9.414861, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.89959716796875, Accuracy = 0.871026873588562
Training iter #1298000:   Batch Loss = 9.384668, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.715692520141602, Accuracy = 0.883863091468811
Training iter #1300000:   Batch Loss = 9.578248, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.720157623291016, Accuracy = 0.8832518458366394
Training iter #1302000:   Batch Loss = 10.491379, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.550500869750977, Accuracy = 0.8795843720436096
Training iter #1304000:   Batch Loss = 9.870225, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.358051300048828, Accuracy = 0.8808068633079529
Training iter #1306000:   Batch Loss = 8.999577, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.362832069396973, Accuracy = 0.8704156279563904
Training iter #1308000:   Batch Loss = 9.729043, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.12812614440918, Accuracy = 0.8850855827331543
Training iter #1310000:   Batch Loss = 9.505945, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.994622230529785, Accuracy = 0.8856968283653259
Training iter #1312000:   Batch Loss = 9.131055, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.939346313476562, Accuracy = 0.8795843720436096
Training iter #1314000:   Batch Loss = 9.272031, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.948175430297852, Accuracy = 0.8844743371009827
Training iter #1316000:   Batch Loss = 8.963703, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.921722412109375, Accuracy = 0.8856968283653259
Training iter #1318000:   Batch Loss = 9.413631, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.076828002929688, Accuracy = 0.8783618807792664
Training iter #1320000:   Batch Loss = 9.333984, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.090010643005371, Accuracy = 0.8685818910598755
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3300
Training iter #1322000:   Batch Loss = 9.460086, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.857011795043945, Accuracy = 0.8814181089401245
Training iter #1324000:   Batch Loss = 8.767714, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.942390441894531, Accuracy = 0.8808068633079529
Training iter #1326000:   Batch Loss = 9.323744, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.70711612701416, Accuracy = 0.8985329866409302
Training iter #1328000:   Batch Loss = 9.753124, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.039787292480469, Accuracy = 0.8887530565261841
Training iter #1330000:   Batch Loss = 9.333025, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.909748077392578, Accuracy = 0.8856968283653259
Training iter #1332000:   Batch Loss = 9.517061, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.219587326049805, Accuracy = 0.890586793422699
Training iter #1334000:   Batch Loss = 9.847241, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.135004043579102, Accuracy = 0.8801956176757812
Training iter #1336000:   Batch Loss = 9.284551, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.963152885437012, Accuracy = 0.8869193196296692
Training iter #1338000:   Batch Loss = 9.364410, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.91146469116211, Accuracy = 0.8936430215835571
Training iter #1340000:   Batch Loss = 9.215477, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.45708179473877, Accuracy = 0.8734718561172485
Training iter #1342000:   Batch Loss = 9.817356, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.719037055969238, Accuracy = 0.8801956176757812
Training iter #1344000:   Batch Loss = 9.258224, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.04124927520752, Accuracy = 0.8911980390548706
Training iter #1346000:   Batch Loss = 8.782805, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.812811851501465, Accuracy = 0.8875305652618408
Training iter #1348000:   Batch Loss = 9.170748, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.295551300048828, Accuracy = 0.8771393895149231
Training iter #1350000:   Batch Loss = 9.358691, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.005178451538086, Accuracy = 0.8911980390548706
Training iter #1352000:   Batch Loss = 9.398437, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.934755325317383, Accuracy = 0.8899755477905273
Training iter #1354000:   Batch Loss = 9.144213, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.888792037963867, Accuracy = 0.8875305652618408
Training iter #1356000:   Batch Loss = 9.126360, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.845647811889648, Accuracy = 0.8911980390548706
Training iter #1358000:   Batch Loss = 9.490303, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.851346015930176, Accuracy = 0.8844743371009827
Training iter #1360000:   Batch Loss = 8.482381, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 10.927380561828613, Accuracy = 0.8814181089401245
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3400
Training iter #1362000:   Batch Loss = 9.146471, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.875419616699219, Accuracy = 0.8832518458366394
Training iter #1364000:   Batch Loss = 9.797970, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.910818099975586, Accuracy = 0.883863091468811
Training iter #1366000:   Batch Loss = 8.861990, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.69787311553955, Accuracy = 0.8899755477905273
Training iter #1368000:   Batch Loss = 9.431100, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.033451080322266, Accuracy = 0.8832518458366394
Training iter #1370000:   Batch Loss = 9.092775, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.946884155273438, Accuracy = 0.8801956176757812
Training iter #1372000:   Batch Loss = 8.893433, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.710494041442871, Accuracy = 0.8832518458366394
Training iter #1374000:   Batch Loss = 8.658341, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.07926082611084, Accuracy = 0.8893643021583557
Training iter #1376000:   Batch Loss = 8.692003, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.000737190246582, Accuracy = 0.8875305652618408
Training iter #1378000:   Batch Loss = 9.170208, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.846745491027832, Accuracy = 0.8863080739974976
Training iter #1380000:   Batch Loss = 9.252838, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.674196243286133, Accuracy = 0.8881418108940125
Training iter #1382000:   Batch Loss = 9.505551, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.98654842376709, Accuracy = 0.8765281438827515
Training iter #1384000:   Batch Loss = 8.891328, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.818571090698242, Accuracy = 0.8856968283653259
Training iter #1386000:   Batch Loss = 9.239129, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.669102668762207, Accuracy = 0.871026873588562
Training iter #1388000:   Batch Loss = 8.729605, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.835420608520508, Accuracy = 0.8820293545722961
Training iter #1390000:   Batch Loss = 9.444666, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.668413162231445, Accuracy = 0.8881418108940125
Training iter #1392000:   Batch Loss = 8.782643, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.72596263885498, Accuracy = 0.8936430215835571
Training iter #1394000:   Batch Loss = 9.628572, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 10.59378433227539, Accuracy = 0.8936430215835571
Training iter #1396000:   Batch Loss = 9.032635, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.74531364440918, Accuracy = 0.8893643021583557
Training iter #1398000:   Batch Loss = 8.422511, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.652606010437012, Accuracy = 0.8826406002044678
Training iter #1400000:   Batch Loss = 8.637718, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.69399356842041, Accuracy = 0.8801956176757812
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3500
Training iter #1402000:   Batch Loss = 9.590202, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.012484550476074, Accuracy = 0.8936430215835571
Training iter #1404000:   Batch Loss = 9.043527, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.626288414001465, Accuracy = 0.883863091468811
Training iter #1406000:   Batch Loss = 8.877353, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.628374099731445, Accuracy = 0.8887530565261841
Training iter #1408000:   Batch Loss = 8.471131, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.74671745300293, Accuracy = 0.8899755477905273
Training iter #1410000:   Batch Loss = 8.944145, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.696368217468262, Accuracy = 0.8814181089401245
Training iter #1412000:   Batch Loss = 9.096471, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.563018798828125, Accuracy = 0.890586793422699
Training iter #1414000:   Batch Loss = 9.035547, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.471888542175293, Accuracy = 0.8899755477905273
Training iter #1416000:   Batch Loss = 8.689332, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.431681632995605, Accuracy = 0.8960880041122437
Training iter #1418000:   Batch Loss = 8.751434, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.402787208557129, Accuracy = 0.8966992497444153
Training iter #1420000:   Batch Loss = 10.193752, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.265985488891602, Accuracy = 0.8881418108940125
Training iter #1422000:   Batch Loss = 9.420979, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.294196128845215, Accuracy = 0.8850855827331543
Training iter #1424000:   Batch Loss = 9.317995, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.073286056518555, Accuracy = 0.8881418108940125
Training iter #1426000:   Batch Loss = 9.083967, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.78756332397461, Accuracy = 0.8844743371009827
Training iter #1428000:   Batch Loss = 8.188361, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.837263107299805, Accuracy = 0.8936430215835571
Training iter #1430000:   Batch Loss = 8.587433, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.632144927978516, Accuracy = 0.8973104953765869
Training iter #1432000:   Batch Loss = 8.544821, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.506726264953613, Accuracy = 0.9009779691696167
Training iter #1434000:   Batch Loss = 8.484220, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.589139938354492, Accuracy = 0.8979217410087585
Training iter #1436000:   Batch Loss = 8.536094, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.868330001831055, Accuracy = 0.8948655128479004
Training iter #1438000:   Batch Loss = 8.791222, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.701757431030273, Accuracy = 0.8899755477905273
Training iter #1440000:   Batch Loss = 8.761148, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.643585205078125, Accuracy = 0.8985329866409302
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3600
Training iter #1442000:   Batch Loss = 9.019116, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.881591796875, Accuracy = 0.8893643021583557
Training iter #1444000:   Batch Loss = 9.095864, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.840174674987793, Accuracy = 0.8850855827331543
Training iter #1446000:   Batch Loss = 9.759978, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.60952377319336, Accuracy = 0.8899755477905273
Training iter #1448000:   Batch Loss = 8.727433, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.45377254486084, Accuracy = 0.8936430215835571
Training iter #1450000:   Batch Loss = 8.815277, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.701273918151855, Accuracy = 0.8936430215835571
Training iter #1452000:   Batch Loss = 8.515723, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.5657377243042, Accuracy = 0.9009779691696167
Training iter #1454000:   Batch Loss = 8.787798, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.61528205871582, Accuracy = 0.8887530565261841
Training iter #1456000:   Batch Loss = 9.667562, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.677262306213379, Accuracy = 0.8942542672157288
Training iter #1458000:   Batch Loss = 9.928377, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.382962226867676, Accuracy = 0.8863080739974976
Training iter #1460000:   Batch Loss = 9.789296, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.066840171813965, Accuracy = 0.895476758480072
Training iter #1462000:   Batch Loss = 8.602378, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 11.102346420288086, Accuracy = 0.8863080739974976
Training iter #1464000:   Batch Loss = 8.915737, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.962660789489746, Accuracy = 0.8832518458366394
Training iter #1466000:   Batch Loss = 9.290659, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.844472885131836, Accuracy = 0.8820293545722961
Training iter #1468000:   Batch Loss = 9.094131, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.683500289916992, Accuracy = 0.8966992497444153
Training iter #1470000:   Batch Loss = 9.498510, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.358906745910645, Accuracy = 0.8887530565261841
Training iter #1472000:   Batch Loss = 9.001281, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.043107032775879, Accuracy = 0.878973126411438
Training iter #1474000:   Batch Loss = 9.458108, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.835867881774902, Accuracy = 0.8820293545722961
Training iter #1476000:   Batch Loss = 9.264935, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.139525413513184, Accuracy = 0.8814181089401245
Training iter #1478000:   Batch Loss = 9.729446, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.91447639465332, Accuracy = 0.8881418108940125
Training iter #1480000:   Batch Loss = 9.157362, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.957218170166016, Accuracy = 0.8881418108940125
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3700
Training iter #1482000:   Batch Loss = 9.389501, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.06199836730957, Accuracy = 0.8832518458366394
Training iter #1484000:   Batch Loss = 9.250928, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.068500518798828, Accuracy = 0.8795843720436096
Training iter #1486000:   Batch Loss = 9.598411, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.061442375183105, Accuracy = 0.8728606104850769
Training iter #1488000:   Batch Loss = 9.528399, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.788172721862793, Accuracy = 0.8869193196296692
Training iter #1490000:   Batch Loss = 9.236938, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.248028755187988, Accuracy = 0.8765281438827515
Training iter #1492000:   Batch Loss = 9.562030, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.230451583862305, Accuracy = 0.8777506351470947
Training iter #1494000:   Batch Loss = 9.324030, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.933394432067871, Accuracy = 0.8869193196296692
Training iter #1496000:   Batch Loss = 8.487808, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 11.170294761657715, Accuracy = 0.8936430215835571
Training iter #1498000:   Batch Loss = 9.842354, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.590476989746094, Accuracy = 0.8850855827331543
Training iter #1500000:   Batch Loss = 9.417127, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.217597007751465, Accuracy = 0.878973126411438
Training iter #1502000:   Batch Loss = 9.457397, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.968233108520508, Accuracy = 0.8856968283653259
Training iter #1504000:   Batch Loss = 10.101078, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.599313735961914, Accuracy = 0.8698043823242188
Training iter #1506000:   Batch Loss = 9.856848, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.173288345336914, Accuracy = 0.878973126411438
Training iter #1508000:   Batch Loss = 9.637330, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.118682861328125, Accuracy = 0.8893643021583557
Training iter #1510000:   Batch Loss = 9.960122, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.393133163452148, Accuracy = 0.8863080739974976
Training iter #1512000:   Batch Loss = 9.581372, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.083861351013184, Accuracy = 0.8820293545722961
Training iter #1514000:   Batch Loss = 9.503030, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.835660934448242, Accuracy = 0.8918092846870422
Training iter #1516000:   Batch Loss = 9.202784, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.778946876525879, Accuracy = 0.8881418108940125
Training iter #1518000:   Batch Loss = 9.203048, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.025938034057617, Accuracy = 0.8771393895149231
Training iter #1520000:   Batch Loss = 8.648334, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.673538208007812, Accuracy = 0.8863080739974976
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3800
Training iter #1522000:   Batch Loss = 8.837920, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.565374374389648, Accuracy = 0.8966992497444153
Training iter #1524000:   Batch Loss = 8.499470, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.849817276000977, Accuracy = 0.8698043823242188
Training iter #1526000:   Batch Loss = 8.925787, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.565061569213867, Accuracy = 0.890586793422699
Training iter #1528000:   Batch Loss = 9.140506, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.613495826721191, Accuracy = 0.890586793422699
Training iter #1530000:   Batch Loss = 8.987734, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 10.553082466125488, Accuracy = 0.890586793422699
Training iter #1532000:   Batch Loss = 8.842064, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.680553436279297, Accuracy = 0.8899755477905273
Training iter #1534000:   Batch Loss = 9.038246, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.595827102661133, Accuracy = 0.8966992497444153
Training iter #1536000:   Batch Loss = 9.039312, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.409679412841797, Accuracy = 0.8911980390548706
Training iter #1538000:   Batch Loss = 8.607243, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.405195236206055, Accuracy = 0.8948655128479004
Training iter #1540000:   Batch Loss = 8.982119, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.371091842651367, Accuracy = 0.9003667235374451
Training iter #1542000:   Batch Loss = 9.630228, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.296402931213379, Accuracy = 0.8881418108940125
Training iter #1544000:   Batch Loss = 9.564219, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.163707733154297, Accuracy = 0.8801956176757812
Training iter #1546000:   Batch Loss = 9.685020, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.416866302490234, Accuracy = 0.8759168982505798
Training iter #1548000:   Batch Loss = 9.440633, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.011905670166016, Accuracy = 0.8826406002044678
Training iter #1550000:   Batch Loss = 9.447196, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.334930419921875, Accuracy = 0.8930317759513855
Training iter #1552000:   Batch Loss = 9.465319, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.28570556640625, Accuracy = 0.8875305652618408
Training iter #1554000:   Batch Loss = 9.072019, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.133611679077148, Accuracy = 0.8869193196296692
Training iter #1556000:   Batch Loss = 9.077183, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.844512939453125, Accuracy = 0.890586793422699
Training iter #1558000:   Batch Loss = 11.439807, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.8154935836792, Accuracy = 0.8887530565261841
Training iter #1560000:   Batch Loss = 10.782875, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.229585647583008, Accuracy = 0.8918092846870422
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-3900
Training iter #1562000:   Batch Loss = 10.251205, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.897634506225586, Accuracy = 0.8887530565261841
Training iter #1564000:   Batch Loss = 9.442252, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.686965942382812, Accuracy = 0.8832518458366394
Training iter #1566000:   Batch Loss = 9.626880, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.414078712463379, Accuracy = 0.8826406002044678
Training iter #1568000:   Batch Loss = 9.833178, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.397665023803711, Accuracy = 0.8820293545722961
Training iter #1570000:   Batch Loss = 9.388620, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.190463066101074, Accuracy = 0.895476758480072
Training iter #1572000:   Batch Loss = 9.563681, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.51628589630127, Accuracy = 0.8826406002044678
Training iter #1574000:   Batch Loss = 9.685089, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.489754676818848, Accuracy = 0.878973126411438
Training iter #1576000:   Batch Loss = 9.835513, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.414116859436035, Accuracy = 0.8832518458366394
Training iter #1578000:   Batch Loss = 9.635859, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.555899620056152, Accuracy = 0.8777506351470947
Training iter #1580000:   Batch Loss = 9.686741, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.353109359741211, Accuracy = 0.8826406002044678
Training iter #1582000:   Batch Loss = 9.529057, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.170303344726562, Accuracy = 0.8869193196296692
Training iter #1584000:   Batch Loss = 9.492310, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.161077499389648, Accuracy = 0.8795843720436096
Training iter #1586000:   Batch Loss = 10.237994, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.962915420532227, Accuracy = 0.8795843720436096
Training iter #1588000:   Batch Loss = 10.117026, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.82542896270752, Accuracy = 0.8771393895149231
Training iter #1590000:   Batch Loss = 9.602518, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.876554489135742, Accuracy = 0.8728606104850769
Training iter #1592000:   Batch Loss = 10.669613, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.492063522338867, Accuracy = 0.8850855827331543
Training iter #1594000:   Batch Loss = 10.649286, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.846941947937012, Accuracy = 0.8820293545722961
Training iter #1596000:   Batch Loss = 10.528297, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.858694076538086, Accuracy = 0.8808068633079529
Training iter #1598000:   Batch Loss = 8.570156, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.955489158630371, Accuracy = 0.8850855827331543
Training iter #1600000:   Batch Loss = 10.114140, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.90661334991455, Accuracy = 0.8863080739974976
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4000
Training iter #1602000:   Batch Loss = 9.832347, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.887079238891602, Accuracy = 0.878973126411438
Training iter #1604000:   Batch Loss = 10.268896, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.743035316467285, Accuracy = 0.8887530565261841
Training iter #1606000:   Batch Loss = 10.276271, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.625642776489258, Accuracy = 0.8911980390548706
Training iter #1608000:   Batch Loss = 10.772472, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.609708786010742, Accuracy = 0.8704156279563904
Training iter #1610000:   Batch Loss = 10.815185, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.043495178222656, Accuracy = 0.8856968283653259
Training iter #1612000:   Batch Loss = 10.382162, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.152284622192383, Accuracy = 0.8820293545722961
Training iter #1614000:   Batch Loss = 10.581573, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.945096969604492, Accuracy = 0.878973126411438
Training iter #1616000:   Batch Loss = 10.073847, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.815762519836426, Accuracy = 0.8887530565261841
Training iter #1618000:   Batch Loss = 9.578555, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.637746810913086, Accuracy = 0.8795843720436096
Training iter #1620000:   Batch Loss = 10.293468, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.903496742248535, Accuracy = 0.8869193196296692
Training iter #1622000:   Batch Loss = 9.967851, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.723995208740234, Accuracy = 0.8814181089401245
Training iter #1624000:   Batch Loss = 10.165998, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.730598449707031, Accuracy = 0.8911980390548706
Training iter #1626000:   Batch Loss = 11.347300, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.826351165771484, Accuracy = 0.8734718561172485
Training iter #1628000:   Batch Loss = 11.107457, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.478129386901855, Accuracy = 0.8728606104850769
Training iter #1630000:   Batch Loss = 9.896606, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.060447692871094, Accuracy = 0.8832518458366394
Training iter #1632000:   Batch Loss = 12.121432, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.916543960571289, Accuracy = 0.8820293545722961
Training iter #1634000:   Batch Loss = 10.087087, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.806056022644043, Accuracy = 0.8783618807792664
Training iter #1636000:   Batch Loss = 9.696095, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.566880226135254, Accuracy = 0.8826406002044678
Training iter #1638000:   Batch Loss = 9.758964, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.54690933227539, Accuracy = 0.8777506351470947
Training iter #1640000:   Batch Loss = 9.834570, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.381698608398438, Accuracy = 0.8856968283653259
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4100
Training iter #1642000:   Batch Loss = 9.395089, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.267997741699219, Accuracy = 0.878973126411438
Training iter #1644000:   Batch Loss = 10.041624, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.179524421691895, Accuracy = 0.8801956176757812
Training iter #1646000:   Batch Loss = 9.788901, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.380180358886719, Accuracy = 0.8899755477905273
Training iter #1648000:   Batch Loss = 9.665766, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.466349601745605, Accuracy = 0.8911980390548706
Training iter #1650000:   Batch Loss = 9.491919, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.378395080566406, Accuracy = 0.8832518458366394
Training iter #1652000:   Batch Loss = 9.486724, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.315391540527344, Accuracy = 0.8936430215835571
Training iter #1654000:   Batch Loss = 9.774439, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.73747730255127, Accuracy = 0.8875305652618408
Training iter #1656000:   Batch Loss = 9.665761, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.557252883911133, Accuracy = 0.8881418108940125
Training iter #1658000:   Batch Loss = 9.505650, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.438180923461914, Accuracy = 0.8850855827331543
Training iter #1660000:   Batch Loss = 9.711939, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.572638511657715, Accuracy = 0.8863080739974976
Training iter #1662000:   Batch Loss = 9.559673, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.196588516235352, Accuracy = 0.8918092846870422
Training iter #1664000:   Batch Loss = 9.466104, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.085397720336914, Accuracy = 0.8924205303192139
Training iter #1666000:   Batch Loss = 9.495590, Accuracy = 0.9411764740943909
PERFORMANCE ON TEST SET: Batch Loss = 11.096885681152344, Accuracy = 0.8850855827331543
Training iter #1668000:   Batch Loss = 9.565182, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.44556999206543, Accuracy = 0.878973126411438
Training iter #1670000:   Batch Loss = 9.793358, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.074281692504883, Accuracy = 0.8893643021583557
Training iter #1672000:   Batch Loss = 9.350904, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.115882873535156, Accuracy = 0.8924205303192139
Training iter #1674000:   Batch Loss = 9.855868, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.366276741027832, Accuracy = 0.8887530565261841
Training iter #1676000:   Batch Loss = 9.842264, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.234010696411133, Accuracy = 0.8808068633079529
Training iter #1678000:   Batch Loss = 9.817155, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.18983268737793, Accuracy = 0.8863080739974976
Training iter #1680000:   Batch Loss = 9.650785, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.249015808105469, Accuracy = 0.8856968283653259
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4200
Training iter #1682000:   Batch Loss = 10.162817, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.327559471130371, Accuracy = 0.8801956176757812
Training iter #1684000:   Batch Loss = 9.530124, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.210468292236328, Accuracy = 0.890586793422699
Training iter #1686000:   Batch Loss = 9.455126, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.960532188415527, Accuracy = 0.8795843720436096
Training iter #1688000:   Batch Loss = 9.228666, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.089478492736816, Accuracy = 0.8881418108940125
Training iter #1690000:   Batch Loss = 9.210011, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.991741180419922, Accuracy = 0.8856968283653259
Training iter #1692000:   Batch Loss = 9.090742, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.814969062805176, Accuracy = 0.8875305652618408
Training iter #1694000:   Batch Loss = 9.481136, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.020137786865234, Accuracy = 0.8808068633079529
Training iter #1696000:   Batch Loss = 9.265421, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.990213394165039, Accuracy = 0.8991442322731018
Training iter #1698000:   Batch Loss = 9.384509, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.181321144104004, Accuracy = 0.8960880041122437
Training iter #1700000:   Batch Loss = 10.522296, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 12.403238296508789, Accuracy = 0.8997554779052734
Training iter #1702000:   Batch Loss = 11.724346, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.487528800964355, Accuracy = 0.8856968283653259
Training iter #1704000:   Batch Loss = 10.933613, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.304883003234863, Accuracy = 0.8826406002044678
Training iter #1706000:   Batch Loss = 10.690636, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.088338851928711, Accuracy = 0.8911980390548706
Training iter #1708000:   Batch Loss = 10.994741, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.588404655456543, Accuracy = 0.8734718561172485
Training iter #1710000:   Batch Loss = 10.876728, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.46174430847168, Accuracy = 0.883863091468811
Training iter #1712000:   Batch Loss = 10.148572, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.082925796508789, Accuracy = 0.8869193196296692
Training iter #1714000:   Batch Loss = 10.524549, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.396001815795898, Accuracy = 0.878973126411438
Training iter #1716000:   Batch Loss = 10.827135, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.211349487304688, Accuracy = 0.8814181089401245
Training iter #1718000:   Batch Loss = 10.357184, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.78879165649414, Accuracy = 0.8856968283653259
Training iter #1720000:   Batch Loss = 9.945939, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.611527442932129, Accuracy = 0.883863091468811
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4300
Training iter #1722000:   Batch Loss = 9.936584, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.500950813293457, Accuracy = 0.8875305652618408
Training iter #1724000:   Batch Loss = 10.226346, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.432445526123047, Accuracy = 0.8942542672157288
Training iter #1726000:   Batch Loss = 9.718348, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.219223022460938, Accuracy = 0.8991442322731018
Training iter #1728000:   Batch Loss = 9.856165, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.476957321166992, Accuracy = 0.8826406002044678
Training iter #1730000:   Batch Loss = 9.422664, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.384073257446289, Accuracy = 0.8881418108940125
Training iter #1732000:   Batch Loss = 9.907103, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.081464767456055, Accuracy = 0.8979217410087585
Training iter #1734000:   Batch Loss = 9.501094, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.090234756469727, Accuracy = 0.8936430215835571
Training iter #1736000:   Batch Loss = 9.458439, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.970647811889648, Accuracy = 0.8948655128479004
Training iter #1738000:   Batch Loss = 8.734310, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.917220115661621, Accuracy = 0.8960880041122437
Training iter #1740000:   Batch Loss = 9.352036, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.89522933959961, Accuracy = 0.8936430215835571
Training iter #1742000:   Batch Loss = 9.368212, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.010393142700195, Accuracy = 0.8979217410087585
Training iter #1744000:   Batch Loss = 9.354183, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.881628036499023, Accuracy = 0.8966992497444153
Training iter #1746000:   Batch Loss = 9.178871, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.715639114379883, Accuracy = 0.8973104953765869
Training iter #1748000:   Batch Loss = 9.225229, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.025918960571289, Accuracy = 0.8869193196296692
Training iter #1750000:   Batch Loss = 9.240746, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.920101165771484, Accuracy = 0.890586793422699
Training iter #1752000:   Batch Loss = 9.208552, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.745702743530273, Accuracy = 0.895476758480072
Training iter #1754000:   Batch Loss = 8.915970, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.85293960571289, Accuracy = 0.8918092846870422
Training iter #1756000:   Batch Loss = 9.361551, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.166150093078613, Accuracy = 0.8936430215835571
Training iter #1758000:   Batch Loss = 8.899253, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.916635513305664, Accuracy = 0.8911980390548706
Training iter #1760000:   Batch Loss = 9.368286, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.746286392211914, Accuracy = 0.8893643021583557
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4400
Training iter #1762000:   Batch Loss = 9.207597, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.121400833129883, Accuracy = 0.8795843720436096
Training iter #1764000:   Batch Loss = 9.171177, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.993241310119629, Accuracy = 0.8814181089401245
Training iter #1766000:   Batch Loss = 9.099653, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.728489875793457, Accuracy = 0.8801956176757812
Training iter #1768000:   Batch Loss = 9.082009, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.7369384765625, Accuracy = 0.8924205303192139
Training iter #1770000:   Batch Loss = 8.914352, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.575130462646484, Accuracy = 0.8966992497444153
Training iter #1772000:   Batch Loss = 9.249535, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.693119049072266, Accuracy = 0.8997554779052734
Training iter #1774000:   Batch Loss = 9.538347, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.975286483764648, Accuracy = 0.8869193196296692
Training iter #1776000:   Batch Loss = 9.165489, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.957401275634766, Accuracy = 0.8844743371009827
Training iter #1778000:   Batch Loss = 9.328226, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.823776245117188, Accuracy = 0.8820293545722961
Training iter #1780000:   Batch Loss = 9.237440, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.620599746704102, Accuracy = 0.8826406002044678
Training iter #1782000:   Batch Loss = 9.536558, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.846013069152832, Accuracy = 0.8753056526184082
Training iter #1784000:   Batch Loss = 10.128987, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.251577377319336, Accuracy = 0.8887530565261841
Training iter #1786000:   Batch Loss = 9.455458, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.081302642822266, Accuracy = 0.8911980390548706
Training iter #1788000:   Batch Loss = 9.235043, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.882643699645996, Accuracy = 0.8911980390548706
Training iter #1790000:   Batch Loss = 9.678868, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.234879493713379, Accuracy = 0.8826406002044678
Training iter #1792000:   Batch Loss = 9.436260, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.001738548278809, Accuracy = 0.8930317759513855
Training iter #1794000:   Batch Loss = 9.611106, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.217643737792969, Accuracy = 0.8893643021583557
Training iter #1796000:   Batch Loss = 9.826794, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.361507415771484, Accuracy = 0.8698043823242188
Training iter #1798000:   Batch Loss = 9.462524, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.990998268127441, Accuracy = 0.8875305652618408
Training iter #1800000:   Batch Loss = 9.509088, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.838685989379883, Accuracy = 0.8918092846870422
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4500
Training iter #1802000:   Batch Loss = 8.399386, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.731889724731445, Accuracy = 0.8869193196296692
Training iter #1804000:   Batch Loss = 8.736065, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.692668914794922, Accuracy = 0.8911980390548706
Training iter #1806000:   Batch Loss = 9.121861, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.025014877319336, Accuracy = 0.8918092846870422
Training iter #1808000:   Batch Loss = 9.691072, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.985420227050781, Accuracy = 0.8918092846870422
Training iter #1810000:   Batch Loss = 9.470183, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.878377914428711, Accuracy = 0.8887530565261841
Training iter #1812000:   Batch Loss = 8.961752, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.894254684448242, Accuracy = 0.8966992497444153
Training iter #1814000:   Batch Loss = 8.819752, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.802118301391602, Accuracy = 0.8814181089401245
Training iter #1816000:   Batch Loss = 8.962009, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.717085838317871, Accuracy = 0.8881418108940125
Training iter #1818000:   Batch Loss = 8.954098, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.631532669067383, Accuracy = 0.8911980390548706
Training iter #1820000:   Batch Loss = 8.801895, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.54346752166748, Accuracy = 0.8887530565261841
Training iter #1822000:   Batch Loss = 8.293580, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.419815063476562, Accuracy = 0.8997554779052734
Training iter #1824000:   Batch Loss = 8.366688, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.445290565490723, Accuracy = 0.9009779691696167
Training iter #1826000:   Batch Loss = 8.723563, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.55651569366455, Accuracy = 0.8930317759513855
Training iter #1828000:   Batch Loss = 8.568762, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.380932807922363, Accuracy = 0.9046455025672913
Training iter #1830000:   Batch Loss = 10.317778, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.976818084716797, Accuracy = 0.8875305652618408
Training iter #1832000:   Batch Loss = 9.648508, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.453032493591309, Accuracy = 0.8924205303192139
Training iter #1834000:   Batch Loss = 9.245693, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.398401260375977, Accuracy = 0.8930317759513855
Training iter #1836000:   Batch Loss = 10.044388, Accuracy = 0.8823529481887817
PERFORMANCE ON TEST SET: Batch Loss = 11.360757827758789, Accuracy = 0.8801956176757812
Training iter #1838000:   Batch Loss = 9.269126, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.3021879196167, Accuracy = 0.8832518458366394
Training iter #1840000:   Batch Loss = 9.241411, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.130126953125, Accuracy = 0.8930317759513855
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4600
Training iter #1842000:   Batch Loss = 9.219765, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.001564025878906, Accuracy = 0.8936430215835571
Training iter #1844000:   Batch Loss = 9.457230, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.17354965209961, Accuracy = 0.8783618807792664
Training iter #1846000:   Batch Loss = 9.006766, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.059146881103516, Accuracy = 0.890586793422699
Training iter #1848000:   Batch Loss = 10.144178, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.652132034301758, Accuracy = 0.9003667235374451
Training iter #1850000:   Batch Loss = 9.414486, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.749747276306152, Accuracy = 0.8765281438827515
Training iter #1852000:   Batch Loss = 9.984334, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.50253677368164, Accuracy = 0.8863080739974976
Training iter #1854000:   Batch Loss = 9.757366, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.512280464172363, Accuracy = 0.8863080739974976
Training iter #1856000:   Batch Loss = 9.425879, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.409774780273438, Accuracy = 0.8930317759513855
Training iter #1858000:   Batch Loss = 9.342264, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.517467498779297, Accuracy = 0.8820293545722961
Training iter #1860000:   Batch Loss = 9.088304, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.211358070373535, Accuracy = 0.8924205303192139
Training iter #1862000:   Batch Loss = 9.760572, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.24226188659668, Accuracy = 0.8942542672157288
Training iter #1864000:   Batch Loss = 10.616836, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.952436447143555, Accuracy = 0.8704156279563904
Training iter #1866000:   Batch Loss = 10.506435, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.9775390625, Accuracy = 0.8973104953765869
Training iter #1868000:   Batch Loss = 9.447010, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.448223114013672, Accuracy = 0.8930317759513855
Training iter #1870000:   Batch Loss = 9.767658, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.389963150024414, Accuracy = 0.8820293545722961
Training iter #1872000:   Batch Loss = 9.822189, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.268024444580078, Accuracy = 0.8911980390548706
Training iter #1874000:   Batch Loss = 9.359379, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.519163131713867, Accuracy = 0.8863080739974976
Training iter #1876000:   Batch Loss = 9.782927, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.38668155670166, Accuracy = 0.8930317759513855
Training iter #1878000:   Batch Loss = 9.807418, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.922295570373535, Accuracy = 0.8863080739974976
Training iter #1880000:   Batch Loss = 9.868367, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.424365997314453, Accuracy = 0.8924205303192139
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4700
Training iter #1882000:   Batch Loss = 10.143067, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.929872512817383, Accuracy = 0.8899755477905273
Training iter #1884000:   Batch Loss = 9.798354, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.693855285644531, Accuracy = 0.8863080739974976
Training iter #1886000:   Batch Loss = 9.857559, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.55409049987793, Accuracy = 0.8753056526184082
Training iter #1888000:   Batch Loss = 10.185295, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.359130859375, Accuracy = 0.8808068633079529
Training iter #1890000:   Batch Loss = 9.306480, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.058809280395508, Accuracy = 0.8869193196296692
Training iter #1892000:   Batch Loss = 9.984388, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.163620948791504, Accuracy = 0.8814181089401245
Training iter #1894000:   Batch Loss = 9.261127, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.878009796142578, Accuracy = 0.8881418108940125
Training iter #1896000:   Batch Loss = 9.607912, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.786500930786133, Accuracy = 0.883863091468811
Training iter #1898000:   Batch Loss = 9.031839, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.804150581359863, Accuracy = 0.8911980390548706
Training iter #1900000:   Batch Loss = 9.031651, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.769730567932129, Accuracy = 0.8930317759513855
Training iter #1902000:   Batch Loss = 8.986986, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.79659366607666, Accuracy = 0.8893643021583557
Training iter #1904000:   Batch Loss = 7.664372, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.02797794342041, Accuracy = 0.8832518458366394
Training iter #1906000:   Batch Loss = 9.220592, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.106060028076172, Accuracy = 0.8911980390548706
Training iter #1908000:   Batch Loss = 9.170000, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.0067138671875, Accuracy = 0.8869193196296692
Training iter #1910000:   Batch Loss = 9.154179, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.924933433532715, Accuracy = 0.8918092846870422
Training iter #1912000:   Batch Loss = 9.605453, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.402098655700684, Accuracy = 0.8740831017494202
Training iter #1914000:   Batch Loss = 9.362432, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.137323379516602, Accuracy = 0.8820293545722961
Training iter #1916000:   Batch Loss = 8.674001, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.054499626159668, Accuracy = 0.8875305652618408
Training iter #1918000:   Batch Loss = 8.948510, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.19240951538086, Accuracy = 0.8930317759513855
Training iter #1920000:   Batch Loss = 9.352576, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.13099479675293, Accuracy = 0.8826406002044678
Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-4800
Training iter #1922000:   Batch Loss = 9.205791, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.855452537536621, Accuracy = 0.8808068633079529
Training iter #1924000:   Batch Loss = 9.270375, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.719734191894531, Accuracy = 0.8875305652618408
Optimization Finished!
FINAL RESULT: Batch Loss = 10.875726699829102, Accuracy = 0.8875305652618408
All train time = 17040.28940677643
Final Model saved in file: ./lstm2/model_mergeall_kfold0.ckpt-final
Precision: 89.02625059898467%
Recall: 88.75305623471883%
f1_score: 88.78815385989223%

Confusion Matrix:
[[149   0   0   1   0   0   4   3   4   3]
 [  6 147   0   0   0   0   3   1   6   1]
 [  2   0 130   4   0   0  11   5  12   0]
 [  1   0   0 148   4   0   0  11   0   1]
 [  2   0   0   1 160   0   0   0   0   0]
 [  0   0   1   0   0 164   0   0   0   0]
 [  3   0   0   2   0   0 143  14   0   1]
 [  3   0   1  15   0   0  20 125   0   0]
 [  0   0  18   2   1   0   7   1 129   0]
 [  5   0   0   0   0   3   0   1   0 157]]

Confusion matrix (normalised to % of total test data):
[[ 9.107579    0.          0.          0.0611247   0.          0.
   0.24449879  0.18337409  0.24449879  0.18337409]
 [ 0.36674818  8.985331    0.          0.          0.          0.
   0.18337409  0.0611247   0.36674818  0.0611247 ]
 [ 0.12224939  0.          7.9462104   0.24449879  0.          0.
   0.6723716   0.30562347  0.73349637  0.        ]
 [ 0.0611247   0.          0.          9.046454    0.24449879  0.
   0.          0.6723716   0.          0.0611247 ]
 [ 0.12224939  0.          0.          0.0611247   9.779951    0.
   0.          0.          0.          0.        ]
 [ 0.          0.          0.0611247   0.          0.         10.02445
   0.          0.          0.          0.        ]
 [ 0.18337409  0.          0.          0.12224939  0.          0.
   8.740831    0.85574573  0.          0.0611247 ]
 [ 0.18337409  0.          0.0611247   0.9168704   0.          0.
   1.2224939   7.640587    0.          0.        ]
 [ 0.          0.          1.1002445   0.12224939  0.0611247   0.
   0.42787287  0.0611247   7.8850856   0.        ]
 [ 0.30562347  0.          0.          0.          0.          0.18337409
   0.          0.0611247   0.          9.596578  ]]/home/sunrepe/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))

Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.

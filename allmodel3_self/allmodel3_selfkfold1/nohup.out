WARNING:tensorflow:From all_three_lstm.py:91: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:93: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From all_three_lstm.py:94: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From all_three_lstm.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From all_three_lstm.py:97: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
loading data...
train: 6446 test: 1607
load data time: 187.0419316291809
Start train!
Training iter #400:   Batch Loss = 26.855629, Accuracy = 0.0949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 26.39413070678711, Accuracy = 0.15183572471141815
Training iter #2000:   Batch Loss = 25.322369, Accuracy = 0.3100000023841858
PERFORMANCE ON TEST SET: Batch Loss = 24.984375, Accuracy = 0.3478531539440155
Training iter #4000:   Batch Loss = 23.614878, Accuracy = 0.3675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 23.23412322998047, Accuracy = 0.3739888072013855
Training iter #6000:   Batch Loss = 21.942186, Accuracy = 0.3449999988079071
PERFORMANCE ON TEST SET: Batch Loss = 21.659414291381836, Accuracy = 0.3739888072013855
Training iter #8000:   Batch Loss = 20.610970, Accuracy = 0.47999998927116394
PERFORMANCE ON TEST SET: Batch Loss = 20.46763801574707, Accuracy = 0.46857497096061707
Training iter #10000:   Batch Loss = 19.460419, Accuracy = 0.5425000190734863
PERFORMANCE ON TEST SET: Batch Loss = 19.33901023864746, Accuracy = 0.5594274997711182
Training iter #12000:   Batch Loss = 18.345686, Accuracy = 0.5899999737739563
PERFORMANCE ON TEST SET: Batch Loss = 18.26724624633789, Accuracy = 0.6210329532623291
Training iter #14000:   Batch Loss = 17.802547, Accuracy = 0.625
PERFORMANCE ON TEST SET: Batch Loss = 17.561447143554688, Accuracy = 0.6365899443626404
Training iter #16000:   Batch Loss = 16.717060, Accuracy = 0.6899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 16.913185119628906, Accuracy = 0.6421903967857361
Training iter #18000:   Batch Loss = 16.884460, Accuracy = 0.5924999713897705
PERFORMANCE ON TEST SET: Batch Loss = 16.239105224609375, Accuracy = 0.6652146577835083
Training iter #20000:   Batch Loss = 15.322273, Accuracy = 0.6675000190734863
PERFORMANCE ON TEST SET: Batch Loss = 15.512628555297852, Accuracy = 0.6932171583175659
Training iter #22000:   Batch Loss = 15.182159, Accuracy = 0.7074999809265137
PERFORMANCE ON TEST SET: Batch Loss = 15.364673614501953, Accuracy = 0.6963285803794861
Training iter #24000:   Batch Loss = 15.301811, Accuracy = 0.6650000214576721
PERFORMANCE ON TEST SET: Batch Loss = 14.946792602539062, Accuracy = 0.7044181823730469
Training iter #26000:   Batch Loss = 15.362389, Accuracy = 0.7099999785423279
PERFORMANCE ON TEST SET: Batch Loss = 14.664651870727539, Accuracy = 0.7181082963943481
Training iter #28000:   Batch Loss = 14.580937, Accuracy = 0.7174999713897705
PERFORMANCE ON TEST SET: Batch Loss = 14.459257125854492, Accuracy = 0.7131300568580627
Training iter #30000:   Batch Loss = 14.104273, Accuracy = 0.699999988079071
PERFORMANCE ON TEST SET: Batch Loss = 14.231294631958008, Accuracy = 0.7143746018409729
Training iter #32000:   Batch Loss = 14.148546, Accuracy = 0.7174999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.757583618164062, Accuracy = 0.7305538058280945
Training iter #34000:   Batch Loss = 12.238708, Accuracy = 0.760869562625885
PERFORMANCE ON TEST SET: Batch Loss = 13.573321342468262, Accuracy = 0.7330429553985596
Training iter #36000:   Batch Loss = 13.866602, Accuracy = 0.7124999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.342032432556152, Accuracy = 0.746110737323761
Training iter #38000:   Batch Loss = 12.565722, Accuracy = 0.7524999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.359126091003418, Accuracy = 0.7560672163963318
Training iter #40000:   Batch Loss = 13.420171, Accuracy = 0.7699999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.275392532348633, Accuracy = 0.7492221593856812
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-100
Training iter #42000:   Batch Loss = 13.735671, Accuracy = 0.7549999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.423280715942383, Accuracy = 0.73677659034729
Training iter #44000:   Batch Loss = 13.382137, Accuracy = 0.7475000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.082998275756836, Accuracy = 0.7529557943344116
Training iter #46000:   Batch Loss = 12.537531, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.930190086364746, Accuracy = 0.7635345458984375
Training iter #48000:   Batch Loss = 12.360510, Accuracy = 0.7900000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.964111328125, Accuracy = 0.7629122734069824
Training iter #50000:   Batch Loss = 12.697882, Accuracy = 0.7749999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.531593322753906, Accuracy = 0.7728686928749084
Training iter #52000:   Batch Loss = 11.541780, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.399287223815918, Accuracy = 0.7697573304176331
Training iter #54000:   Batch Loss = 12.130818, Accuracy = 0.7825000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.238211631774902, Accuracy = 0.779091477394104
Training iter #56000:   Batch Loss = 12.184538, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.240655899047852, Accuracy = 0.7871810793876648
Training iter #58000:   Batch Loss = 12.083779, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.057282447814941, Accuracy = 0.7815805673599243
Training iter #60000:   Batch Loss = 11.659254, Accuracy = 0.7774999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.230339050292969, Accuracy = 0.7853142619132996
Training iter #62000:   Batch Loss = 11.916262, Accuracy = 0.7900000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.128488540649414, Accuracy = 0.79091477394104
Training iter #64000:   Batch Loss = 11.833179, Accuracy = 0.8025000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.248998641967773, Accuracy = 0.7840697169303894
Training iter #66000:   Batch Loss = 11.878989, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.957401275634766, Accuracy = 0.7977597713470459
Training iter #68000:   Batch Loss = 11.703402, Accuracy = 0.804347813129425
PERFORMANCE ON TEST SET: Batch Loss = 11.795281410217285, Accuracy = 0.7902924418449402
Training iter #70000:   Batch Loss = 11.721907, Accuracy = 0.7900000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.081769943237305, Accuracy = 0.7772246599197388
Training iter #72000:   Batch Loss = 12.114698, Accuracy = 0.762499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.046195983886719, Accuracy = 0.7840697169303894
Training iter #74000:   Batch Loss = 11.638090, Accuracy = 0.8025000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.042791366577148, Accuracy = 0.7915370464324951
Training iter #76000:   Batch Loss = 11.386945, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.734268188476562, Accuracy = 0.7871810793876648
Training iter #78000:   Batch Loss = 11.334469, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.555523872375488, Accuracy = 0.7977597713470459
Training iter #80000:   Batch Loss = 11.233929, Accuracy = 0.8100000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.427135467529297, Accuracy = 0.8089607954025269
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-200
Training iter #82000:   Batch Loss = 11.291994, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.761958122253418, Accuracy = 0.7983821034431458
Training iter #84000:   Batch Loss = 11.129393, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.746185302734375, Accuracy = 0.7990043759346008
Training iter #86000:   Batch Loss = 11.928892, Accuracy = 0.7774999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.80630874633789, Accuracy = 0.8077162504196167
Training iter #88000:   Batch Loss = 11.137527, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.51201057434082, Accuracy = 0.8070939779281616
Training iter #90000:   Batch Loss = 11.172059, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.876753807067871, Accuracy = 0.8151835799217224
Training iter #92000:   Batch Loss = 11.566515, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.883368492126465, Accuracy = 0.8126944899559021
Training iter #94000:   Batch Loss = 11.517621, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.000313758850098, Accuracy = 0.8201618194580078
Training iter #96000:   Batch Loss = 11.952506, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.186941146850586, Accuracy = 0.8083385229110718
Training iter #98000:   Batch Loss = 11.854376, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.672914505004883, Accuracy = 0.8108276128768921
Training iter #100000:   Batch Loss = 11.356514, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 11.485276222229004, Accuracy = 0.810205340385437
Training iter #102000:   Batch Loss = 12.292426, Accuracy = 0.804347813129425
PERFORMANCE ON TEST SET: Batch Loss = 11.691627502441406, Accuracy = 0.8133167624473572
Training iter #104000:   Batch Loss = 11.542618, Accuracy = 0.8324999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.886293411254883, Accuracy = 0.8120721578598022
Training iter #106000:   Batch Loss = 11.389328, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.515152931213379, Accuracy = 0.8226509094238281
Training iter #108000:   Batch Loss = 11.309690, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.434457778930664, Accuracy = 0.8238954544067383
Training iter #110000:   Batch Loss = 11.282154, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.674275398254395, Accuracy = 0.8108276128768921
Training iter #112000:   Batch Loss = 11.297271, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.453664779663086, Accuracy = 0.8238954544067383
Training iter #114000:   Batch Loss = 10.906443, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.410325050354004, Accuracy = 0.8288736939430237
Training iter #116000:   Batch Loss = 10.677463, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.545151710510254, Accuracy = 0.8207840919494629
Training iter #118000:   Batch Loss = 11.087296, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.221880912780762, Accuracy = 0.8419415354728699
Training iter #120000:   Batch Loss = 10.442661, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.953615188598633, Accuracy = 0.8332296013832092
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-300
Training iter #122000:   Batch Loss = 10.443309, Accuracy = 0.8324999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.840133666992188, Accuracy = 0.8307405114173889
Training iter #124000:   Batch Loss = 11.277184, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.424790382385254, Accuracy = 0.8357187509536743
Training iter #126000:   Batch Loss = 10.791625, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.12816047668457, Accuracy = 0.8375855684280396
Training iter #128000:   Batch Loss = 10.383237, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.18302059173584, Accuracy = 0.8369632959365845
Training iter #130000:   Batch Loss = 10.457428, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.187787055969238, Accuracy = 0.8288736939430237
Training iter #132000:   Batch Loss = 10.454294, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.888215065002441, Accuracy = 0.8406969308853149
Training iter #134000:   Batch Loss = 10.505750, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.731473922729492, Accuracy = 0.8444306254386902
Training iter #136000:   Batch Loss = 9.210158, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 10.919103622436523, Accuracy = 0.84131920337677
Training iter #138000:   Batch Loss = 10.244371, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.833080291748047, Accuracy = 0.8419415354728699
Training iter #140000:   Batch Loss = 9.826712, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.764059066772461, Accuracy = 0.8456751704216003
Training iter #142000:   Batch Loss = 10.434366, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.85397720336914, Accuracy = 0.8450528979301453
Training iter #144000:   Batch Loss = 10.553569, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.944788932800293, Accuracy = 0.8450528979301453
Training iter #146000:   Batch Loss = 10.960278, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.78034782409668, Accuracy = 0.8375855684280396
Training iter #148000:   Batch Loss = 10.213188, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.668078422546387, Accuracy = 0.8394523859024048
Training iter #150000:   Batch Loss = 11.750701, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.49380111694336, Accuracy = 0.819539487361908
Training iter #152000:   Batch Loss = 12.127360, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.190863609313965, Accuracy = 0.8394523859024048
Training iter #154000:   Batch Loss = 11.341142, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.581433296203613, Accuracy = 0.852520227432251
Training iter #156000:   Batch Loss = 11.070636, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.437173843383789, Accuracy = 0.8512756824493408
Training iter #158000:   Batch Loss = 10.710073, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.13990306854248, Accuracy = 0.8475419878959656
Training iter #160000:   Batch Loss = 10.602734, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.090392112731934, Accuracy = 0.8444306254386902
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-400
Training iter #162000:   Batch Loss = 10.594630, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.113746643066406, Accuracy = 0.8537647724151611
Training iter #164000:   Batch Loss = 10.474407, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.353992462158203, Accuracy = 0.8438083529472351
Training iter #166000:   Batch Loss = 11.141829, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.221024513244629, Accuracy = 0.8382078409194946
Training iter #168000:   Batch Loss = 10.395031, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.023839950561523, Accuracy = 0.8469197154045105
Training iter #170000:   Batch Loss = 10.186952, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 10.873300552368164, Accuracy = 0.8618543744087219
Training iter #172000:   Batch Loss = 10.252586, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.794244766235352, Accuracy = 0.8655880689620972
Training iter #174000:   Batch Loss = 9.940743, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.611869812011719, Accuracy = 0.8574984669685364
Training iter #176000:   Batch Loss = 9.687875, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.589875221252441, Accuracy = 0.8568761944770813
Training iter #178000:   Batch Loss = 9.975493, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.622810363769531, Accuracy = 0.8562538623809814
Training iter #180000:   Batch Loss = 9.463640, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.74092960357666, Accuracy = 0.8581207394599915
Training iter #182000:   Batch Loss = 10.321801, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.71825122833252, Accuracy = 0.8606098294258118
Training iter #184000:   Batch Loss = 10.538260, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.658275604248047, Accuracy = 0.8599875569343567
Training iter #186000:   Batch Loss = 10.493278, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.906368255615234, Accuracy = 0.8556315898895264
Training iter #188000:   Batch Loss = 10.050461, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.747208595275879, Accuracy = 0.8562538623809814
Training iter #190000:   Batch Loss = 9.886018, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.752334594726562, Accuracy = 0.8543870449066162
Training iter #192000:   Batch Loss = 10.161990, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.700483322143555, Accuracy = 0.8518979549407959
Training iter #194000:   Batch Loss = 10.560363, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.615479469299316, Accuracy = 0.8537647724151611
Training iter #196000:   Batch Loss = 9.769187, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.74159049987793, Accuracy = 0.8568761944770813
Training iter #198000:   Batch Loss = 9.718210, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.575132369995117, Accuracy = 0.8593652844429016
Training iter #200000:   Batch Loss = 9.592367, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.432052612304688, Accuracy = 0.8581207394599915
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-500
Training iter #202000:   Batch Loss = 9.937243, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.392274856567383, Accuracy = 0.8630989193916321
Training iter #204000:   Batch Loss = 9.502019, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 10.735967636108398, Accuracy = 0.8662103414535522
Training iter #206000:   Batch Loss = 10.629388, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.99476432800293, Accuracy = 0.8649657964706421
Training iter #208000:   Batch Loss = 9.609694, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.562091827392578, Accuracy = 0.8593652844429016
Training iter #210000:   Batch Loss = 9.244033, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.717683792114258, Accuracy = 0.8562538623809814
Training iter #212000:   Batch Loss = 9.996927, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.576058387756348, Accuracy = 0.8730553984642029
Training iter #214000:   Batch Loss = 9.823915, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.755809783935547, Accuracy = 0.8593652844429016
Training iter #216000:   Batch Loss = 9.360273, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.595574378967285, Accuracy = 0.8618543744087219
Training iter #218000:   Batch Loss = 10.263631, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.704161643981934, Accuracy = 0.8705662488937378
Training iter #220000:   Batch Loss = 10.426934, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.677896499633789, Accuracy = 0.8618543744087219
Training iter #222000:   Batch Loss = 10.150046, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.432126998901367, Accuracy = 0.8655880689620972
Training iter #224000:   Batch Loss = 9.405338, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.132786750793457, Accuracy = 0.8755444884300232
Training iter #226000:   Batch Loss = 9.562574, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.17762279510498, Accuracy = 0.8699439764022827
Training iter #228000:   Batch Loss = 9.231649, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.16319465637207, Accuracy = 0.8662103414535522
Training iter #230000:   Batch Loss = 9.396414, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.019250869750977, Accuracy = 0.8749222159385681
Training iter #232000:   Batch Loss = 9.093018, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.238033294677734, Accuracy = 0.8724331259727478
Training iter #234000:   Batch Loss = 9.963752, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.94957447052002, Accuracy = 0.8761667609214783
Training iter #236000:   Batch Loss = 10.009632, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.77655029296875, Accuracy = 0.8761667609214783
Training iter #238000:   Batch Loss = 11.512731, Accuracy = 0.8260869383811951
PERFORMANCE ON TEST SET: Batch Loss = 10.761133193969727, Accuracy = 0.8842563629150391
Training iter #240000:   Batch Loss = 10.029772, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.673789024353027, Accuracy = 0.8761667609214783
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-600
Training iter #242000:   Batch Loss = 9.794701, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.634771347045898, Accuracy = 0.8662103414535522
Training iter #244000:   Batch Loss = 9.577028, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.411697387695312, Accuracy = 0.8774113059043884
Training iter #246000:   Batch Loss = 9.379418, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.633508682250977, Accuracy = 0.8718108534812927
Training iter #248000:   Batch Loss = 9.732239, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.545281410217285, Accuracy = 0.8699439764022827
Training iter #250000:   Batch Loss = 9.756055, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.288803100585938, Accuracy = 0.8755444884300232
Training iter #252000:   Batch Loss = 10.405825, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.088066101074219, Accuracy = 0.8680771589279175
Training iter #254000:   Batch Loss = 9.669452, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.827922821044922, Accuracy = 0.8630989193916321
Training iter #256000:   Batch Loss = 9.970770, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.942455291748047, Accuracy = 0.8612321019172668
Training iter #258000:   Batch Loss = 9.838123, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.740665435791016, Accuracy = 0.8630989193916321
Training iter #260000:   Batch Loss = 9.921532, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.709993362426758, Accuracy = 0.8674548864364624
Training iter #262000:   Batch Loss = 9.710093, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.462919235229492, Accuracy = 0.8662103414535522
Training iter #264000:   Batch Loss = 10.441612, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.30495834350586, Accuracy = 0.8699439764022827
Training iter #266000:   Batch Loss = 9.747195, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.451374053955078, Accuracy = 0.8612321019172668
Training iter #268000:   Batch Loss = 10.003092, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.24286937713623, Accuracy = 0.8780335783958435
Training iter #270000:   Batch Loss = 9.170935, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.281063079833984, Accuracy = 0.8761667609214783
Training iter #272000:   Batch Loss = 10.256278, Accuracy = 0.8478260636329651
PERFORMANCE ON TEST SET: Batch Loss = 10.225831985473633, Accuracy = 0.8767890334129333
Training iter #274000:   Batch Loss = 9.477789, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.36927604675293, Accuracy = 0.8774113059043884
Training iter #276000:   Batch Loss = 9.241150, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.282593727111816, Accuracy = 0.8749222159385681
Training iter #278000:   Batch Loss = 9.660811, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.32314395904541, Accuracy = 0.8780335783958435
Training iter #280000:   Batch Loss = 10.081133, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.51141357421875, Accuracy = 0.8718108534812927
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-700
Training iter #282000:   Batch Loss = 9.758915, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.414653778076172, Accuracy = 0.8786558508872986
Training iter #284000:   Batch Loss = 9.362164, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.360767364501953, Accuracy = 0.8786558508872986
Training iter #286000:   Batch Loss = 9.361063, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.597646713256836, Accuracy = 0.8680771589279175
Training iter #288000:   Batch Loss = 9.405028, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.377927780151367, Accuracy = 0.8718108534812927
Training iter #290000:   Batch Loss = 9.155210, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.07295036315918, Accuracy = 0.8867455124855042
Training iter #292000:   Batch Loss = 9.271507, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.02546215057373, Accuracy = 0.8898568749427795
Training iter #294000:   Batch Loss = 9.156373, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.652130126953125, Accuracy = 0.8662103414535522
Training iter #296000:   Batch Loss = 9.953294, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.47571849822998, Accuracy = 0.883634090423584
Training iter #298000:   Batch Loss = 9.701564, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.343088150024414, Accuracy = 0.8830118179321289
Training iter #300000:   Batch Loss = 9.310302, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.27823257446289, Accuracy = 0.883634090423584
Training iter #302000:   Batch Loss = 8.706390, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.163614273071289, Accuracy = 0.8911014199256897
Training iter #304000:   Batch Loss = 9.431233, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.344123840332031, Accuracy = 0.8873677849769592
Training iter #306000:   Batch Loss = 10.282586, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 10.555686950683594, Accuracy = 0.8761667609214783
Training iter #308000:   Batch Loss = 9.855629, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.59866714477539, Accuracy = 0.8724331259727478
Training iter #310000:   Batch Loss = 9.867453, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.459192276000977, Accuracy = 0.8823895454406738
Training iter #312000:   Batch Loss = 9.243959, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.21364688873291, Accuracy = 0.8873677849769592
Training iter #314000:   Batch Loss = 9.855169, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.462635040283203, Accuracy = 0.8767890334129333
Training iter #316000:   Batch Loss = 9.963110, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.522115707397461, Accuracy = 0.8693217039108276
Training iter #318000:   Batch Loss = 9.376740, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.265922546386719, Accuracy = 0.8730553984642029
Training iter #320000:   Batch Loss = 9.291058, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.306934356689453, Accuracy = 0.8761667609214783
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-800
Training iter #322000:   Batch Loss = 9.509554, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.54349422454834, Accuracy = 0.8724331259727478
Training iter #324000:   Batch Loss = 9.444319, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.442244529724121, Accuracy = 0.8637211918830872
Training iter #326000:   Batch Loss = 9.451513, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.183563232421875, Accuracy = 0.8761667609214783
Training iter #328000:   Batch Loss = 9.348186, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.240121841430664, Accuracy = 0.8786558508872986
Training iter #330000:   Batch Loss = 8.803938, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.264039993286133, Accuracy = 0.874299943447113
Training iter #332000:   Batch Loss = 9.755396, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.147127151489258, Accuracy = 0.8749222159385681
Training iter #334000:   Batch Loss = 9.270161, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.388731002807617, Accuracy = 0.8799004554748535
Training iter #336000:   Batch Loss = 8.942843, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.173511505126953, Accuracy = 0.8780335783958435
Training iter #338000:   Batch Loss = 9.298488, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.307422637939453, Accuracy = 0.8711885213851929
Training iter #340000:   Batch Loss = 10.625455, Accuracy = 0.8695651888847351
PERFORMANCE ON TEST SET: Batch Loss = 10.174150466918945, Accuracy = 0.873677670955658
Training iter #342000:   Batch Loss = 9.924699, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.574239730834961, Accuracy = 0.8730553984642029
Training iter #344000:   Batch Loss = 9.790069, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.373563766479492, Accuracy = 0.8792781829833984
Training iter #346000:   Batch Loss = 9.740573, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.386438369750977, Accuracy = 0.8761667609214783
Training iter #348000:   Batch Loss = 9.065154, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.312750816345215, Accuracy = 0.8799004554748535
Training iter #350000:   Batch Loss = 9.510650, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.283117294311523, Accuracy = 0.874299943447113
Training iter #352000:   Batch Loss = 9.563395, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.202917098999023, Accuracy = 0.8830118179321289
Training iter #354000:   Batch Loss = 8.831856, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.159503936767578, Accuracy = 0.8848786354064941
Training iter #356000:   Batch Loss = 9.087111, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 9.963051795959473, Accuracy = 0.8842563629150391
Training iter #358000:   Batch Loss = 9.401610, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 9.845791816711426, Accuracy = 0.8842563629150391
Training iter #360000:   Batch Loss = 8.978101, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.019983291625977, Accuracy = 0.8855009078979492
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-900
Training iter #362000:   Batch Loss = 9.410607, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.568917274475098, Accuracy = 0.8823895454406738
Training iter #364000:   Batch Loss = 10.084002, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.14830493927002, Accuracy = 0.8699439764022827
Training iter #366000:   Batch Loss = 10.540249, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.714456558227539, Accuracy = 0.8786558508872986
Training iter #368000:   Batch Loss = 10.861876, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.627351760864258, Accuracy = 0.8668326139450073
Training iter #370000:   Batch Loss = 10.282927, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.170732498168945, Accuracy = 0.8904791474342346
Training iter #372000:   Batch Loss = 10.405926, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.245620727539062, Accuracy = 0.8786558508872986
Training iter #374000:   Batch Loss = 8.974212, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.019513130187988, Accuracy = 0.8873677849769592
Training iter #376000:   Batch Loss = 9.440739, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.063028335571289, Accuracy = 0.8867455124855042
Training iter #378000:   Batch Loss = 10.545903, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.132403373718262, Accuracy = 0.8879900574684143
Training iter #380000:   Batch Loss = 10.489326, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.007789611816406, Accuracy = 0.8886123299598694
Training iter #382000:   Batch Loss = 9.924989, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.843002319335938, Accuracy = 0.8811450004577637
Training iter #384000:   Batch Loss = 9.727376, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.724065780639648, Accuracy = 0.8830118179321289
Training iter #386000:   Batch Loss = 9.978350, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.804729461669922, Accuracy = 0.8780335783958435
Training iter #388000:   Batch Loss = 9.674398, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.91281509399414, Accuracy = 0.8817672729492188
Training iter #390000:   Batch Loss = 9.834811, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.842330932617188, Accuracy = 0.8842563629150391
Training iter #392000:   Batch Loss = 9.263588, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.677618026733398, Accuracy = 0.8911014199256897
Training iter #394000:   Batch Loss = 9.968296, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.648473739624023, Accuracy = 0.8879900574684143
Training iter #396000:   Batch Loss = 9.774633, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.832408905029297, Accuracy = 0.8792781829833984
Training iter #398000:   Batch Loss = 10.032213, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.54629898071289, Accuracy = 0.8898568749427795
Training iter #400000:   Batch Loss = 9.638367, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.44481086730957, Accuracy = 0.9023024439811707
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1000
Training iter #402000:   Batch Loss = 9.229203, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.370920181274414, Accuracy = 0.8861232399940491
Training iter #404000:   Batch Loss = 9.375361, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.33281421661377, Accuracy = 0.8867455124855042
Training iter #406000:   Batch Loss = 9.404224, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.4003267288208, Accuracy = 0.8892346024513245
Training iter #408000:   Batch Loss = 8.596598, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.582687377929688, Accuracy = 0.8892346024513245
Training iter #410000:   Batch Loss = 9.281906, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.387746810913086, Accuracy = 0.8935905694961548
Training iter #412000:   Batch Loss = 8.755045, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.299072265625, Accuracy = 0.9004355669021606
Training iter #414000:   Batch Loss = 9.354546, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.261513710021973, Accuracy = 0.8960796594619751
Training iter #416000:   Batch Loss = 9.675556, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.387784004211426, Accuracy = 0.8898568749427795
Training iter #418000:   Batch Loss = 9.066429, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.258174896240234, Accuracy = 0.8942128419876099
Training iter #420000:   Batch Loss = 8.774354, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.023788452148438, Accuracy = 0.8935905694961548
Training iter #422000:   Batch Loss = 8.960445, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.212714195251465, Accuracy = 0.8979464769363403
Training iter #424000:   Batch Loss = 9.343302, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.358016967773438, Accuracy = 0.89545738697052
Training iter #426000:   Batch Loss = 9.450325, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.33227825164795, Accuracy = 0.8929682374000549
Training iter #428000:   Batch Loss = 8.966860, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.193826675415039, Accuracy = 0.9029247164726257
Training iter #430000:   Batch Loss = 9.383258, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.251751899719238, Accuracy = 0.9029247164726257
Training iter #432000:   Batch Loss = 9.114239, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.420397758483887, Accuracy = 0.8985687494277954
Training iter #434000:   Batch Loss = 8.943017, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.284285545349121, Accuracy = 0.8892346024513245
Training iter #436000:   Batch Loss = 9.057241, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.457315444946289, Accuracy = 0.8904791474342346
Training iter #438000:   Batch Loss = 9.210231, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.229809761047363, Accuracy = 0.9029247164726257
Training iter #440000:   Batch Loss = 9.319875, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.416017532348633, Accuracy = 0.9072806239128113
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1100
Training iter #442000:   Batch Loss = 8.046623, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.607436180114746, Accuracy = 0.8991910219192505
Training iter #444000:   Batch Loss = 8.970940, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.40277099609375, Accuracy = 0.8960796594619751
Training iter #446000:   Batch Loss = 9.357527, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.92099380493164, Accuracy = 0.8985687494277954
Training iter #448000:   Batch Loss = 9.710521, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.969581604003906, Accuracy = 0.8923459649085999
Training iter #450000:   Batch Loss = 9.921375, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.984752655029297, Accuracy = 0.8923459649085999
Training iter #452000:   Batch Loss = 9.953757, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.115864753723145, Accuracy = 0.8848786354064941
Training iter #454000:   Batch Loss = 9.710766, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.900751113891602, Accuracy = 0.89545738697052
Training iter #456000:   Batch Loss = 9.895433, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.226059913635254, Accuracy = 0.8830118179321289
Training iter #458000:   Batch Loss = 10.187953, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.111997604370117, Accuracy = 0.8774113059043884
Training iter #460000:   Batch Loss = 10.084107, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.882255554199219, Accuracy = 0.883634090423584
Training iter #462000:   Batch Loss = 9.861397, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.714798927307129, Accuracy = 0.8774113059043884
Training iter #464000:   Batch Loss = 9.630440, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.99250602722168, Accuracy = 0.8867455124855042
Training iter #466000:   Batch Loss = 9.853936, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.786713600158691, Accuracy = 0.8767890334129333
Training iter #468000:   Batch Loss = 9.469274, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.676025390625, Accuracy = 0.8823895454406738
Training iter #470000:   Batch Loss = 9.928448, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.757225036621094, Accuracy = 0.8786558508872986
Training iter #472000:   Batch Loss = 9.676768, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.505167961120605, Accuracy = 0.8967019319534302
Training iter #474000:   Batch Loss = 9.364466, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.296462059020996, Accuracy = 0.89545738697052
Training iter #476000:   Batch Loss = 9.436929, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.277457237243652, Accuracy = 0.8979464769363403
Training iter #478000:   Batch Loss = 9.469469, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.320777893066406, Accuracy = 0.8967019319534302
Training iter #480000:   Batch Loss = 9.680233, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.251678466796875, Accuracy = 0.8879900574684143
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1200
Training iter #482000:   Batch Loss = 8.898332, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.138113975524902, Accuracy = 0.8998132944107056
Training iter #484000:   Batch Loss = 9.266454, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.320844650268555, Accuracy = 0.8917236924171448
Training iter #486000:   Batch Loss = 9.202763, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.152212142944336, Accuracy = 0.8935905694961548WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.

Training iter #488000:   Batch Loss = 9.100829, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.334529876708984, Accuracy = 0.9004355669021606
Training iter #490000:   Batch Loss = 8.874774, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.16275405883789, Accuracy = 0.9016801714897156
Training iter #492000:   Batch Loss = 8.787785, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.044622421264648, Accuracy = 0.9041692614555359
Training iter #494000:   Batch Loss = 9.002281, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.09136962890625, Accuracy = 0.9035469889640808
Training iter #496000:   Batch Loss = 9.237060, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 9.938273429870605, Accuracy = 0.9041692614555359
Training iter #498000:   Batch Loss = 8.684721, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 9.976890563964844, Accuracy = 0.9097697734832764
Training iter #500000:   Batch Loss = 8.733538, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.940753936767578, Accuracy = 0.9035469889640808
Training iter #502000:   Batch Loss = 8.626403, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.867891311645508, Accuracy = 0.9066583514213562
Training iter #504000:   Batch Loss = 9.144222, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 9.87167739868164, Accuracy = 0.9066583514213562
Training iter #506000:   Batch Loss = 8.931881, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 9.819473266601562, Accuracy = 0.9097697734832764
Training iter #508000:   Batch Loss = 8.760413, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 9.940009117126465, Accuracy = 0.9010578989982605
Training iter #510000:   Batch Loss = 8.575778, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 9.74897289276123, Accuracy = 0.904791533946991
Training iter #512000:   Batch Loss = 8.529027, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 9.972472190856934, Accuracy = 0.9066583514213562
Training iter #514000:   Batch Loss = 8.295038, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 9.940045356750488, Accuracy = 0.9016801714897156
Training iter #516000:   Batch Loss = 8.375336, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 9.80389404296875, Accuracy = 0.9097697734832764
Training iter #518000:   Batch Loss = 8.772534, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 9.954946517944336, Accuracy = 0.9041692614555359
Training iter #520000:   Batch Loss = 8.402121, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 9.892322540283203, Accuracy = 0.9159925580024719
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1300
Training iter #522000:   Batch Loss = 8.613355, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 9.760168075561523, Accuracy = 0.9060360789299011
Training iter #524000:   Batch Loss = 8.493353, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.053107261657715, Accuracy = 0.8998132944107056
Training iter #526000:   Batch Loss = 9.116775, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.29574203491211, Accuracy = 0.9041692614555359
Training iter #528000:   Batch Loss = 8.710974, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 9.983439445495605, Accuracy = 0.9085252285003662
Training iter #530000:   Batch Loss = 8.799337, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.01515007019043, Accuracy = 0.9029247164726257
Training iter #532000:   Batch Loss = 8.743405, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.066640853881836, Accuracy = 0.9085252285003662
Training iter #534000:   Batch Loss = 8.478293, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 9.894675254821777, Accuracy = 0.9110143184661865
Training iter #536000:   Batch Loss = 8.746552, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 9.743799209594727, Accuracy = 0.9116365909576416
Training iter #538000:   Batch Loss = 8.481979, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.918718338012695, Accuracy = 0.8935905694961548
Training iter #540000:   Batch Loss = 8.399307, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.688131332397461, Accuracy = 0.9122588634490967
Training iter #542000:   Batch Loss = 7.815136, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.525742530822754, Accuracy = 0.9085252285003662
Training iter #544000:   Batch Loss = 8.005939, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 9.634547233581543, Accuracy = 0.9060360789299011
Training iter #546000:   Batch Loss = 9.316368, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.650026321411133, Accuracy = 0.9010578989982605
Training iter #548000:   Batch Loss = 8.978463, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.297338485717773, Accuracy = 0.9091475009918213
Training iter #550000:   Batch Loss = 9.073173, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.340015411376953, Accuracy = 0.9041692614555359
Training iter #552000:   Batch Loss = 9.235185, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.406925201416016, Accuracy = 0.9010578989982605
Training iter #554000:   Batch Loss = 9.403778, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.266888618469238, Accuracy = 0.9066583514213562
Training iter #556000:   Batch Loss = 9.093942, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.12042236328125, Accuracy = 0.9116365909576416
Training iter #558000:   Batch Loss = 9.102758, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.378129959106445, Accuracy = 0.8935905694961548
Training iter #560000:   Batch Loss = 8.976931, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.215415000915527, Accuracy = 0.8973242044448853
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1400
Training iter #562000:   Batch Loss = 8.599458, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.941415786743164, Accuracy = 0.9079028964042664
Training iter #564000:   Batch Loss = 8.911052, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.919942855834961, Accuracy = 0.9091475009918213
Training iter #566000:   Batch Loss = 8.756637, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.046679496765137, Accuracy = 0.9091475009918213
Training iter #568000:   Batch Loss = 9.260292, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.055269241333008, Accuracy = 0.914747953414917
Training iter #570000:   Batch Loss = 8.566183, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 9.98082160949707, Accuracy = 0.9079028964042664
Training iter #572000:   Batch Loss = 8.361342, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 9.934784889221191, Accuracy = 0.9159925580024719
Training iter #574000:   Batch Loss = 8.517401, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.007242202758789, Accuracy = 0.9085252285003662
Training iter #576000:   Batch Loss = 9.089762, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 9.931914329528809, Accuracy = 0.9116365909576416
Training iter #578000:   Batch Loss = 8.957848, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 9.963851928710938, Accuracy = 0.9023024439811707
Training iter #580000:   Batch Loss = 8.488115, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.808425903320312, Accuracy = 0.9091475009918213
Training iter #582000:   Batch Loss = 8.396025, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 9.727738380432129, Accuracy = 0.9079028964042664
Training iter #584000:   Batch Loss = 8.372501, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 9.521988868713379, Accuracy = 0.9172371029853821
Training iter #586000:   Batch Loss = 8.629292, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 9.712692260742188, Accuracy = 0.904791533946991
Training iter #588000:   Batch Loss = 8.411794, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 9.688179016113281, Accuracy = 0.9085252285003662
Training iter #590000:   Batch Loss = 8.260137, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 9.728852272033691, Accuracy = 0.9122588634490967
Training iter #592000:   Batch Loss = 8.602223, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.337666511535645, Accuracy = 0.9091475009918213
Training iter #594000:   Batch Loss = 9.274228, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.491884231567383, Accuracy = 0.9066583514213562
Training iter #596000:   Batch Loss = 8.971829, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.300528526306152, Accuracy = 0.9079028964042664
Training iter #598000:   Batch Loss = 9.325139, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.17199993133545, Accuracy = 0.9153702259063721
Training iter #600000:   Batch Loss = 9.043552, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.265631675720215, Accuracy = 0.9110143184661865
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1500
Training iter #602000:   Batch Loss = 9.009199, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.147516250610352, Accuracy = 0.9103920459747314
Training iter #604000:   Batch Loss = 8.871798, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 9.919351577758789, Accuracy = 0.9110143184661865
Training iter #606000:   Batch Loss = 8.484884, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 9.935544967651367, Accuracy = 0.9110143184661865
Training iter #608000:   Batch Loss = 8.424663, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 9.91940975189209, Accuracy = 0.9184816479682922
Training iter #610000:   Batch Loss = 8.238738, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.792829513549805, Accuracy = 0.9135034084320068
Training iter #612000:   Batch Loss = 8.744901, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 9.906740188598633, Accuracy = 0.9122588634490967
Training iter #614000:   Batch Loss = 9.925835, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.980144500732422, Accuracy = 0.8991910219192505
Training iter #616000:   Batch Loss = 9.415510, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.164090156555176, Accuracy = 0.8998132944107056
Training iter #618000:   Batch Loss = 10.020332, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.216337203979492, Accuracy = 0.8911014199256897
Training iter #620000:   Batch Loss = 9.777373, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.925646781921387, Accuracy = 0.89545738697052
Training iter #622000:   Batch Loss = 9.924024, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.24177360534668, Accuracy = 0.8848786354064941
Training iter #624000:   Batch Loss = 9.720518, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.940391540527344, Accuracy = 0.8780335783958435
Training iter #626000:   Batch Loss = 9.548057, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.312923431396484, Accuracy = 0.8892346024513245
Training iter #628000:   Batch Loss = 10.493366, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.281911849975586, Accuracy = 0.8823895454406738
Training iter #630000:   Batch Loss = 9.873368, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.811443328857422, Accuracy = 0.8911014199256897
Training iter #632000:   Batch Loss = 9.345013, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.633874893188477, Accuracy = 0.8942128419876099
Training iter #634000:   Batch Loss = 9.326916, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.688369750976562, Accuracy = 0.8861232399940491
Training iter #636000:   Batch Loss = 9.502661, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.528084754943848, Accuracy = 0.8985687494277954
Training iter #638000:   Batch Loss = 9.298288, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.310640335083008, Accuracy = 0.8935905694961548
Training iter #640000:   Batch Loss = 9.341069, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.70024299621582, Accuracy = 0.8929682374000549
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1600
Training iter #642000:   Batch Loss = 9.419752, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.357505798339844, Accuracy = 0.8979464769363403
Training iter #644000:   Batch Loss = 9.191790, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.213502883911133, Accuracy = 0.8948351144790649
Training iter #646000:   Batch Loss = 8.434735, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 10.22163200378418, Accuracy = 0.9010578989982605
Training iter #648000:   Batch Loss = 8.536348, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.174192428588867, Accuracy = 0.9023024439811707
Training iter #650000:   Batch Loss = 8.683243, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.068262100219727, Accuracy = 0.9029247164726257
Training iter #652000:   Batch Loss = 8.967648, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.049927711486816, Accuracy = 0.89545738697052
Training iter #654000:   Batch Loss = 9.305639, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.265409469604492, Accuracy = 0.8873677849769592
Training iter #656000:   Batch Loss = 8.877217, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.086377143859863, Accuracy = 0.8973242044448853
Training iter #658000:   Batch Loss = 8.837387, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 9.95435905456543, Accuracy = 0.904791533946991
Training iter #660000:   Batch Loss = 8.989677, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.360749244689941, Accuracy = 0.9004355669021606
Training iter #662000:   Batch Loss = 8.614402, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.259753227233887, Accuracy = 0.8991910219192505
Training iter #664000:   Batch Loss = 8.899690, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.062602043151855, Accuracy = 0.9066583514213562
Training iter #666000:   Batch Loss = 10.056660, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.774398803710938, Accuracy = 0.8879900574684143
Training iter #668000:   Batch Loss = 9.321697, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.492155075073242, Accuracy = 0.8942128419876099
Training iter #670000:   Batch Loss = 9.192316, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.368241310119629, Accuracy = 0.9035469889640808
Training iter #672000:   Batch Loss = 9.284131, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.075958251953125, Accuracy = 0.8979464769363403
Training iter #674000:   Batch Loss = 9.362382, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.273153305053711, Accuracy = 0.8948351144790649
Training iter #676000:   Batch Loss = 9.456470, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.27686882019043, Accuracy = 0.8948351144790649
Training iter #678000:   Batch Loss = 9.002715, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.196245193481445, Accuracy = 0.8979464769363403
Training iter #680000:   Batch Loss = 8.281147, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 9.98807430267334, Accuracy = 0.9029247164726257
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1700
Training iter #682000:   Batch Loss = 8.793770, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.043183326721191, Accuracy = 0.8973242044448853
Training iter #684000:   Batch Loss = 8.920168, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.124195098876953, Accuracy = 0.9004355669021606
Training iter #686000:   Batch Loss = 8.657164, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 9.917573928833008, Accuracy = 0.9103920459747314
Training iter #688000:   Batch Loss = 8.533684, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 9.994033813476562, Accuracy = 0.89545738697052
Training iter #690000:   Batch Loss = 9.073682, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 9.919281959533691, Accuracy = 0.9010578989982605
Training iter #692000:   Batch Loss = 8.471626, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.010838508605957, Accuracy = 0.905413806438446
Training iter #694000:   Batch Loss = 8.365113, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.255889892578125, Accuracy = 0.9041692614555359
Training iter #696000:   Batch Loss = 9.073735, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.168092727661133, Accuracy = 0.9023024439811707
Training iter #698000:   Batch Loss = 10.049080, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.174844741821289, Accuracy = 0.8985687494277954
Training iter #700000:   Batch Loss = 9.205068, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.65339469909668, Accuracy = 0.9029247164726257
Training iter #702000:   Batch Loss = 9.366126, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.659745216369629, Accuracy = 0.9116365909576416
Training iter #704000:   Batch Loss = 9.382774, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.663854598999023, Accuracy = 0.9066583514213562
Training iter #706000:   Batch Loss = 9.569079, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.539981842041016, Accuracy = 0.89545738697052
Training iter #708000:   Batch Loss = 8.906681, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.380638122558594, Accuracy = 0.9023024439811707
Training iter #710000:   Batch Loss = 8.955364, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.151976585388184, Accuracy = 0.904791533946991
Training iter #712000:   Batch Loss = 8.370209, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.046972274780273, Accuracy = 0.9066583514213562
Training iter #714000:   Batch Loss = 9.426325, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 10.035223960876465, Accuracy = 0.905413806438446
Training iter #716000:   Batch Loss = 8.743540, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.069358825683594, Accuracy = 0.9035469889640808
Training iter #718000:   Batch Loss = 8.594988, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 9.88311767578125, Accuracy = 0.9091475009918213
Training iter #720000:   Batch Loss = 8.207915, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 9.801928520202637, Accuracy = 0.9178593754768372
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1800
Training iter #722000:   Batch Loss = 8.486758, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 9.908559799194336, Accuracy = 0.9066583514213562
Training iter #724000:   Batch Loss = 8.178861, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 9.988975524902344, Accuracy = 0.9085252285003662
Training iter #726000:   Batch Loss = 8.489952, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.039346694946289, Accuracy = 0.9135034084320068
Training iter #728000:   Batch Loss = 9.421745, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.098945617675781, Accuracy = 0.8973242044448853
Training iter #730000:   Batch Loss = 9.336214, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.814047813415527, Accuracy = 0.9091475009918213
Training iter #732000:   Batch Loss = 9.657545, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.038968086242676, Accuracy = 0.8917236924171448
Training iter #734000:   Batch Loss = 10.336712, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.669167518615723, Accuracy = 0.9091475009918213
Training iter #736000:   Batch Loss = 9.626663, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.721782684326172, Accuracy = 0.8985687494277954
Training iter #738000:   Batch Loss = 9.453463, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.070859909057617, Accuracy = 0.9103920459747314
Training iter #740000:   Batch Loss = 10.067930, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.796459197998047, Accuracy = 0.9122588634490967
Training iter #742000:   Batch Loss = 10.320881, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.552008628845215, Accuracy = 0.8998132944107056
Training iter #744000:   Batch Loss = 9.524765, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.036237716674805, Accuracy = 0.8991910219192505
Training iter #746000:   Batch Loss = 9.474623, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.899097442626953, Accuracy = 0.8911014199256897
Training iter #748000:   Batch Loss = 9.022701, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 11.180259704589844, Accuracy = 0.8991910219192505
Training iter #750000:   Batch Loss = 9.672447, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.866921424865723, Accuracy = 0.9029247164726257
Training iter #752000:   Batch Loss = 9.540456, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.592692375183105, Accuracy = 0.9066583514213562
Training iter #754000:   Batch Loss = 9.153494, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.487382888793945, Accuracy = 0.9097697734832764
Training iter #756000:   Batch Loss = 9.000722, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.67426586151123, Accuracy = 0.9029247164726257
Training iter #758000:   Batch Loss = 9.144856, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.678180694580078, Accuracy = 0.9072806239128113
Training iter #760000:   Batch Loss = 9.071094, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.529569625854492, Accuracy = 0.8967019319534302
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-1900
Training iter #762000:   Batch Loss = 8.975283, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.627168655395508, Accuracy = 0.9079028964042664
Training iter #764000:   Batch Loss = 9.735634, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.468484878540039, Accuracy = 0.9029247164726257
Training iter #766000:   Batch Loss = 8.866606, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.32581901550293, Accuracy = 0.9072806239128113
Training iter #768000:   Batch Loss = 9.285233, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.22575569152832, Accuracy = 0.9110143184661865
Training iter #770000:   Batch Loss = 8.852125, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.254968643188477, Accuracy = 0.9103920459747314
Training iter #772000:   Batch Loss = 9.197245, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.820934295654297, Accuracy = 0.9159925580024719
Training iter #774000:   Batch Loss = 8.820183, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.398622512817383, Accuracy = 0.9122588634490967
Training iter #776000:   Batch Loss = 8.984638, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.637018203735352, Accuracy = 0.9097697734832764
Training iter #778000:   Batch Loss = 9.336173, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.910274505615234, Accuracy = 0.9029247164726257
Training iter #780000:   Batch Loss = 9.506657, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.887035369873047, Accuracy = 0.8973242044448853
Training iter #782000:   Batch Loss = 9.761780, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 11.019194602966309, Accuracy = 0.8998132944107056
Training iter #784000:   Batch Loss = 10.132519, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.023387908935547, Accuracy = 0.9016801714897156
Training iter #786000:   Batch Loss = 9.702597, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.97832202911377, Accuracy = 0.89545738697052
Training iter #788000:   Batch Loss = 9.425345, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.755643844604492, Accuracy = 0.8911014199256897
Training iter #790000:   Batch Loss = 9.416600, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.79040813446045, Accuracy = 0.8911014199256897
Training iter #792000:   Batch Loss = 9.375446, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.629960060119629, Accuracy = 0.8973242044448853
Training iter #794000:   Batch Loss = 9.238356, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.315264701843262, Accuracy = 0.9103920459747314
Training iter #796000:   Batch Loss = 8.810340, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.406822204589844, Accuracy = 0.8948351144790649
Training iter #798000:   Batch Loss = 9.308661, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.235610008239746, Accuracy = 0.9103920459747314
Training iter #800000:   Batch Loss = 8.797082, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.071330070495605, Accuracy = 0.9110143184661865
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2000
Training iter #802000:   Batch Loss = 8.774151, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.099601745605469, Accuracy = 0.9122588634490967
Training iter #804000:   Batch Loss = 8.630054, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.159263610839844, Accuracy = 0.9066583514213562
Training iter #806000:   Batch Loss = 8.554722, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.112378120422363, Accuracy = 0.9128811359405518
Training iter #808000:   Batch Loss = 9.059238, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.194709777832031, Accuracy = 0.905413806438446
Training iter #810000:   Batch Loss = 9.049038, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.43973445892334, Accuracy = 0.8979464769363403
Training iter #812000:   Batch Loss = 8.754814, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.626018524169922, Accuracy = 0.9035469889640808
Training iter #814000:   Batch Loss = 9.422520, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.581262588500977, Accuracy = 0.9103920459747314
Training iter #816000:   Batch Loss = 9.474109, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.555514335632324, Accuracy = 0.904791533946991
Training iter #818000:   Batch Loss = 9.638425, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.72073745727539, Accuracy = 0.9016801714897156
Training iter #820000:   Batch Loss = 9.117136, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.449881553649902, Accuracy = 0.8973242044448853
Training iter #822000:   Batch Loss = 9.151890, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.145127296447754, Accuracy = 0.904791533946991
Training iter #824000:   Batch Loss = 9.113504, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.174541473388672, Accuracy = 0.9016801714897156
Training iter #826000:   Batch Loss = 9.156773, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.121097564697266, Accuracy = 0.9041692614555359
Training iter #828000:   Batch Loss = 8.970826, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.019332885742188, Accuracy = 0.9091475009918213
Training iter #830000:   Batch Loss = 8.517532, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.048686981201172, Accuracy = 0.9091475009918213
Training iter #832000:   Batch Loss = 8.803492, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 9.90677261352539, Accuracy = 0.9110143184661865
Training iter #834000:   Batch Loss = 8.422040, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.224535942077637, Accuracy = 0.9091475009918213
Training iter #836000:   Batch Loss = 8.747507, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.184946060180664, Accuracy = 0.904791533946991
Training iter #838000:   Batch Loss = 8.585602, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.190695762634277, Accuracy = 0.9122588634490967
Training iter #840000:   Batch Loss = 8.575784, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.088408470153809, Accuracy = 0.9066583514213562
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2100
Training iter #842000:   Batch Loss = 8.289656, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 9.919828414916992, Accuracy = 0.904791533946991
Training iter #844000:   Batch Loss = 8.729506, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.119058609008789, Accuracy = 0.9122588634490967
Training iter #846000:   Batch Loss = 8.074179, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.012694358825684, Accuracy = 0.9066583514213562
Training iter #848000:   Batch Loss = 8.357315, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 9.847818374633789, Accuracy = 0.9079028964042664
Training iter #850000:   Batch Loss = 8.894533, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 9.912548065185547, Accuracy = 0.9153702259063721
Training iter #852000:   Batch Loss = 9.567620, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.807649612426758, Accuracy = 0.8991910219192505
Training iter #854000:   Batch Loss = 9.229671, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.615497589111328, Accuracy = 0.8960796594619751
Training iter #856000:   Batch Loss = 9.226944, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.823555946350098, Accuracy = 0.8991910219192505
Training iter #858000:   Batch Loss = 9.742299, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.914840698242188, Accuracy = 0.8991910219192505
Training iter #860000:   Batch Loss = 8.989324, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.624850273132324, Accuracy = 0.9066583514213562
Training iter #862000:   Batch Loss = 8.995173, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.330509185791016, Accuracy = 0.9097697734832764
Training iter #864000:   Batch Loss = 8.503853, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.367809295654297, Accuracy = 0.89545738697052
Training iter #866000:   Batch Loss = 8.768597, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.210280418395996, Accuracy = 0.9029247164726257
Training iter #868000:   Batch Loss = 8.582740, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.044898986816406, Accuracy = 0.9128811359405518
Training iter #870000:   Batch Loss = 9.514015, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.81917953491211, Accuracy = 0.8911014199256897
Training iter #872000:   Batch Loss = 9.141391, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.430030822753906, Accuracy = 0.8973242044448853
Training iter #874000:   Batch Loss = 9.614988, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.790948867797852, Accuracy = 0.8985687494277954
Training iter #876000:   Batch Loss = 9.378028, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.612311363220215, Accuracy = 0.8960796594619751
Training iter #878000:   Batch Loss = 9.779860, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.150749206542969, Accuracy = 0.9004355669021606
Training iter #880000:   Batch Loss = 9.312197, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.786853790283203, Accuracy = 0.9023024439811707
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2200
Training iter #882000:   Batch Loss = 9.757369, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.423179626464844, Accuracy = 0.9122588634490967
Training iter #884000:   Batch Loss = 9.759439, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.80683422088623, Accuracy = 0.904791533946991
Training iter #886000:   Batch Loss = 9.003351, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.574583053588867, Accuracy = 0.9010578989982605
Training iter #888000:   Batch Loss = 9.272782, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.444564819335938, Accuracy = 0.8935905694961548
Training iter #890000:   Batch Loss = 9.015269, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.336099624633789, Accuracy = 0.9085252285003662
Training iter #892000:   Batch Loss = 9.488536, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.667717933654785, Accuracy = 0.9072806239128113
Training iter #894000:   Batch Loss = 9.195259, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.40566635131836, Accuracy = 0.9103920459747314
Training iter #896000:   Batch Loss = 9.098479, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.487274169921875, Accuracy = 0.9023024439811707
Training iter #898000:   Batch Loss = 8.687015, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.73882007598877, Accuracy = 0.9041692614555359
Training iter #900000:   Batch Loss = 9.513963, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.744318962097168, Accuracy = 0.9060360789299011
Training iter #902000:   Batch Loss = 9.039770, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.601564407348633, Accuracy = 0.9079028964042664
Training iter #904000:   Batch Loss = 9.317987, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.46259593963623, Accuracy = 0.9097697734832764
Training iter #906000:   Batch Loss = 9.552307, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.834169387817383, Accuracy = 0.9122588634490967
Training iter #908000:   Batch Loss = 9.355856, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.679488182067871, Accuracy = 0.9128811359405518
Training iter #910000:   Batch Loss = 9.877764, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.231947898864746, Accuracy = 0.9085252285003662
Training iter #912000:   Batch Loss = 9.841748, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.964869499206543, Accuracy = 0.9035469889640808
Training iter #914000:   Batch Loss = 9.230410, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.827999114990234, Accuracy = 0.9103920459747314
Training iter #916000:   Batch Loss = 9.100984, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.535951614379883, Accuracy = 0.9072806239128113
Training iter #918000:   Batch Loss = 8.225455, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.451255798339844, Accuracy = 0.9097697734832764
Training iter #920000:   Batch Loss = 9.249975, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.426666259765625, Accuracy = 0.914747953414917
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2300
Training iter #922000:   Batch Loss = 9.279024, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.389850616455078, Accuracy = 0.9041692614555359
Training iter #924000:   Batch Loss = 8.903283, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.223005294799805, Accuracy = 0.9097697734832764
Training iter #926000:   Batch Loss = 9.817625, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.319049835205078, Accuracy = 0.9116365909576416
Training iter #928000:   Batch Loss = 9.584184, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.218561172485352, Accuracy = 0.9122588634490967
Training iter #930000:   Batch Loss = 9.983038, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.173673629760742, Accuracy = 0.9010578989982605
Training iter #932000:   Batch Loss = 9.641026, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.066421508789062, Accuracy = 0.9091475009918213
Training iter #934000:   Batch Loss = 9.993936, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.018719673156738, Accuracy = 0.9172371029853821
Training iter #936000:   Batch Loss = 9.619531, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.826423645019531, Accuracy = 0.9159925580024719
Training iter #938000:   Batch Loss = 9.356846, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.682415008544922, Accuracy = 0.9110143184661865
Training iter #940000:   Batch Loss = 8.827617, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.552021980285645, Accuracy = 0.9159925580024719
Training iter #942000:   Batch Loss = 8.804430, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.506763458251953, Accuracy = 0.9184816479682922
Training iter #944000:   Batch Loss = 9.216962, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.841744422912598, Accuracy = 0.9153702259063721
Training iter #946000:   Batch Loss = 9.940565, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.342170715332031, Accuracy = 0.9079028964042664
Training iter #948000:   Batch Loss = 9.706015, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.263352394104004, Accuracy = 0.8998132944107056
Training iter #950000:   Batch Loss = 9.514348, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.017005920410156, Accuracy = 0.9010578989982605
Training iter #952000:   Batch Loss = 9.574668, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.963664054870605, Accuracy = 0.8979464769363403
Training iter #954000:   Batch Loss = 9.866979, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.261323928833008, Accuracy = 0.8917236924171448
Training iter #956000:   Batch Loss = 11.043416, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.292402267456055, Accuracy = 0.8848786354064941
Training iter #958000:   Batch Loss = 10.349767, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.577789306640625, Accuracy = 0.8948351144790649
Training iter #960000:   Batch Loss = 10.792165, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.893136024475098, Accuracy = 0.8948351144790649
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2400
Training iter #962000:   Batch Loss = 10.369533, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.529942512512207, Accuracy = 0.8942128419876099
Training iter #964000:   Batch Loss = 10.238537, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.669971466064453, Accuracy = 0.8929682374000549
Training iter #966000:   Batch Loss = 9.792248, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.688501358032227, Accuracy = 0.8823895454406738
Training iter #968000:   Batch Loss = 10.221279, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.520545959472656, Accuracy = 0.8979464769363403
Training iter #970000:   Batch Loss = 9.960602, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.037405967712402, Accuracy = 0.8998132944107056
Training iter #972000:   Batch Loss = 9.442759, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.966829299926758, Accuracy = 0.9035469889640808
Training iter #974000:   Batch Loss = 9.473411, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.928579330444336, Accuracy = 0.8948351144790649
Training iter #976000:   Batch Loss = 9.484260, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.787778854370117, Accuracy = 0.9004355669021606
Training iter #978000:   Batch Loss = 9.152347, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.649885177612305, Accuracy = 0.9010578989982605
Training iter #980000:   Batch Loss = 9.296001, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.941699981689453, Accuracy = 0.8991910219192505
Training iter #982000:   Batch Loss = 9.428762, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.83332347869873, Accuracy = 0.8942128419876099
Training iter #984000:   Batch Loss = 9.306418, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.735854148864746, Accuracy = 0.9023024439811707
Training iter #986000:   Batch Loss = 8.764676, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.839415550231934, Accuracy = 0.9004355669021606
Training iter #988000:   Batch Loss = 9.282403, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.93921184539795, Accuracy = 0.9079028964042664
Training iter #990000:   Batch Loss = 9.365377, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.717283248901367, Accuracy = 0.9029247164726257
Training iter #992000:   Batch Loss = 9.509012, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.507319450378418, Accuracy = 0.9066583514213562
Training iter #994000:   Batch Loss = 9.339774, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.638742446899414, Accuracy = 0.9029247164726257
Training iter #996000:   Batch Loss = 8.878469, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.605095863342285, Accuracy = 0.9016801714897156
Training iter #998000:   Batch Loss = 8.879355, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.479061126708984, Accuracy = 0.9085252285003662
Training iter #1000000:   Batch Loss = 8.693249, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.627352714538574, Accuracy = 0.8979464769363403
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2500
Training iter #1002000:   Batch Loss = 9.015652, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.50123405456543, Accuracy = 0.9004355669021606
Training iter #1004000:   Batch Loss = 8.971664, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.2664794921875, Accuracy = 0.9091475009918213
Training iter #1006000:   Batch Loss = 8.596132, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.298229217529297, Accuracy = 0.9029247164726257
Training iter #1008000:   Batch Loss = 8.488118, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.618369102478027, Accuracy = 0.9004355669021606
Training iter #1010000:   Batch Loss = 8.899078, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.311527252197266, Accuracy = 0.9122588634490967
Training iter #1012000:   Batch Loss = 8.724119, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.308753967285156, Accuracy = 0.9116365909576416
Training iter #1014000:   Batch Loss = 8.783769, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.368695259094238, Accuracy = 0.9103920459747314
Training iter #1016000:   Batch Loss = 8.882138, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.864275932312012, Accuracy = 0.8979464769363403
Training iter #1018000:   Batch Loss = 8.700949, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.749975204467773, Accuracy = 0.9141256809234619
Training iter #1020000:   Batch Loss = 8.552775, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.715080261230469, Accuracy = 0.9029247164726257
Training iter #1022000:   Batch Loss = 9.419258, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.917699813842773, Accuracy = 0.9128811359405518
Training iter #1024000:   Batch Loss = 9.409445, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.95484733581543, Accuracy = 0.9029247164726257
Training iter #1026000:   Batch Loss = 9.319205, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.747511863708496, Accuracy = 0.9072806239128113
Training iter #1028000:   Batch Loss = 9.479167, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.886877059936523, Accuracy = 0.9091475009918213
Training iter #1030000:   Batch Loss = 8.676928, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.640595436096191, Accuracy = 0.9097697734832764
Training iter #1032000:   Batch Loss = 8.803930, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.57270622253418, Accuracy = 0.905413806438446
Training iter #1034000:   Batch Loss = 8.990903, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.330406188964844, Accuracy = 0.9209707379341125
Training iter #1036000:   Batch Loss = 9.163433, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.530681610107422, Accuracy = 0.8979464769363403
Training iter #1038000:   Batch Loss = 10.168145, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.332930564880371, Accuracy = 0.8998132944107056
Training iter #1040000:   Batch Loss = 9.949989, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.353036880493164, Accuracy = 0.8929682374000549
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2600
Training iter #1042000:   Batch Loss = 9.273059, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.162152290344238, Accuracy = 0.9072806239128113
Training iter #1044000:   Batch Loss = 9.588795, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.067763328552246, Accuracy = 0.8942128419876099
Training iter #1046000:   Batch Loss = 12.468470, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.765811920166016, Accuracy = 0.8879900574684143
Training iter #1048000:   Batch Loss = 11.336765, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.043181419372559, Accuracy = 0.8886123299598694
Training iter #1050000:   Batch Loss = 10.497449, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.252643585205078, Accuracy = 0.8948351144790649
Training iter #1052000:   Batch Loss = 10.140791, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.849851608276367, Accuracy = 0.8929682374000549
Training iter #1054000:   Batch Loss = 9.976257, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 11.902299880981445, Accuracy = 0.9004355669021606
Training iter #1056000:   Batch Loss = 11.020629, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.12548542022705, Accuracy = 0.8998132944107056
Training iter #1058000:   Batch Loss = 10.774513, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.913835525512695, Accuracy = 0.89545738697052
Training iter #1060000:   Batch Loss = 10.771046, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.614245414733887, Accuracy = 0.9041692614555359
Training iter #1062000:   Batch Loss = 10.367803, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.773632049560547, Accuracy = 0.8948351144790649
Training iter #1064000:   Batch Loss = 10.133709, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.346260070800781, Accuracy = 0.9041692614555359
Training iter #1066000:   Batch Loss = 10.349656, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.262645721435547, Accuracy = 0.89545738697052
Training iter #1068000:   Batch Loss = 10.169696, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.700587272644043, Accuracy = 0.9023024439811707
Training iter #1070000:   Batch Loss = 10.375456, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.579792022705078, Accuracy = 0.8991910219192505
Training iter #1072000:   Batch Loss = 9.796013, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.492603302001953, Accuracy = 0.8991910219192505
Training iter #1074000:   Batch Loss = 10.316429, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.401751518249512, Accuracy = 0.9023024439811707
Training iter #1076000:   Batch Loss = 10.150530, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.436058044433594, Accuracy = 0.8979464769363403
Training iter #1078000:   Batch Loss = 9.741945, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.276209831237793, Accuracy = 0.904791533946991
Training iter #1080000:   Batch Loss = 9.945595, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.43209457397461, Accuracy = 0.8998132944107056
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2700
Training iter #1082000:   Batch Loss = 10.153276, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.828426361083984, Accuracy = 0.8935905694961548
Training iter #1084000:   Batch Loss = 10.300285, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.476017951965332, Accuracy = 0.8985687494277954
Training iter #1086000:   Batch Loss = 9.789687, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.558143615722656, Accuracy = 0.8998132944107056
Training iter #1088000:   Batch Loss = 9.746986, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 11.711320877075195, Accuracy = 0.9035469889640808
Training iter #1090000:   Batch Loss = 10.035740, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.589559555053711, Accuracy = 0.89545738697052
Training iter #1092000:   Batch Loss = 9.953702, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.54600715637207, Accuracy = 0.89545738697052
Training iter #1094000:   Batch Loss = 9.804836, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.30745792388916, Accuracy = 0.9029247164726257
Training iter #1096000:   Batch Loss = 9.897613, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.557870864868164, Accuracy = 0.8973242044448853
Training iter #1098000:   Batch Loss = 9.840164, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.382743835449219, Accuracy = 0.8979464769363403
Training iter #1100000:   Batch Loss = 9.832288, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.118897438049316, Accuracy = 0.8991910219192505
Training iter #1102000:   Batch Loss = 9.944481, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.340545654296875, Accuracy = 0.8991910219192505
Training iter #1104000:   Batch Loss = 9.850911, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.359695434570312, Accuracy = 0.9016801714897156
Training iter #1106000:   Batch Loss = 9.518568, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.066961288452148, Accuracy = 0.9060360789299011
Training iter #1108000:   Batch Loss = 9.640778, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.015138626098633, Accuracy = 0.905413806438446
Training iter #1110000:   Batch Loss = 9.995800, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.1082124710083, Accuracy = 0.8998132944107056
Training iter #1112000:   Batch Loss = 9.416428, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.082538604736328, Accuracy = 0.9029247164726257
Training iter #1114000:   Batch Loss = 9.332956, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.973402976989746, Accuracy = 0.9004355669021606
Training iter #1116000:   Batch Loss = 9.547695, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.141031265258789, Accuracy = 0.905413806438446
Training iter #1118000:   Batch Loss = 9.766603, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.077816009521484, Accuracy = 0.9010578989982605
Training iter #1120000:   Batch Loss = 9.575218, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.319424629211426, Accuracy = 0.8967019319534302
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2800
Training iter #1122000:   Batch Loss = 10.913208, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.655993461608887, Accuracy = 0.8898568749427795
Training iter #1124000:   Batch Loss = 9.837621, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.675098419189453, Accuracy = 0.8911014199256897
Training iter #1126000:   Batch Loss = 10.537552, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.801839828491211, Accuracy = 0.8973242044448853
Training iter #1128000:   Batch Loss = 10.184083, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.637961387634277, Accuracy = 0.8967019319534302
Training iter #1130000:   Batch Loss = 10.140764, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.896656036376953, Accuracy = 0.8948351144790649
Training iter #1132000:   Batch Loss = 11.045419, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.716093063354492, Accuracy = 0.9060360789299011
Training iter #1134000:   Batch Loss = 10.017705, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.457139015197754, Accuracy = 0.904791533946991
Training iter #1136000:   Batch Loss = 9.507709, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.358620643615723, Accuracy = 0.8911014199256897
Training iter #1138000:   Batch Loss = 10.299722, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.752037048339844, Accuracy = 0.8967019319534302
Training iter #1140000:   Batch Loss = 9.795305, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.522576332092285, Accuracy = 0.8917236924171448
Training iter #1142000:   Batch Loss = 9.876543, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.538433074951172, Accuracy = 0.8985687494277954
Training iter #1144000:   Batch Loss = 9.870331, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.703439712524414, Accuracy = 0.8917236924171448
Training iter #1146000:   Batch Loss = 9.862324, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.402585983276367, Accuracy = 0.8948351144790649
Training iter #1148000:   Batch Loss = 9.378162, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.274188995361328, Accuracy = 0.8942128419876099
Training iter #1150000:   Batch Loss = 9.787153, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.379980087280273, Accuracy = 0.8948351144790649
Training iter #1152000:   Batch Loss = 9.800346, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.210240364074707, Accuracy = 0.905413806438446
Training iter #1154000:   Batch Loss = 9.857473, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.013676643371582, Accuracy = 0.9016801714897156
Training iter #1156000:   Batch Loss = 10.311111, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.948734283447266, Accuracy = 0.9010578989982605
Training iter #1158000:   Batch Loss = 9.232691, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.05201530456543, Accuracy = 0.9023024439811707
Training iter #1160000:   Batch Loss = 9.628042, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.803654670715332, Accuracy = 0.9128811359405518
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-2900
Training iter #1162000:   Batch Loss = 9.163622, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.736791610717773, Accuracy = 0.9060360789299011
Training iter #1164000:   Batch Loss = 9.033728, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.828006744384766, Accuracy = 0.9079028964042664
Training iter #1166000:   Batch Loss = 9.392645, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.715211868286133, Accuracy = 0.9066583514213562
Training iter #1168000:   Batch Loss = 9.142961, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.61622428894043, Accuracy = 0.9079028964042664
Training iter #1170000:   Batch Loss = 8.948070, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.97311019897461, Accuracy = 0.9035469889640808
Training iter #1172000:   Batch Loss = 9.459407, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.753578186035156, Accuracy = 0.9116365909576416
Training iter #1174000:   Batch Loss = 9.297719, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.780416488647461, Accuracy = 0.9004355669021606
Training iter #1176000:   Batch Loss = 9.736233, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.609155654907227, Accuracy = 0.905413806438446
Training iter #1178000:   Batch Loss = 9.134992, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.645370483398438, Accuracy = 0.9041692614555359
Training iter #1180000:   Batch Loss = 9.579105, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.520801544189453, Accuracy = 0.9079028964042664
Training iter #1182000:   Batch Loss = 8.779058, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.407571792602539, Accuracy = 0.9103920459747314
Training iter #1184000:   Batch Loss = 9.080933, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.8123779296875, Accuracy = 0.9035469889640808
Training iter #1186000:   Batch Loss = 9.264150, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.900716781616211, Accuracy = 0.9116365909576416
Training iter #1188000:   Batch Loss = 8.957715, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.599332809448242, Accuracy = 0.9079028964042664
Training iter #1190000:   Batch Loss = 9.318051, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.598904609680176, Accuracy = 0.9103920459747314
Training iter #1192000:   Batch Loss = 9.287853, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.578866958618164, Accuracy = 0.9110143184661865
Training iter #1194000:   Batch Loss = 8.977708, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.630839347839355, Accuracy = 0.9110143184661865
Training iter #1196000:   Batch Loss = 8.626234, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.535335540771484, Accuracy = 0.9029247164726257
Training iter #1198000:   Batch Loss = 9.006910, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.590450286865234, Accuracy = 0.9116365909576416
Training iter #1200000:   Batch Loss = 8.948498, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.537574768066406, Accuracy = 0.9041692614555359
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3000
Training iter #1202000:   Batch Loss = 9.591329, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.573816299438477, Accuracy = 0.9116365909576416
Training iter #1204000:   Batch Loss = 8.943743, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.61972427368164, Accuracy = 0.9016801714897156
Training iter #1206000:   Batch Loss = 9.080441, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.376387596130371, Accuracy = 0.905413806438446
Training iter #1208000:   Batch Loss = 8.721652, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.299978256225586, Accuracy = 0.9066583514213562
Training iter #1210000:   Batch Loss = 8.701736, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.261201858520508, Accuracy = 0.9085252285003662
Training iter #1212000:   Batch Loss = 9.042889, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.469271659851074, Accuracy = 0.9116365909576416
Training iter #1214000:   Batch Loss = 9.087805, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.314301490783691, Accuracy = 0.9091475009918213
Training iter #1216000:   Batch Loss = 9.373371, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.492006301879883, Accuracy = 0.905413806438446
Training iter #1218000:   Batch Loss = 9.506128, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.6537446975708, Accuracy = 0.8967019319534302
Training iter #1220000:   Batch Loss = 9.052770, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.547124862670898, Accuracy = 0.8979464769363403
Training iter #1222000:   Batch Loss = 9.067606, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.437569618225098, Accuracy = 0.9041692614555359
Training iter #1224000:   Batch Loss = 8.146017, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.297536849975586, Accuracy = 0.9041692614555359
Training iter #1226000:   Batch Loss = 8.852777, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.333847045898438, Accuracy = 0.905413806438446
Training iter #1228000:   Batch Loss = 8.786591, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.208802223205566, Accuracy = 0.9072806239128113
Training iter #1230000:   Batch Loss = 8.779338, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.442917823791504, Accuracy = 0.9085252285003662
Training iter #1232000:   Batch Loss = 10.384539, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.551817893981934, Accuracy = 0.9029247164726257
Training iter #1234000:   Batch Loss = 12.400870, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.223000526428223, Accuracy = 0.9023024439811707
Training iter #1236000:   Batch Loss = 10.359605, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.14258861541748, Accuracy = 0.9004355669021606
Training iter #1238000:   Batch Loss = 10.405035, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.527746200561523, Accuracy = 0.9029247164726257
Training iter #1240000:   Batch Loss = 9.896014, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.464347839355469, Accuracy = 0.9060360789299011
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3100
Training iter #1242000:   Batch Loss = 9.751497, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.592869758605957, Accuracy = 0.9035469889640808
Training iter #1244000:   Batch Loss = 9.992178, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.46983528137207, Accuracy = 0.9091475009918213
Training iter #1246000:   Batch Loss = 10.644616, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.472697257995605, Accuracy = 0.9091475009918213
Training iter #1248000:   Batch Loss = 10.341820, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.485539436340332, Accuracy = 0.9010578989982605
Training iter #1250000:   Batch Loss = 10.115541, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.446455001831055, Accuracy = 0.9035469889640808
Training iter #1252000:   Batch Loss = 9.713676, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.448610305786133, Accuracy = 0.8979464769363403
Training iter #1254000:   Batch Loss = 9.819031, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.029781341552734, Accuracy = 0.9066583514213562
Training iter #1256000:   Batch Loss = 9.674221, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.890877723693848, Accuracy = 0.9110143184661865
Training iter #1258000:   Batch Loss = 8.658791, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.880690574645996, Accuracy = 0.9066583514213562
Training iter #1260000:   Batch Loss = 9.553610, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.038243293762207, Accuracy = 0.914747953414917
Training iter #1262000:   Batch Loss = 9.463722, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.927526473999023, Accuracy = 0.9041692614555359
Training iter #1264000:   Batch Loss = 9.301678, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.999786376953125, Accuracy = 0.9116365909576416
Training iter #1266000:   Batch Loss = 9.532064, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.184632301330566, Accuracy = 0.9103920459747314
Training iter #1268000:   Batch Loss = 9.370365, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.977855682373047, Accuracy = 0.905413806438446
Training iter #1270000:   Batch Loss = 9.313801, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.150296211242676, Accuracy = 0.9097697734832764
Training iter #1272000:   Batch Loss = 10.000623, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.279241561889648, Accuracy = 0.9091475009918213
Training iter #1274000:   Batch Loss = 9.404317, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.058265686035156, Accuracy = 0.8973242044448853
Training iter #1276000:   Batch Loss = 9.767326, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.054408073425293, Accuracy = 0.8960796594619751
Training iter #1278000:   Batch Loss = 9.242406, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.023519515991211, Accuracy = 0.9091475009918213
Training iter #1280000:   Batch Loss = 9.885568, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.263703346252441, Accuracy = 0.9016801714897156
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3200
Training iter #1282000:   Batch Loss = 9.314054, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.149728775024414, Accuracy = 0.9110143184661865
Training iter #1284000:   Batch Loss = 9.056991, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.086505889892578, Accuracy = 0.9085252285003662
Training iter #1286000:   Batch Loss = 9.439757, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.122279167175293, Accuracy = 0.904791533946991
Training iter #1288000:   Batch Loss = 9.391596, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.19489860534668, Accuracy = 0.9085252285003662
Training iter #1290000:   Batch Loss = 9.463829, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.96153736114502, Accuracy = 0.914747953414917
Training iter #1292000:   Batch Loss = 9.845418, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.942780494689941, Accuracy = 0.9097697734832764
Training iter #1294000:   Batch Loss = 9.619951, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.878751754760742, Accuracy = 0.9097697734832764
Training iter #1296000:   Batch Loss = 8.984210, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.856889724731445, Accuracy = 0.9122588634490967
Training iter #1298000:   Batch Loss = 9.566689, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.835573196411133, Accuracy = 0.9159925580024719
Training iter #1300000:   Batch Loss = 9.527565, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.891416549682617, Accuracy = 0.9103920459747314
Training iter #1302000:   Batch Loss = 9.062231, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.741613388061523, Accuracy = 0.9091475009918213
Training iter #1304000:   Batch Loss = 9.308729, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.921462059020996, Accuracy = 0.9116365909576416
Training iter #1306000:   Batch Loss = 9.280458, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.989535331726074, Accuracy = 0.9029247164726257
Training iter #1308000:   Batch Loss = 9.040826, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.839954376220703, Accuracy = 0.905413806438446
Training iter #1310000:   Batch Loss = 9.135314, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.600262641906738, Accuracy = 0.9016801714897156
Training iter #1312000:   Batch Loss = 8.663244, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.514184951782227, Accuracy = 0.9085252285003662
Training iter #1314000:   Batch Loss = 8.797501, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.457040786743164, Accuracy = 0.9116365909576416
Training iter #1316000:   Batch Loss = 9.494541, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.447673797607422, Accuracy = 0.9197261929512024
Training iter #1318000:   Batch Loss = 9.075851, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.626834869384766, Accuracy = 0.914747953414917
Training iter #1320000:   Batch Loss = 9.015788, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.61925983428955, Accuracy = 0.9153702259063721
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3300
Training iter #1322000:   Batch Loss = 8.905916, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.497448921203613, Accuracy = 0.9091475009918213
Training iter #1324000:   Batch Loss = 8.781301, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.527056694030762, Accuracy = 0.9097697734832764
Training iter #1326000:   Batch Loss = 7.897879, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.457742691040039, Accuracy = 0.9110143184661865
Training iter #1328000:   Batch Loss = 8.200605, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.541047096252441, Accuracy = 0.9141256809234619
Training iter #1330000:   Batch Loss = 9.008278, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.811327934265137, Accuracy = 0.9085252285003662
Training iter #1332000:   Batch Loss = 9.233086, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.787187576293945, Accuracy = 0.9029247164726257
Training iter #1334000:   Batch Loss = 9.321959, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.679455757141113, Accuracy = 0.9072806239128113
Training iter #1336000:   Batch Loss = 8.771469, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.518840789794922, Accuracy = 0.9116365909576416
Training iter #1338000:   Batch Loss = 8.485966, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.343955993652344, Accuracy = 0.9029247164726257
Training iter #1340000:   Batch Loss = 8.693077, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.376996994018555, Accuracy = 0.9060360789299011
Training iter #1342000:   Batch Loss = 8.604550, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.312986373901367, Accuracy = 0.9085252285003662
Training iter #1344000:   Batch Loss = 8.679966, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.246195793151855, Accuracy = 0.9141256809234619
Training iter #1346000:   Batch Loss = 8.555003, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.266627311706543, Accuracy = 0.9159925580024719
Training iter #1348000:   Batch Loss = 8.605119, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.366781234741211, Accuracy = 0.9135034084320068
Training iter #1350000:   Batch Loss = 8.538898, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.498329162597656, Accuracy = 0.9103920459747314
Training iter #1352000:   Batch Loss = 8.738298, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.40859603881836, Accuracy = 0.9110143184661865
Training iter #1354000:   Batch Loss = 9.122858, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.543960571289062, Accuracy = 0.9079028964042664
Training iter #1356000:   Batch Loss = 8.423929, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.461221694946289, Accuracy = 0.9091475009918213
Training iter #1358000:   Batch Loss = 8.927828, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.70883560180664, Accuracy = 0.9103920459747314
Training iter #1360000:   Batch Loss = 9.930533, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 11.13358211517334, Accuracy = 0.9110143184661865
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3400
Training iter #1362000:   Batch Loss = 9.973336, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.327101707458496, Accuracy = 0.9041692614555359
Training iter #1364000:   Batch Loss = 9.238648, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.105573654174805, Accuracy = 0.9079028964042664
Training iter #1366000:   Batch Loss = 9.206002, Accuracy = 0.9900000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.878191947937012, Accuracy = 0.9153702259063721
Training iter #1368000:   Batch Loss = 9.272457, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.031038284301758, Accuracy = 0.9122588634490967
Training iter #1370000:   Batch Loss = 9.141180, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.96714973449707, Accuracy = 0.9072806239128113
Training iter #1372000:   Batch Loss = 9.129832, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.738560676574707, Accuracy = 0.914747953414917
Training iter #1374000:   Batch Loss = 8.994837, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.802770614624023, Accuracy = 0.914747953414917
Training iter #1376000:   Batch Loss = 9.026332, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.580327987670898, Accuracy = 0.9159925580024719
Training iter #1378000:   Batch Loss = 8.992452, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.610036849975586, Accuracy = 0.9178593754768372
Training iter #1380000:   Batch Loss = 8.661313, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.494267463684082, Accuracy = 0.914747953414917
Training iter #1382000:   Batch Loss = 9.143442, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.013334274291992, Accuracy = 0.9172371029853821
Training iter #1384000:   Batch Loss = 9.019892, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.941191673278809, Accuracy = 0.9141256809234619
Training iter #1386000:   Batch Loss = 9.378900, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.880020141601562, Accuracy = 0.9010578989982605
Training iter #1388000:   Batch Loss = 10.631752, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.135106086730957, Accuracy = 0.9141256809234619
Training iter #1390000:   Batch Loss = 10.074404, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.7153902053833, Accuracy = 0.9215930104255676
Training iter #1392000:   Batch Loss = 9.921431, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.456233024597168, Accuracy = 0.9215930104255676
Training iter #1394000:   Batch Loss = 10.994247, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.550603866577148, Accuracy = 0.9085252285003662
Training iter #1396000:   Batch Loss = 9.669090, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.587489128112793, Accuracy = 0.9135034084320068
Training iter #1398000:   Batch Loss = 10.316620, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.540254592895508, Accuracy = 0.916614830493927
Training iter #1400000:   Batch Loss = 10.038577, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.472837448120117, Accuracy = 0.9097697734832764
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3500
Training iter #1402000:   Batch Loss = 9.773834, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.32204532623291, Accuracy = 0.914747953414917
Training iter #1404000:   Batch Loss = 9.615757, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.174125671386719, Accuracy = 0.9072806239128113
Training iter #1406000:   Batch Loss = 9.544337, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.9600830078125, Accuracy = 0.9122588634490967
Training iter #1408000:   Batch Loss = 9.211782, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.043436050415039, Accuracy = 0.9060360789299011
Training iter #1410000:   Batch Loss = 9.610482, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.004304885864258, Accuracy = 0.9153702259063721
Training iter #1412000:   Batch Loss = 9.567676, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.072871208190918, Accuracy = 0.9110143184661865
Training iter #1414000:   Batch Loss = 9.443899, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.830282211303711, Accuracy = 0.9159925580024719
Training iter #1416000:   Batch Loss = 9.349148, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.146866798400879, Accuracy = 0.9153702259063721
Training iter #1418000:   Batch Loss = 9.277970, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.894817352294922, Accuracy = 0.9178593754768372
Training iter #1420000:   Batch Loss = 9.125944, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.060261726379395, Accuracy = 0.9135034084320068
Training iter #1422000:   Batch Loss = 8.687743, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.88293170928955, Accuracy = 0.9153702259063721
Training iter #1424000:   Batch Loss = 9.176857, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.923539161682129, Accuracy = 0.9197261929512024
Training iter #1426000:   Batch Loss = 9.292093, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.910284996032715, Accuracy = 0.9116365909576416
Training iter #1428000:   Batch Loss = 9.415514, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.869144439697266, Accuracy = 0.9197261929512024
Training iter #1430000:   Batch Loss = 9.631984, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.3006591796875, Accuracy = 0.916614830493927
Training iter #1432000:   Batch Loss = 9.648811, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.949562072753906, Accuracy = 0.9191039204597473
Training iter #1434000:   Batch Loss = 9.368133, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.012971878051758, Accuracy = 0.9159925580024719
Training iter #1436000:   Batch Loss = 9.449818, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.926673889160156, Accuracy = 0.9110143184661865
Training iter #1438000:   Batch Loss = 9.259892, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.911983489990234, Accuracy = 0.9153702259063721
Training iter #1440000:   Batch Loss = 9.209219, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.924942970275879, Accuracy = 0.9128811359405518
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3600
Training iter #1442000:   Batch Loss = 9.055085, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.022114753723145, Accuracy = 0.9060360789299011
Training iter #1444000:   Batch Loss = 9.495947, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.870343208312988, Accuracy = 0.9141256809234619
Training iter #1446000:   Batch Loss = 9.250663, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.888321876525879, Accuracy = 0.916614830493927
Training iter #1448000:   Batch Loss = 10.177614, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.651192665100098, Accuracy = 0.9191039204597473
Training iter #1450000:   Batch Loss = 11.268297, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.031070709228516, Accuracy = 0.9085252285003662
Training iter #1452000:   Batch Loss = 10.641313, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.263009071350098, Accuracy = 0.905413806438446
Training iter #1454000:   Batch Loss = 10.210892, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.733742713928223, Accuracy = 0.9141256809234619
Training iter #1456000:   Batch Loss = 9.671373, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.711141586303711, Accuracy = 0.9184816479682922
Training iter #1458000:   Batch Loss = 9.973852, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.406021118164062, Accuracy = 0.9135034084320068
Training iter #1460000:   Batch Loss = 9.741348, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.401830673217773, Accuracy = 0.9085252285003662
Training iter #1462000:   Batch Loss = 9.681013, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 11.416011810302734, Accuracy = 0.9060360789299011
Training iter #1464000:   Batch Loss = 9.913366, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.169502258300781, Accuracy = 0.9135034084320068
Training iter #1466000:   Batch Loss = 9.519391, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.057068824768066, Accuracy = 0.9209707379341125
Training iter #1468000:   Batch Loss = 9.592867, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.145882606506348, Accuracy = 0.9135034084320068
Training iter #1470000:   Batch Loss = 9.541771, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.372294425964355, Accuracy = 0.9116365909576416
Training iter #1472000:   Batch Loss = 9.166167, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.028138160705566, Accuracy = 0.9178593754768372
Training iter #1474000:   Batch Loss = 8.786664, Accuracy = 0.9900000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.866809844970703, Accuracy = 0.9197261929512024
Training iter #1476000:   Batch Loss = 9.080303, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.951494216918945, Accuracy = 0.9172371029853821
Training iter #1478000:   Batch Loss = 9.344063, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.298273086547852, Accuracy = 0.9122588634490967
Training iter #1480000:   Batch Loss = 9.990801, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.469433784484863, Accuracy = 0.9103920459747314
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3700
Training iter #1482000:   Batch Loss = 9.724292, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.217691421508789, Accuracy = 0.9153702259063721
Training iter #1484000:   Batch Loss = 9.637203, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.26089096069336, Accuracy = 0.9178593754768372
Training iter #1486000:   Batch Loss = 9.793182, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.267557144165039, Accuracy = 0.9135034084320068
Training iter #1488000:   Batch Loss = 9.743507, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.122669219970703, Accuracy = 0.9135034084320068
Training iter #1490000:   Batch Loss = 9.601664, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.442917823791504, Accuracy = 0.9122588634490967
Training iter #1492000:   Batch Loss = 9.824095, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.291790008544922, Accuracy = 0.9159925580024719
Training iter #1494000:   Batch Loss = 9.404077, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.127418518066406, Accuracy = 0.9172371029853821
Training iter #1496000:   Batch Loss = 9.084049, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.066255569458008, Accuracy = 0.9135034084320068
Training iter #1498000:   Batch Loss = 8.999191, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.013975143432617, Accuracy = 0.9122588634490967
Training iter #1500000:   Batch Loss = 9.122776, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.856067657470703, Accuracy = 0.916614830493927
Training iter #1502000:   Batch Loss = 9.087314, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.78845500946045, Accuracy = 0.9159925580024719
Training iter #1504000:   Batch Loss = 8.895991, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.741215705871582, Accuracy = 0.9122588634490967
Training iter #1506000:   Batch Loss = 9.118812, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.938026428222656, Accuracy = 0.9122588634490967
Training iter #1508000:   Batch Loss = 9.051763, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.846860885620117, Accuracy = 0.9103920459747314
Training iter #1510000:   Batch Loss = 8.967800, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.92491340637207, Accuracy = 0.9128811359405518
Training iter #1512000:   Batch Loss = 8.964922, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.805179595947266, Accuracy = 0.9141256809234619
Training iter #1514000:   Batch Loss = 9.207513, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.480302810668945, Accuracy = 0.9153702259063721
Training iter #1516000:   Batch Loss = 9.160999, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.455709457397461, Accuracy = 0.9184816479682922
Training iter #1518000:   Batch Loss = 8.955766, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.529693603515625, Accuracy = 0.9110143184661865
Training iter #1520000:   Batch Loss = 9.294970, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.377301216125488, Accuracy = 0.9234598875045776
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3800
Training iter #1522000:   Batch Loss = 9.058279, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.56490421295166, Accuracy = 0.9178593754768372
Training iter #1524000:   Batch Loss = 9.659903, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.806556701660156, Accuracy = 0.9066583514213562
Training iter #1526000:   Batch Loss = 9.197511, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.703764915466309, Accuracy = 0.9116365909576416
Training iter #1528000:   Batch Loss = 9.006391, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.57937240600586, Accuracy = 0.9222152829170227
Training iter #1530000:   Batch Loss = 8.716069, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 10.677499771118164, Accuracy = 0.9128811359405518
Training iter #1532000:   Batch Loss = 8.894167, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.563824653625488, Accuracy = 0.925948977470398
Training iter #1534000:   Batch Loss = 9.587936, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.745803833007812, Accuracy = 0.9079028964042664
Training iter #1536000:   Batch Loss = 9.066211, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.53096866607666, Accuracy = 0.9085252285003662
Training iter #1538000:   Batch Loss = 9.078588, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.720697402954102, Accuracy = 0.904791533946991
Training iter #1540000:   Batch Loss = 8.736129, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.410888671875, Accuracy = 0.9159925580024719
Training iter #1542000:   Batch Loss = 8.749401, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.469467163085938, Accuracy = 0.9116365909576416
Training iter #1544000:   Batch Loss = 8.828161, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.377252578735352, Accuracy = 0.9159925580024719
Training iter #1546000:   Batch Loss = 8.683880, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.406902313232422, Accuracy = 0.9128811359405518
Training iter #1548000:   Batch Loss = 8.796093, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.382304191589355, Accuracy = 0.9203484654426575
Training iter #1550000:   Batch Loss = 8.491837, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.332862854003906, Accuracy = 0.9135034084320068
Training iter #1552000:   Batch Loss = 8.538930, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.562416076660156, Accuracy = 0.9116365909576416
Training iter #1554000:   Batch Loss = 8.448475, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.497998237609863, Accuracy = 0.9222152829170227
Training iter #1556000:   Batch Loss = 8.879696, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.501529693603516, Accuracy = 0.9153702259063721
Training iter #1558000:   Batch Loss = 8.587171, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.419634819030762, Accuracy = 0.9203484654426575
Training iter #1560000:   Batch Loss = 8.753909, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.485634803771973, Accuracy = 0.9178593754768372
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-3900
Training iter #1562000:   Batch Loss = 8.455994, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.247961044311523, Accuracy = 0.9222152829170227
Training iter #1564000:   Batch Loss = 8.811234, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 10.211209297180176, Accuracy = 0.9284380674362183
Training iter #1566000:   Batch Loss = 8.406117, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.149308204650879, Accuracy = 0.914747953414917
Training iter #1568000:   Batch Loss = 8.853329, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.181157112121582, Accuracy = 0.9215930104255676
Training iter #1570000:   Batch Loss = 8.476818, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.46107292175293, Accuracy = 0.9228376150131226
Training iter #1572000:   Batch Loss = 8.883558, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.63135051727295, Accuracy = 0.9234598875045776
Training iter #1574000:   Batch Loss = 8.726906, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.516669273376465, Accuracy = 0.9103920459747314
Training iter #1576000:   Batch Loss = 8.829123, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.41312026977539, Accuracy = 0.914747953414917
Training iter #1578000:   Batch Loss = 8.789762, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.360677719116211, Accuracy = 0.9097697734832764
Training iter #1580000:   Batch Loss = 8.380498, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.207723617553711, Accuracy = 0.9197261929512024
Training iter #1582000:   Batch Loss = 8.463299, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.116366386413574, Accuracy = 0.9228376150131226
Training iter #1584000:   Batch Loss = 8.136911, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.183828353881836, Accuracy = 0.9172371029853821
Training iter #1586000:   Batch Loss = 8.420035, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.255428314208984, Accuracy = 0.9228376150131226
Training iter #1588000:   Batch Loss = 8.446290, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.115944862365723, Accuracy = 0.9240821599960327
Training iter #1590000:   Batch Loss = 8.852250, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.437243461608887, Accuracy = 0.9247044324874878
Training iter #1592000:   Batch Loss = 9.159514, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.45547103881836, Accuracy = 0.9141256809234619
Training iter #1594000:   Batch Loss = 8.291203, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.209182739257812, Accuracy = 0.914747953414917
Training iter #1596000:   Batch Loss = 8.707561, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.32327651977539, Accuracy = 0.9234598875045776
Training iter #1598000:   Batch Loss = 8.060608, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 10.780881881713867, Accuracy = 0.9097697734832764
Training iter #1600000:   Batch Loss = 9.030623, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.431619644165039, Accuracy = 0.9228376150131226
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4000
Training iter #1602000:   Batch Loss = 9.122977, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.336451530456543, Accuracy = 0.9222152829170227
Training iter #1604000:   Batch Loss = 8.967762, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.04189682006836, Accuracy = 0.9209707379341125
Training iter #1606000:   Batch Loss = 8.988931, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.290794372558594, Accuracy = 0.9184816479682922
Training iter #1608000:   Batch Loss = 8.684748, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.166450500488281, Accuracy = 0.9197261929512024
Training iter #1610000:   Batch Loss = 8.511035, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.148710250854492, Accuracy = 0.9178593754768372
Training iter #1612000:   Batch Loss = 8.414531, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.508952140808105, Accuracy = 0.9209707379341125
Training iter #1614000:   Batch Loss = 8.704650, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.151901245117188, Accuracy = 0.9172371029853821
Training iter #1616000:   Batch Loss = 8.649023, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.132216453552246, Accuracy = 0.9110143184661865
Training iter #1618000:   Batch Loss = 9.074360, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.301291465759277, Accuracy = 0.9085252285003662
Training iter #1620000:   Batch Loss = 9.284071, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.413711547851562, Accuracy = 0.9153702259063721
Training iter #1622000:   Batch Loss = 9.597555, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.968835830688477, Accuracy = 0.9191039204597473
Training iter #1624000:   Batch Loss = 9.176199, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.486799240112305, Accuracy = 0.9247044324874878
Training iter #1626000:   Batch Loss = 9.083957, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.507596969604492, Accuracy = 0.9097697734832764
Training iter #1628000:   Batch Loss = 8.800308, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.549729347229004, Accuracy = 0.914747953414917
Training iter #1630000:   Batch Loss = 8.587692, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.475861549377441, Accuracy = 0.9128811359405518
Training iter #1632000:   Batch Loss = 9.607143, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.773895263671875, Accuracy = 0.9178593754768372
Training iter #1634000:   Batch Loss = 8.908958, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.556774139404297, Accuracy = 0.9110143184661865
Training iter #1636000:   Batch Loss = 8.505886, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.416325569152832, Accuracy = 0.9122588634490967
Training iter #1638000:   Batch Loss = 8.173910, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.19194507598877, Accuracy = 0.9184816479682922
Training iter #1640000:   Batch Loss = 9.052938, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.302444458007812, Accuracy = 0.9215930104255676
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4100
Training iter #1642000:   Batch Loss = 8.392885, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.232096672058105, Accuracy = 0.9184816479682922
Training iter #1644000:   Batch Loss = 8.191142, Accuracy = 0.9900000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.109817504882812, Accuracy = 0.9203484654426575
Training iter #1646000:   Batch Loss = 8.018536, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.080358505249023, Accuracy = 0.9278157949447632
Training iter #1648000:   Batch Loss = 8.419947, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.317530632019043, Accuracy = 0.916614830493927
Training iter #1650000:   Batch Loss = 8.700722, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.257185935974121, Accuracy = 0.9197261929512024
Training iter #1652000:   Batch Loss = 8.230235, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.231432914733887, Accuracy = 0.9203484654426575
Training iter #1654000:   Batch Loss = 8.511100, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.426950454711914, Accuracy = 0.9066583514213562
Training iter #1656000:   Batch Loss = 9.479600, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.905922889709473, Accuracy = 0.9110143184661865
Training iter #1658000:   Batch Loss = 9.096106, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.508405685424805, Accuracy = 0.916614830493927
Training iter #1660000:   Batch Loss = 8.986324, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.583187103271484, Accuracy = 0.9172371029853821
Training iter #1662000:   Batch Loss = 8.793151, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.43867015838623, Accuracy = 0.9234598875045776
Training iter #1664000:   Batch Loss = 8.240287, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.276260375976562, Accuracy = 0.9203484654426575
Training iter #1666000:   Batch Loss = 8.022716, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.334747314453125, Accuracy = 0.9159925580024719
Training iter #1668000:   Batch Loss = 8.636227, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.297101974487305, Accuracy = 0.9178593754768372
Training iter #1670000:   Batch Loss = 8.709470, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.335760116577148, Accuracy = 0.9203484654426575
Training iter #1672000:   Batch Loss = 8.297844, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.094375610351562, Accuracy = 0.9240821599960327
Training iter #1674000:   Batch Loss = 8.307790, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.373482704162598, Accuracy = 0.9215930104255676
Training iter #1676000:   Batch Loss = 8.402393, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.37360668182373, Accuracy = 0.9116365909576416
Training iter #1678000:   Batch Loss = 8.603027, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.114148139953613, Accuracy = 0.914747953414917
Training iter #1680000:   Batch Loss = 8.186604, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.118254661560059, Accuracy = 0.9191039204597473
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4200
Training iter #1682000:   Batch Loss = 8.251909, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.223031044006348, Accuracy = 0.9184816479682922
Training iter #1684000:   Batch Loss = 8.076320, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 9.951173782348633, Accuracy = 0.9222152829170227
Training iter #1686000:   Batch Loss = 8.097749, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 9.91757583618164, Accuracy = 0.9234598875045776
Training iter #1688000:   Batch Loss = 8.159344, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.104657173156738, Accuracy = 0.9222152829170227
Training iter #1690000:   Batch Loss = 8.537827, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.993768692016602, Accuracy = 0.9290603399276733
Training iter #1692000:   Batch Loss = 8.046086, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.951459884643555, Accuracy = 0.9247044324874878
Training iter #1694000:   Batch Loss = 8.027552, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.052021026611328, Accuracy = 0.914747953414917
Training iter #1696000:   Batch Loss = 8.318207, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.159815788269043, Accuracy = 0.9197261929512024
Training iter #1698000:   Batch Loss = 8.131291, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.912814140319824, Accuracy = 0.916614830493927
Training iter #1700000:   Batch Loss = 9.602580, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.000823974609375, Accuracy = 0.9172371029853821
Training iter #1702000:   Batch Loss = 8.172874, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.025382041931152, Accuracy = 0.9290603399276733
Training iter #1704000:   Batch Loss = 8.804717, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.173775672912598, Accuracy = 0.9197261929512024
Training iter #1706000:   Batch Loss = 8.635575, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 9.881671905517578, Accuracy = 0.9209707379341125
Training iter #1708000:   Batch Loss = 7.758988, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 9.995729446411133, Accuracy = 0.9228376150131226
Training iter #1710000:   Batch Loss = 7.930980, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.971708297729492, Accuracy = 0.9278157949447632
Training iter #1712000:   Batch Loss = 7.777305, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 9.891584396362305, Accuracy = 0.9278157949447632
Training iter #1714000:   Batch Loss = 8.281065, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.459080696105957, Accuracy = 0.9178593754768372
Training iter #1716000:   Batch Loss = 8.309875, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.632509231567383, Accuracy = 0.9091475009918213
Training iter #1718000:   Batch Loss = 8.872250, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.404728889465332, Accuracy = 0.9191039204597473
Training iter #1720000:   Batch Loss = 8.424691, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.276629447937012, Accuracy = 0.9103920459747314
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4300
Training iter #1722000:   Batch Loss = 8.017820, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.326192855834961, Accuracy = 0.9110143184661865
Training iter #1724000:   Batch Loss = 8.387737, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.146200180053711, Accuracy = 0.9097697734832764
Training iter #1726000:   Batch Loss = 8.239957, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.074724197387695, Accuracy = 0.9203484654426575
Training iter #1728000:   Batch Loss = 8.258794, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.247540473937988, Accuracy = 0.9128811359405518
Training iter #1730000:   Batch Loss = 8.101953, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.07104206085205, Accuracy = 0.9209707379341125
Training iter #1732000:   Batch Loss = 7.943457, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.060821533203125, Accuracy = 0.9215930104255676
Training iter #1734000:   Batch Loss = 7.016636, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 10.370001792907715, Accuracy = 0.9184816479682922
Training iter #1736000:   Batch Loss = 8.727974, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.304243087768555, Accuracy = 0.9159925580024719
Training iter #1738000:   Batch Loss = 7.845839, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.299016952514648, Accuracy = 0.9172371029853821
Training iter #1740000:   Batch Loss = 9.078155, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.539457321166992, Accuracy = 0.9172371029853821
Training iter #1742000:   Batch Loss = 8.731063, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.487220764160156, Accuracy = 0.9172371029853821
Training iter #1744000:   Batch Loss = 8.604386, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.459531784057617, Accuracy = 0.9103920459747314
Training iter #1746000:   Batch Loss = 9.126842, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.374197006225586, Accuracy = 0.9178593754768372
Training iter #1748000:   Batch Loss = 8.639547, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.521713256835938, Accuracy = 0.9178593754768372
Training iter #1750000:   Batch Loss = 8.736886, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.397746086120605, Accuracy = 0.9253267049789429
Training iter #1752000:   Batch Loss = 8.262320, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.227960586547852, Accuracy = 0.9203484654426575
Training iter #1754000:   Batch Loss = 8.363599, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.283524513244629, Accuracy = 0.9234598875045776
Training iter #1756000:   Batch Loss = 8.696748, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.452481269836426, Accuracy = 0.9203484654426575
Training iter #1758000:   Batch Loss = 8.822126, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.272489547729492, Accuracy = 0.925948977470398
Training iter #1760000:   Batch Loss = 9.020809, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.368738174438477, Accuracy = 0.9110143184661865
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4400
Training iter #1762000:   Batch Loss = 8.818013, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.367424011230469, Accuracy = 0.9153702259063721
Training iter #1764000:   Batch Loss = 8.691415, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.282219886779785, Accuracy = 0.9116365909576416
Training iter #1766000:   Batch Loss = 8.186960, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.265050888061523, Accuracy = 0.9122588634490967
Training iter #1768000:   Batch Loss = 8.860402, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.327609062194824, Accuracy = 0.9159925580024719
Training iter #1770000:   Batch Loss = 8.552074, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.414936065673828, Accuracy = 0.9110143184661865
Training iter #1772000:   Batch Loss = 8.310200, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.094823837280273, Accuracy = 0.9178593754768372
Training iter #1774000:   Batch Loss = 8.609426, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.148059844970703, Accuracy = 0.914747953414917
Training iter #1776000:   Batch Loss = 8.803606, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.224082946777344, Accuracy = 0.9153702259063721
Training iter #1778000:   Batch Loss = 8.293725, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.159704208374023, Accuracy = 0.9172371029853821
Training iter #1780000:   Batch Loss = 8.859510, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.760594367980957, Accuracy = 0.916614830493927
Training iter #1782000:   Batch Loss = 8.789507, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.328861236572266, Accuracy = 0.9228376150131226
Training iter #1784000:   Batch Loss = 8.513485, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.323504447937012, Accuracy = 0.9184816479682922
Training iter #1786000:   Batch Loss = 8.468897, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.222694396972656, Accuracy = 0.9228376150131226
Training iter #1788000:   Batch Loss = 8.240204, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.133001327514648, Accuracy = 0.9103920459747314
Training iter #1790000:   Batch Loss = 8.445314, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.269743919372559, Accuracy = 0.9116365909576416
Training iter #1792000:   Batch Loss = 8.479887, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.197382926940918, Accuracy = 0.9153702259063721
Training iter #1794000:   Batch Loss = 8.242544, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 9.948960304260254, Accuracy = 0.9209707379341125
Training iter #1796000:   Batch Loss = 8.986618, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.585543632507324, Accuracy = 0.914747953414917
Training iter #1798000:   Batch Loss = 8.319819, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.324934959411621, Accuracy = 0.9247044324874878
Training iter #1800000:   Batch Loss = 8.442293, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.244537353515625, Accuracy = 0.9135034084320068
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4500
Training iter #1802000:   Batch Loss = 8.680379, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.192756652832031, Accuracy = 0.9215930104255676
Training iter #1804000:   Batch Loss = 8.235092, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.115213394165039, Accuracy = 0.9234598875045776
Training iter #1806000:   Batch Loss = 8.479551, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.093472480773926, Accuracy = 0.9234598875045776
Training iter #1808000:   Batch Loss = 8.361695, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.082154273986816, Accuracy = 0.9197261929512024
Training iter #1810000:   Batch Loss = 7.821247, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.101053237915039, Accuracy = 0.9296826124191284
Training iter #1812000:   Batch Loss = 8.313657, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.989459991455078, Accuracy = 0.9303049445152283
Training iter #1814000:   Batch Loss = 8.220519, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.884666442871094, Accuracy = 0.9247044324874878
Training iter #1816000:   Batch Loss = 8.085371, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.106109619140625, Accuracy = 0.9153702259063721
Training iter #1818000:   Batch Loss = 8.374596, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.505044937133789, Accuracy = 0.9240821599960327
Training iter #1820000:   Batch Loss = 8.767244, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.236368179321289, Accuracy = 0.9184816479682922
Training iter #1822000:   Batch Loss = 8.188141, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.085038185119629, Accuracy = 0.9234598875045776
Training iter #1824000:   Batch Loss = 8.178642, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 9.827384948730469, Accuracy = 0.9303049445152283
Training iter #1826000:   Batch Loss = 7.945546, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 9.728440284729004, Accuracy = 0.9290603399276733
Training iter #1828000:   Batch Loss = 8.057344, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 9.820435523986816, Accuracy = 0.914747953414917
Training iter #1830000:   Batch Loss = 8.163025, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 9.840679168701172, Accuracy = 0.9184816479682922
Training iter #1832000:   Batch Loss = 8.030378, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 9.826273918151855, Accuracy = 0.926571249961853
Training iter #1834000:   Batch Loss = 7.639025, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 9.764533996582031, Accuracy = 0.9222152829170227
Training iter #1836000:   Batch Loss = 7.240802, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 9.836146354675293, Accuracy = 0.9228376150131226
Training iter #1838000:   Batch Loss = 8.168471, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 9.753666877746582, Accuracy = 0.9315494894981384
Training iter #1840000:   Batch Loss = 8.081875, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.93114185333252, Accuracy = 0.9234598875045776
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4600
Training iter #1842000:   Batch Loss = 8.090437, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 9.810720443725586, Accuracy = 0.9247044324874878
Training iter #1844000:   Batch Loss = 9.152231, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.086883544921875, Accuracy = 0.914747953414917
Training iter #1846000:   Batch Loss = 10.900736, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.234326362609863, Accuracy = 0.9178593754768372
Training iter #1848000:   Batch Loss = 10.088333, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.745760917663574, Accuracy = 0.9153702259063721
Training iter #1850000:   Batch Loss = 10.188307, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.509361267089844, Accuracy = 0.9247044324874878
Training iter #1852000:   Batch Loss = 9.455458, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.305078506469727, Accuracy = 0.926571249961853
Training iter #1854000:   Batch Loss = 10.089373, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.222702980041504, Accuracy = 0.9203484654426575
Training iter #1856000:   Batch Loss = 9.569546, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.33625602722168, Accuracy = 0.9184816479682922
Training iter #1858000:   Batch Loss = 9.992375, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.78017520904541, Accuracy = 0.9035469889640808
Training iter #1860000:   Batch Loss = 9.916475, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.438922882080078, Accuracy = 0.9085252285003662
Training iter #1862000:   Batch Loss = 9.839847, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.217151641845703, Accuracy = 0.914747953414917
Training iter #1864000:   Batch Loss = 9.387137, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.016032218933105, Accuracy = 0.9103920459747314
Training iter #1866000:   Batch Loss = 9.556597, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.90115737915039, Accuracy = 0.9159925580024719
Training iter #1868000:   Batch Loss = 9.466251, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.866756439208984, Accuracy = 0.9184816479682922
Training iter #1870000:   Batch Loss = 9.215203, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 10.742746353149414, Accuracy = 0.9172371029853821
Training iter #1872000:   Batch Loss = 8.701160, Accuracy = 0.987500011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.59783935546875, Accuracy = 0.9209707379341125
Training iter #1874000:   Batch Loss = 8.535713, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.653922080993652, Accuracy = 0.9153702259063721
Training iter #1876000:   Batch Loss = 9.068619, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.774676322937012, Accuracy = 0.9116365909576416
Training iter #1878000:   Batch Loss = 9.254360, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.776789665222168, Accuracy = 0.9172371029853821
Training iter #1880000:   Batch Loss = 9.285048, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.795269012451172, Accuracy = 0.9215930104255676
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4700
Training iter #1882000:   Batch Loss = 9.207599, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.705408096313477, Accuracy = 0.9215930104255676
Training iter #1884000:   Batch Loss = 8.906073, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.785029411315918, Accuracy = 0.9135034084320068
Training iter #1886000:   Batch Loss = 9.031143, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.628541946411133, Accuracy = 0.9122588634490967
Training iter #1888000:   Batch Loss = 8.605978, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.500245094299316, Accuracy = 0.9209707379341125
Training iter #1890000:   Batch Loss = 8.956143, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.725391387939453, Accuracy = 0.9178593754768372
Training iter #1892000:   Batch Loss = 9.701366, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.036030769348145, Accuracy = 0.914747953414917
Training iter #1894000:   Batch Loss = 9.252205, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.950227737426758, Accuracy = 0.9153702259063721
Training iter #1896000:   Batch Loss = 9.547683, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.944693565368652, Accuracy = 0.9197261929512024
Training iter #1898000:   Batch Loss = 9.246211, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.805973052978516, Accuracy = 0.9172371029853821
Training iter #1900000:   Batch Loss = 9.252226, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.779948234558105, Accuracy = 0.9072806239128113
Training iter #1902000:   Batch Loss = 9.261622, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.86233901977539, Accuracy = 0.9128811359405518
Training iter #1904000:   Batch Loss = 9.439384, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 10.734750747680664, Accuracy = 0.9159925580024719
Training iter #1906000:   Batch Loss = 9.198579, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.642452239990234, Accuracy = 0.9197261929512024
Training iter #1908000:   Batch Loss = 9.207924, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.545753479003906, Accuracy = 0.9172371029853821
Training iter #1910000:   Batch Loss = 8.863316, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.324813842773438, Accuracy = 0.916614830493927
Training iter #1912000:   Batch Loss = 8.916298, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.496293067932129, Accuracy = 0.9172371029853821
Training iter #1914000:   Batch Loss = 8.814439, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.338663101196289, Accuracy = 0.9228376150131226
Training iter #1916000:   Batch Loss = 8.679552, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.168152809143066, Accuracy = 0.9197261929512024
Training iter #1918000:   Batch Loss = 8.033510, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.193597793579102, Accuracy = 0.926571249961853
Training iter #1920000:   Batch Loss = 8.429916, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.192641258239746, Accuracy = 0.9290603399276733
Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-4800
Training iter #1922000:   Batch Loss = 8.540134, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.1222562789917, Accuracy = 0.9215930104255676
Training iter #1924000:   Batch Loss = 8.482989, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.188846588134766, Accuracy = 0.9234598875045776
Training iter #1926000:   Batch Loss = 8.325078, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.263500213623047, Accuracy = 0.9234598875045776
Training iter #1928000:   Batch Loss = 8.295242, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.243525505065918, Accuracy = 0.9234598875045776
Training iter #1930000:   Batch Loss = 8.419771, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.141606330871582, Accuracy = 0.9284380674362183
Training iter #1932000:   Batch Loss = 8.646528, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.245125770568848, Accuracy = 0.9296826124191284
Optimization Finished!
FINAL RESULT: Batch Loss = 10.330289840698242, Accuracy = 0.9271935224533081
All train time = 16947.623321056366
Final Model saved in file: ./lstm2/model_selftest_kfold1.ckpt-final
Precision: 92.67886021024638%
Recall: 92.71935283136278%
f1_score: 92.68698832027403%

Confusion Matrix:
[[152   1   1   0   0   1   0   0   2   4]
 [  0 159   0   0   0   0   1   0   1   0]
 [  1   0 144   1   0   2   1   3   9   0]
 [  1   2   1 142   4   0   1   9   1   1]
 [  0   1   0   0 158   0   0   1   0   0]
 [  0   0   1   0   0 162   0   0   0   0]
 [  1   1   5   3   0   0 145   5   1   0]
 [  0   0   1  10   0   0  11 136   2   1]
 [  0   0  13   2   0   3   1   0 136   0]
 [  6   0   0   0   0   0   0   0   0 156]]

Confusion matrix (normalised to % of total test data):
[[ 9.458618    0.06222776  0.06222776  0.          0.          0.06222776
   0.          0.          0.12445551  0.24891102]
 [ 0.          9.894213    0.          0.          0.          0.
   0.06222776  0.          0.06222776  0.        ]
 [ 0.06222776  0.          8.960796    0.06222776  0.          0.12445551
   0.06222776  0.18668327  0.5600498   0.        ]
 [ 0.06222776  0.12445551  0.06222776  8.836341    0.24891102  0.
   0.06222776  0.5600498   0.06222776  0.06222776]
 [ 0.          0.06222776  0.          0.          9.831985    0.
   0.          0.06222776  0.          0.        ]
 [ 0.          0.          0.06222776  0.          0.         10.080896
   0.          0.          0.          0.        ]
 [ 0.06222776  0.06222776  0.31113875  0.18668327  0.          0.
   9.023025    0.31113875  0.06222776  0.        ]
 [ 0.          0.          0.06222776  0.6222775   0.          0.
   0.6845053   8.462975    0.12445551  0.06222776]
 [ 0.          0.          0.80896074  0.12445551  0.          0.18668327
   0.06222776  0.          8.462975    0.        ]
 [ 0.37336653  0.          0.          0.          0.          0.
   0.          0.          0.          9.70753   ]]/home/sunrepe/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))

Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.

WARNING:tensorflow:From all_three_lstm.py:91: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:93: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From all_three_lstm.py:94: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From all_three_lstm.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From all_three_lstm.py:97: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
loading data...
train: 6446 test: 1607
load data time: 198.94242238998413
Start train!
Training iter #400:   Batch Loss = 26.809155, Accuracy = 0.10499999672174454
PERFORMANCE ON TEST SET: Batch Loss = 26.368648529052734, Accuracy = 0.16179215908050537
Training iter #2000:   Batch Loss = 25.265766, Accuracy = 0.3400000035762787
PERFORMANCE ON TEST SET: Batch Loss = 24.940738677978516, Accuracy = 0.35967642068862915
Training iter #4000:   Batch Loss = 23.531824, Accuracy = 0.3449999988079071
PERFORMANCE ON TEST SET: Batch Loss = 23.15350341796875, Accuracy = 0.3708774149417877
Training iter #6000:   Batch Loss = 22.133326, Accuracy = 0.38999998569488525
PERFORMANCE ON TEST SET: Batch Loss = 21.52592658996582, Accuracy = 0.467952698469162
Training iter #8000:   Batch Loss = 20.498863, Accuracy = 0.44999998807907104
PERFORMANCE ON TEST SET: Batch Loss = 20.31862449645996, Accuracy = 0.5239576697349548
Training iter #10000:   Batch Loss = 19.369583, Accuracy = 0.5375000238418579
PERFORMANCE ON TEST SET: Batch Loss = 19.253746032714844, Accuracy = 0.5451151132583618
Training iter #12000:   Batch Loss = 18.328375, Accuracy = 0.5950000286102295
PERFORMANCE ON TEST SET: Batch Loss = 18.227951049804688, Accuracy = 0.5855631828308105
Training iter #14000:   Batch Loss = 17.568874, Accuracy = 0.6100000143051147
PERFORMANCE ON TEST SET: Batch Loss = 17.6256046295166, Accuracy = 0.598630964756012
Training iter #16000:   Batch Loss = 17.145254, Accuracy = 0.6549999713897705
PERFORMANCE ON TEST SET: Batch Loss = 17.012550354003906, Accuracy = 0.6297448873519897
Training iter #18000:   Batch Loss = 16.430729, Accuracy = 0.6549999713897705
PERFORMANCE ON TEST SET: Batch Loss = 16.431245803833008, Accuracy = 0.6558805108070374
Training iter #20000:   Batch Loss = 15.650364, Accuracy = 0.6850000023841858
PERFORMANCE ON TEST SET: Batch Loss = 15.895313262939453, Accuracy = 0.6776602268218994
Training iter #22000:   Batch Loss = 15.639266, Accuracy = 0.6949999928474426
PERFORMANCE ON TEST SET: Batch Loss = 15.529619216918945, Accuracy = 0.673926591873169
Training iter #24000:   Batch Loss = 14.526580, Accuracy = 0.7350000143051147
PERFORMANCE ON TEST SET: Batch Loss = 15.101158142089844, Accuracy = 0.6919726133346558
Training iter #26000:   Batch Loss = 15.069711, Accuracy = 0.6725000143051147
PERFORMANCE ON TEST SET: Batch Loss = 15.083988189697266, Accuracy = 0.6950840353965759
Training iter #28000:   Batch Loss = 15.457292, Accuracy = 0.6349999904632568
PERFORMANCE ON TEST SET: Batch Loss = 14.876920700073242, Accuracy = 0.6932171583175659
Training iter #30000:   Batch Loss = 14.584318, Accuracy = 0.7024999856948853
PERFORMANCE ON TEST SET: Batch Loss = 14.773193359375, Accuracy = 0.7143746018409729
Training iter #32000:   Batch Loss = 14.137165, Accuracy = 0.7149999737739563
PERFORMANCE ON TEST SET: Batch Loss = 14.738195419311523, Accuracy = 0.7137523293495178
Training iter #34000:   Batch Loss = 14.638150, Accuracy = 0.782608687877655
PERFORMANCE ON TEST SET: Batch Loss = 14.582840919494629, Accuracy = 0.7069072723388672
Training iter #36000:   Batch Loss = 13.596129, Accuracy = 0.7475000023841858
PERFORMANCE ON TEST SET: Batch Loss = 14.20283317565918, Accuracy = 0.7087740898132324
Training iter #38000:   Batch Loss = 14.030041, Accuracy = 0.6949999928474426
PERFORMANCE ON TEST SET: Batch Loss = 14.064361572265625, Accuracy = 0.7100186944007874
Training iter #40000:   Batch Loss = 14.275158, Accuracy = 0.7124999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.70388126373291, Accuracy = 0.7349097728729248
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-100
Training iter #42000:   Batch Loss = 13.457955, Accuracy = 0.7475000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.653085708618164, Accuracy = 0.7386434078216553
Training iter #44000:   Batch Loss = 12.829492, Accuracy = 0.7524999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.613286018371582, Accuracy = 0.7373988628387451
Training iter #46000:   Batch Loss = 13.284651, Accuracy = 0.7425000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.624740600585938, Accuracy = 0.7305538058280945
Training iter #48000:   Batch Loss = 14.056502, Accuracy = 0.6825000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.599201202392578, Accuracy = 0.7280647158622742
Training iter #50000:   Batch Loss = 13.127474, Accuracy = 0.7425000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.301979064941406, Accuracy = 0.7373988628387451
Training iter #52000:   Batch Loss = 12.468272, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.196271896362305, Accuracy = 0.7498444318771362
Training iter #54000:   Batch Loss = 12.676905, Accuracy = 0.7825000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.930368423461914, Accuracy = 0.7492221593856812
Training iter #56000:   Batch Loss = 12.484352, Accuracy = 0.7674999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.908414840698242, Accuracy = 0.7585563063621521
Training iter #58000:   Batch Loss = 12.656884, Accuracy = 0.7875000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.909290313720703, Accuracy = 0.7510889768600464
Training iter #60000:   Batch Loss = 12.522546, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.306424140930176, Accuracy = 0.7405102849006653
Training iter #62000:   Batch Loss = 12.884328, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.692588806152344, Accuracy = 0.7311760783195496
Training iter #64000:   Batch Loss = 13.384004, Accuracy = 0.7749999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.53345775604248, Accuracy = 0.7436216473579407
Training iter #66000:   Batch Loss = 13.339392, Accuracy = 0.737500011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.327392578125, Accuracy = 0.7510889768600464
Training iter #68000:   Batch Loss = 13.692473, Accuracy = 0.782608687877655
PERFORMANCE ON TEST SET: Batch Loss = 13.519214630126953, Accuracy = 0.7485998868942261
Training iter #70000:   Batch Loss = 13.412560, Accuracy = 0.7699999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.278336524963379, Accuracy = 0.7510889768600464
Training iter #72000:   Batch Loss = 12.228554, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.08267593383789, Accuracy = 0.7622900009155273
Training iter #74000:   Batch Loss = 12.779450, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.8911714553833, Accuracy = 0.7610454559326172
Training iter #76000:   Batch Loss = 12.152365, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.955113410949707, Accuracy = 0.7510889768600464
Training iter #78000:   Batch Loss = 12.273499, Accuracy = 0.7774999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.943929672241211, Accuracy = 0.7598008513450623
Training iter #80000:   Batch Loss = 12.273814, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.814627647399902, Accuracy = 0.7616677284240723
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-200
Training iter #82000:   Batch Loss = 12.164629, Accuracy = 0.7749999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.125344276428223, Accuracy = 0.7504667043685913
Training iter #84000:   Batch Loss = 12.899180, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.176072120666504, Accuracy = 0.7591785788536072
Training iter #86000:   Batch Loss = 12.703840, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.690671920776367, Accuracy = 0.7778469324111938
Training iter #88000:   Batch Loss = 11.792133, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.466773986816406, Accuracy = 0.7815805673599243
Training iter #90000:   Batch Loss = 11.815222, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.446548461914062, Accuracy = 0.7784692049026489
Training iter #92000:   Batch Loss = 12.462042, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.312621116638184, Accuracy = 0.7734909653663635
Training iter #94000:   Batch Loss = 11.448335, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.150278091430664, Accuracy = 0.7878033518791199
Training iter #96000:   Batch Loss = 11.696230, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.408222198486328, Accuracy = 0.7902924418449402
Training iter #98000:   Batch Loss = 11.545171, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.485921859741211, Accuracy = 0.7747355103492737
Training iter #100000:   Batch Loss = 11.880753, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.289825439453125, Accuracy = 0.7722464203834534
Training iter #102000:   Batch Loss = 10.593378, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 12.274237632751465, Accuracy = 0.7828251123428345
Training iter #104000:   Batch Loss = 11.696782, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 12.314345359802246, Accuracy = 0.7865588068962097
Training iter #106000:   Batch Loss = 11.404393, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.152690887451172, Accuracy = 0.7859365344047546
Training iter #108000:   Batch Loss = 11.773396, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.048930168151855, Accuracy = 0.7809582948684692
Training iter #110000:   Batch Loss = 11.471361, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.894311904907227, Accuracy = 0.79091477394104
Training iter #112000:   Batch Loss = 11.270969, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.829023361206055, Accuracy = 0.79091477394104
Training iter #114000:   Batch Loss = 10.702999, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.761980056762695, Accuracy = 0.7990043759346008
Training iter #116000:   Batch Loss = 10.824759, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.909308433532715, Accuracy = 0.7927815914154053
Training iter #118000:   Batch Loss = 11.465499, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.11568546295166, Accuracy = 0.7902924418449402
Training iter #120000:   Batch Loss = 11.574583, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.186240196228027, Accuracy = 0.78904789686203
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-300
Training iter #122000:   Batch Loss = 11.184317, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.014080047607422, Accuracy = 0.7840697169303894
Training iter #124000:   Batch Loss = 11.122267, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.829527854919434, Accuracy = 0.7846919894218445
Training iter #126000:   Batch Loss = 11.259952, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.608916282653809, Accuracy = 0.7934038639068604
Training iter #128000:   Batch Loss = 10.856157, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.691034317016602, Accuracy = 0.7797137498855591
Training iter #130000:   Batch Loss = 10.843134, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.936258316040039, Accuracy = 0.788425624370575
Training iter #132000:   Batch Loss = 11.279525, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.736135482788086, Accuracy = 0.8008711934089661
Training iter #134000:   Batch Loss = 10.420116, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.5519437789917, Accuracy = 0.7934038639068604
Training iter #136000:   Batch Loss = 10.404902, Accuracy = 0.8260869383811951
PERFORMANCE ON TEST SET: Batch Loss = 11.707541465759277, Accuracy = 0.8089607954025269
Training iter #138000:   Batch Loss = 10.570314, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.524022102355957, Accuracy = 0.78904789686203
Training iter #140000:   Batch Loss = 10.750147, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.544179916381836, Accuracy = 0.8046048283576965
Training iter #142000:   Batch Loss = 11.328656, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.469412803649902, Accuracy = 0.8077162504196167
Training iter #144000:   Batch Loss = 10.931677, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.945557594299316, Accuracy = 0.8108276128768921
Training iter #146000:   Batch Loss = 11.167354, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.835309982299805, Accuracy = 0.8064717054367065
Training iter #148000:   Batch Loss = 11.261382, Accuracy = 0.8100000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.893503189086914, Accuracy = 0.8058494329452515
Training iter #150000:   Batch Loss = 10.888502, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.90549087524414, Accuracy = 0.8126944899559021
Training iter #152000:   Batch Loss = 10.585194, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.565606117248535, Accuracy = 0.8070939779281616
Training iter #154000:   Batch Loss = 10.182953, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.43405532836914, Accuracy = 0.8095830678939819
Training iter #156000:   Batch Loss = 11.072912, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.814876556396484, Accuracy = 0.8021157383918762
Training iter #158000:   Batch Loss = 11.591554, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.940540313720703, Accuracy = 0.8014934659004211
Training iter #160000:   Batch Loss = 11.056122, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.853029251098633, Accuracy = 0.8064717054367065
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-400
Training iter #162000:   Batch Loss = 10.838776, Accuracy = 0.8349999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.738590240478516, Accuracy = 0.8095830678939819
Training iter #164000:   Batch Loss = 10.815954, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.811906814575195, Accuracy = 0.8108276128768921
Training iter #166000:   Batch Loss = 10.929211, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.709814071655273, Accuracy = 0.8164281249046326
Training iter #168000:   Batch Loss = 11.373582, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.692359924316406, Accuracy = 0.8126944899559021
Training iter #170000:   Batch Loss = 10.650101, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 11.674149513244629, Accuracy = 0.8170503973960876
Training iter #172000:   Batch Loss = 10.920814, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.6005277633667, Accuracy = 0.8077162504196167
Training iter #174000:   Batch Loss = 10.735010, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.790595054626465, Accuracy = 0.8151835799217224
Training iter #176000:   Batch Loss = 10.828322, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.881399154663086, Accuracy = 0.819539487361908
Training iter #178000:   Batch Loss = 11.248239, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.111278533935547, Accuracy = 0.8226509094238281
Training iter #180000:   Batch Loss = 10.680972, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.92341136932373, Accuracy = 0.8089607954025269
Training iter #182000:   Batch Loss = 11.014099, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.749921798706055, Accuracy = 0.8164281249046326
Training iter #184000:   Batch Loss = 10.588558, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.773449897766113, Accuracy = 0.8139390349388123
Training iter #186000:   Batch Loss = 10.993527, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.69318962097168, Accuracy = 0.8139390349388123
Training iter #188000:   Batch Loss = 10.530210, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.63299560546875, Accuracy = 0.8158058524131775
Training iter #190000:   Batch Loss = 9.887354, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.402798652648926, Accuracy = 0.8288736939430237
Training iter #192000:   Batch Loss = 10.286709, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.389627456665039, Accuracy = 0.8238954544067383
Training iter #194000:   Batch Loss = 11.186231, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.374841690063477, Accuracy = 0.8288736939430237
Training iter #196000:   Batch Loss = 10.466249, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.300544738769531, Accuracy = 0.8232731819152832
Training iter #198000:   Batch Loss = 10.594739, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.304436683654785, Accuracy = 0.8263845443725586
Training iter #200000:   Batch Loss = 9.746642, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.150057792663574, Accuracy = 0.8276291489601135
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-500
Training iter #202000:   Batch Loss = 10.008322, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.053574562072754, Accuracy = 0.8338518738746643
Training iter #204000:   Batch Loss = 10.181614, Accuracy = 0.804347813129425
PERFORMANCE ON TEST SET: Batch Loss = 10.920462608337402, Accuracy = 0.8301182389259338
Training iter #206000:   Batch Loss = 10.067409, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.24475383758545, Accuracy = 0.821406364440918
Training iter #208000:   Batch Loss = 10.664762, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.439720153808594, Accuracy = 0.8245177268981934
Training iter #210000:   Batch Loss = 10.294057, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.245692253112793, Accuracy = 0.8276291489601135
Training iter #212000:   Batch Loss = 10.185104, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.33165168762207, Accuracy = 0.8232731819152832
Training iter #214000:   Batch Loss = 10.280683, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.394002914428711, Accuracy = 0.8263845443725586
Training iter #216000:   Batch Loss = 10.852396, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.641721725463867, Accuracy = 0.8226509094238281
Training iter #218000:   Batch Loss = 10.658767, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.615900039672852, Accuracy = 0.8201618194580078
Training iter #220000:   Batch Loss = 11.294033, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.533992767333984, Accuracy = 0.8288736939430237
Training iter #222000:   Batch Loss = 10.622175, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.714051246643066, Accuracy = 0.8363410234451294
Training iter #224000:   Batch Loss = 10.942373, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.660938262939453, Accuracy = 0.8294959664344788
Training iter #226000:   Batch Loss = 10.701745, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.544891357421875, Accuracy = 0.8176726698875427
Training iter #228000:   Batch Loss = 9.790766, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.262518882751465, Accuracy = 0.8344741463661194
Training iter #230000:   Batch Loss = 10.257985, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.257471084594727, Accuracy = 0.8257622718811035
Training iter #232000:   Batch Loss = 10.665651, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.260767936706543, Accuracy = 0.8326073288917542
Training iter #234000:   Batch Loss = 9.933495, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.249249458312988, Accuracy = 0.8301182389259338
Training iter #236000:   Batch Loss = 10.256802, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.225422859191895, Accuracy = 0.8450528979301453
Training iter #238000:   Batch Loss = 10.571115, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 11.568354606628418, Accuracy = 0.8294959664344788
Training iter #240000:   Batch Loss = 10.221585, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.466609954833984, Accuracy = 0.8338518738746643
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-600
Training iter #242000:   Batch Loss = 10.006326, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.186386108398438, Accuracy = 0.8406969308853149
Training iter #244000:   Batch Loss = 10.377129, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.864940643310547, Accuracy = 0.819539487361908
Training iter #246000:   Batch Loss = 11.588375, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.471433639526367, Accuracy = 0.8350964784622192
Training iter #248000:   Batch Loss = 10.915305, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.217716217041016, Accuracy = 0.8263845443725586
Training iter #250000:   Batch Loss = 11.124669, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.20513916015625, Accuracy = 0.8406969308853149
Training iter #252000:   Batch Loss = 11.433067, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.305594444274902, Accuracy = 0.8382078409194946
Training iter #254000:   Batch Loss = 11.152555, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.856254577636719, Accuracy = 0.8444306254386902
Training iter #256000:   Batch Loss = 10.903572, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.735931396484375, Accuracy = 0.8363410234451294
Training iter #258000:   Batch Loss = 10.538745, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.488134384155273, Accuracy = 0.8406969308853149
Training iter #260000:   Batch Loss = 10.495829, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.614164352416992, Accuracy = 0.8388301134109497
Training iter #262000:   Batch Loss = 10.942167, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.803191184997559, Accuracy = 0.8375855684280396
Training iter #264000:   Batch Loss = 11.220536, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.413368225097656, Accuracy = 0.8319850564002991
Training iter #266000:   Batch Loss = 10.888006, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.02004623413086, Accuracy = 0.8350964784622192
Training iter #268000:   Batch Loss = 10.883049, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.904129028320312, Accuracy = 0.8369632959365845
Training iter #270000:   Batch Loss = 10.565952, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.830042839050293, Accuracy = 0.8238954544067383
Training iter #272000:   Batch Loss = 11.806708, Accuracy = 0.8695651888847351
PERFORMANCE ON TEST SET: Batch Loss = 11.831819534301758, Accuracy = 0.8394523859024048
Training iter #274000:   Batch Loss = 11.224586, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.909407615661621, Accuracy = 0.8419415354728699
Training iter #276000:   Batch Loss = 10.744903, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.825268745422363, Accuracy = 0.8456751704216003
Training iter #278000:   Batch Loss = 10.885857, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.90944766998291, Accuracy = 0.8363410234451294
Training iter #280000:   Batch Loss = 11.058624, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.936186790466309, Accuracy = 0.8319850564002991
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-700
Training iter #282000:   Batch Loss = 10.718901, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.703843116760254, Accuracy = 0.8363410234451294
Training iter #284000:   Batch Loss = 10.610275, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.490958213806152, Accuracy = 0.8438083529472351
Training iter #286000:   Batch Loss = 10.522971, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.530052185058594, Accuracy = 0.84131920337677
Training iter #288000:   Batch Loss = 10.138371, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.362403869628906, Accuracy = 0.8500311374664307
Training iter #290000:   Batch Loss = 10.046557, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.400139808654785, Accuracy = 0.8394523859024048
Training iter #292000:   Batch Loss = 9.941803, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.330988883972168, Accuracy = 0.8382078409194946
Training iter #294000:   Batch Loss = 10.415427, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.355915069580078, Accuracy = 0.8388301134109497
Training iter #296000:   Batch Loss = 10.112221, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.243541717529297, Accuracy = 0.8494088649749756
Training iter #298000:   Batch Loss = 10.254567, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.170557022094727, Accuracy = 0.8438083529472351
Training iter #300000:   Batch Loss = 10.274229, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.525769233703613, Accuracy = 0.842563807964325
Training iter #302000:   Batch Loss = 10.620743, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.350200653076172, Accuracy = 0.8481642603874207
Training iter #304000:   Batch Loss = 10.145767, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.453678131103516, Accuracy = 0.8400746583938599
Training iter #306000:   Batch Loss = 10.933829, Accuracy = 0.8260869383811951
PERFORMANCE ON TEST SET: Batch Loss = 11.430394172668457, Accuracy = 0.8332296013832092
Training iter #308000:   Batch Loss = 9.999470, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.24302864074707, Accuracy = 0.8394523859024048
Training iter #310000:   Batch Loss = 10.219728, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.291487693786621, Accuracy = 0.8375855684280396
Training iter #312000:   Batch Loss = 10.436199, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.354198455810547, Accuracy = 0.8475419878959656
Training iter #314000:   Batch Loss = 10.355454, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.36274242401123, Accuracy = 0.842563807964325
Training iter #316000:   Batch Loss = 10.645956, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.45848560333252, Accuracy = 0.8350964784622192
Training iter #318000:   Batch Loss = 10.404950, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.242215156555176, Accuracy = 0.8475419878959656
Training iter #320000:   Batch Loss = 10.007485, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.32070541381836, Accuracy = 0.852520227432251
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-800
Training iter #322000:   Batch Loss = 9.994865, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.07400131225586, Accuracy = 0.8518979549407959
Training iter #324000:   Batch Loss = 10.160233, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.033432960510254, Accuracy = 0.852520227432251
Training iter #326000:   Batch Loss = 10.751212, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.466943740844727, Accuracy = 0.8363410234451294
Training iter #328000:   Batch Loss = 9.921751, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.661331176757812, Accuracy = 0.8350964784622192
Training iter #330000:   Batch Loss = 10.256688, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.273611068725586, Accuracy = 0.8462974429130554
Training iter #332000:   Batch Loss = 9.908112, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.08021354675293, Accuracy = 0.8469197154045105
Training iter #334000:   Batch Loss = 9.640226, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.213337898254395, Accuracy = 0.8450528979301453
Training iter #336000:   Batch Loss = 10.588518, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.0819091796875, Accuracy = 0.8543870449066162
Training iter #338000:   Batch Loss = 10.044653, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.103253364562988, Accuracy = 0.853142499923706
Training iter #340000:   Batch Loss = 10.833184, Accuracy = 0.8478260636329651
PERFORMANCE ON TEST SET: Batch Loss = 11.177714347839355, Accuracy = 0.8456751704216003
Training iter #342000:   Batch Loss = 10.084596, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.032953262329102, Accuracy = 0.8506534099578857
Training iter #344000:   Batch Loss = 9.635911, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.949786186218262, Accuracy = 0.8512756824493408
Training iter #346000:   Batch Loss = 9.311638, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.780929565429688, Accuracy = 0.8512756824493408
Training iter #348000:   Batch Loss = 9.648939, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.045705795288086, Accuracy = 0.852520227432251
Training iter #350000:   Batch Loss = 9.964943, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.061698913574219, Accuracy = 0.8500311374664307
Training iter #352000:   Batch Loss = 10.089248, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.91784954071045, Accuracy = 0.8637211918830872
Training iter #354000:   Batch Loss = 9.813372, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.162906646728516, Accuracy = 0.8481642603874207
Training iter #356000:   Batch Loss = 10.157286, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.785578727722168, Accuracy = 0.853142499923706
Training iter #358000:   Batch Loss = 9.414836, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.63762092590332, Accuracy = 0.8630989193916321
Training iter #360000:   Batch Loss = 9.181709, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.596037864685059, Accuracy = 0.8618543744087219
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-900
Training iter #362000:   Batch Loss = 9.338504, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.65182113647461, Accuracy = 0.8556315898895264
Training iter #364000:   Batch Loss = 8.954401, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.439419746398926, Accuracy = 0.8612321019172668
Training iter #366000:   Batch Loss = 9.024481, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.544896125793457, Accuracy = 0.8606098294258118
Training iter #368000:   Batch Loss = 9.067640, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.025318145751953, Accuracy = 0.8581207394599915
Training iter #370000:   Batch Loss = 9.613482, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.645708084106445, Accuracy = 0.8568761944770813
Training iter #372000:   Batch Loss = 9.102748, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.790237426757812, Accuracy = 0.8581207394599915
Training iter #374000:   Batch Loss = 11.039555, Accuracy = 0.8260869383811951
PERFORMANCE ON TEST SET: Batch Loss = 11.067317962646484, Accuracy = 0.8518979549407959
Training iter #376000:   Batch Loss = 9.735588, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.236268043518066, Accuracy = 0.8375855684280396
Training iter #378000:   Batch Loss = 9.966957, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.045173645019531, Accuracy = 0.864343523979187
Training iter #380000:   Batch Loss = 9.913864, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.887304306030273, Accuracy = 0.8543870449066162
Training iter #382000:   Batch Loss = 9.709402, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.94792366027832, Accuracy = 0.8562538623809814
Training iter #384000:   Batch Loss = 9.571236, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.85462760925293, Accuracy = 0.8630989193916321
Training iter #386000:   Batch Loss = 9.233975, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.771408081054688, Accuracy = 0.8618543744087219
Training iter #388000:   Batch Loss = 9.462157, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.927854537963867, Accuracy = 0.8587430119514465
Training iter #390000:   Batch Loss = 8.661260, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.696982383728027, Accuracy = 0.8506534099578857
Training iter #392000:   Batch Loss = 9.760706, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.74074935913086, Accuracy = 0.8655880689620972
Training iter #394000:   Batch Loss = 9.242519, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.48411750793457, Accuracy = 0.8612321019172668
Training iter #396000:   Batch Loss = 9.414687, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.894906044006348, Accuracy = 0.8649657964706421
Training iter #398000:   Batch Loss = 9.341260, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.767388343811035, Accuracy = 0.8686994314193726
Training iter #400000:   Batch Loss = 9.233613, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.644821166992188, Accuracy = 0.8693217039108276
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1000
Training iter #402000:   Batch Loss = 9.564522, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.795916557312012, Accuracy = 0.8550093173980713
Training iter #404000:   Batch Loss = 9.835505, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.619063377380371, Accuracy = 0.8618543744087219
Training iter #406000:   Batch Loss = 8.979864, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.6397705078125, Accuracy = 0.8674548864364624
Training iter #408000:   Batch Loss = 8.574532, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.716788291931152, Accuracy = 0.8637211918830872
Training iter #410000:   Batch Loss = 9.009330, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.540149688720703, Accuracy = 0.8699439764022827
Training iter #412000:   Batch Loss = 9.160160, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.504761695861816, Accuracy = 0.8693217039108276
Training iter #414000:   Batch Loss = 9.093631, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.463348388671875, Accuracy = 0.8693217039108276
Training iter #416000:   Batch Loss = 9.803446, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.821144104003906, Accuracy = 0.8506534099578857
Training iter #418000:   Batch Loss = 8.965724, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.73391056060791, Accuracy = 0.8699439764022827
Training iter #420000:   Batch Loss = 9.209682, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.704703330993652, Accuracy = 0.864343523979187
Training iter #422000:   Batch Loss = 9.598875, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.070075988769531, Accuracy = 0.8680771589279175
Training iter #424000:   Batch Loss = 9.595883, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.217279434204102, Accuracy = 0.8693217039108276
Training iter #426000:   Batch Loss = 12.367370, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 15.045772552490234, Accuracy = 0.8176726698875427
Training iter #428000:   Batch Loss = 13.584135, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 14.232580184936523, Accuracy = 0.8350964784622192
Training iter #430000:   Batch Loss = 12.795858, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.646249771118164, Accuracy = 0.8388301134109497
Training iter #432000:   Batch Loss = 12.042597, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.046510696411133, Accuracy = 0.8462974429130554
Training iter #434000:   Batch Loss = 12.129712, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.735072135925293, Accuracy = 0.8518979549407959
Training iter #436000:   Batch Loss = 11.553019, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.636758804321289, Accuracy = 0.8574984669685364
Training iter #438000:   Batch Loss = 11.420954, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.4714937210083, Accuracy = 0.8562538623809814
Training iter #440000:   Batch Loss = 11.285757, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.298315048217773, Accuracy = 0.8438083529472351
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1100
Training iter #442000:   Batch Loss = 10.833680, Accuracy = 0.8695651888847351
PERFORMANCE ON TEST SET: Batch Loss = 12.162591934204102, Accuracy = 0.8363410234451294
Training iter #444000:   Batch Loss = 11.285264, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.146890640258789, Accuracy = 0.8469197154045105
Training iter #446000:   Batch Loss = 10.840170, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.007131576538086, Accuracy = 0.8444306254386902
Training iter #448000:   Batch Loss = 10.478378, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.122445106506348, Accuracy = 0.842563807964325
Training iter #450000:   Batch Loss = 11.205978, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.050495147705078, Accuracy = 0.8475419878959656
Training iter #452000:   Batch Loss = 10.678408, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.070700645446777, Accuracy = 0.8500311374664307
Training iter #454000:   Batch Loss = 11.353840, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.83255672454834, Accuracy = 0.8450528979301453
Training iter #456000:   Batch Loss = 11.085919, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.36695671081543, Accuracy = 0.8450528979301453
Training iter #458000:   Batch Loss = 10.890465, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.269667625427246, Accuracy = 0.8245177268981934
Training iter #460000:   Batch Loss = 11.050100, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.242016792297363, Accuracy = 0.842563807964325
Training iter #462000:   Batch Loss = 10.846478, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.37007999420166, Accuracy = 0.8438083529472351
Training iter #464000:   Batch Loss = 10.791546, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.54759407043457, Accuracy = 0.8481642603874207
Training iter #466000:   Batch Loss = 11.018861, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.628229141235352, Accuracy = 0.8394523859024048
Training iter #468000:   Batch Loss = 11.189746, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.343072891235352, Accuracy = 0.8556315898895264
Training iter #470000:   Batch Loss = 11.665343, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.409305572509766, Accuracy = 0.8469197154045105
Training iter #472000:   Batch Loss = 10.889921, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.233644485473633, Accuracy = 0.8593652844429016
Training iter #474000:   Batch Loss = 10.696321, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.950393676757812, Accuracy = 0.8518979549407959
Training iter #476000:   Batch Loss = 10.947649, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 11.951356887817383, Accuracy = 0.84131920337677
Training iter #478000:   Batch Loss = 10.157078, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.851235389709473, Accuracy = 0.8456751704216003
Training iter #480000:   Batch Loss = 10.866859, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.837779998779297, Accuracy = 0.8462974429130554
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1200
Training iter #482000:   Batch Loss = 10.437345, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.597579956054688, Accuracy = 0.8587430119514465
Training iter #484000:   Batch Loss = 10.354162, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.653173446655273, Accuracy = 0.8568761944770813
Training iter #486000:   Batch Loss = 10.403961, Accuracy = 0.8999999761581421WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.

PERFORMANCE ON TEST SET: Batch Loss = 11.594229698181152, Accuracy = 0.8587430119514465
Training iter #488000:   Batch Loss = 10.609968, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.613380432128906, Accuracy = 0.8562538623809814
Training iter #490000:   Batch Loss = 10.338856, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.47105884552002, Accuracy = 0.8550093173980713
Training iter #492000:   Batch Loss = 10.782427, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.921792984008789, Accuracy = 0.8550093173980713
Training iter #494000:   Batch Loss = 10.737452, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.772347450256348, Accuracy = 0.8469197154045105
Training iter #496000:   Batch Loss = 10.275462, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.831766128540039, Accuracy = 0.8574984669685364
Training iter #498000:   Batch Loss = 10.641188, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.82579517364502, Accuracy = 0.8618543744087219
Training iter #500000:   Batch Loss = 10.490741, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.694131851196289, Accuracy = 0.8562538623809814
Training iter #502000:   Batch Loss = 10.517784, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.61117172241211, Accuracy = 0.8587430119514465
Training iter #504000:   Batch Loss = 10.561914, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.602935791015625, Accuracy = 0.862476646900177
Training iter #506000:   Batch Loss = 10.160519, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.294831275939941, Accuracy = 0.8593652844429016
Training iter #508000:   Batch Loss = 10.081665, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.486132621765137, Accuracy = 0.862476646900177
Training iter #510000:   Batch Loss = 9.386965, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.393431663513184, Accuracy = 0.8662103414535522
Training iter #512000:   Batch Loss = 10.004574, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.475220680236816, Accuracy = 0.8556315898895264
Training iter #514000:   Batch Loss = 10.085806, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.373194694519043, Accuracy = 0.8649657964706421
Training iter #516000:   Batch Loss = 9.677418, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.110482215881348, Accuracy = 0.8674548864364624
Training iter #518000:   Batch Loss = 10.342128, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.275150299072266, Accuracy = 0.8618543744087219
Training iter #520000:   Batch Loss = 9.513624, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.255325317382812, Accuracy = 0.864343523979187
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1300
Training iter #522000:   Batch Loss = 10.600249, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.306233406066895, Accuracy = 0.862476646900177
Training iter #524000:   Batch Loss = 11.061008, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.5401029586792, Accuracy = 0.8500311374664307
Training iter #526000:   Batch Loss = 11.325053, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.068039894104004, Accuracy = 0.8574984669685364
Training iter #528000:   Batch Loss = 11.029697, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.856776237487793, Accuracy = 0.862476646900177
Training iter #530000:   Batch Loss = 10.670567, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.901718139648438, Accuracy = 0.8662103414535522
Training iter #532000:   Batch Loss = 10.395470, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.837972640991211, Accuracy = 0.8574984669685364
Training iter #534000:   Batch Loss = 10.448574, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.813007354736328, Accuracy = 0.8581207394599915
Training iter #536000:   Batch Loss = 10.399638, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.624791145324707, Accuracy = 0.8655880689620972
Training iter #538000:   Batch Loss = 10.141317, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.749578475952148, Accuracy = 0.8599875569343567
Training iter #540000:   Batch Loss = 10.393812, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.807788848876953, Accuracy = 0.8593652844429016
Training iter #542000:   Batch Loss = 10.125658, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.86114501953125, Accuracy = 0.8550093173980713
Training iter #544000:   Batch Loss = 10.095062, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 12.197006225585938, Accuracy = 0.8574984669685364
Training iter #546000:   Batch Loss = 11.400242, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.593680381774902, Accuracy = 0.8456751704216003
Training iter #548000:   Batch Loss = 11.567743, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.080994606018066, Accuracy = 0.8494088649749756
Training iter #550000:   Batch Loss = 11.505950, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.489102363586426, Accuracy = 0.8550093173980713
Training iter #552000:   Batch Loss = 11.908115, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 13.499614715576172, Accuracy = 0.8599875569343567
Training iter #554000:   Batch Loss = 11.943190, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.958688735961914, Accuracy = 0.8556315898895264
Training iter #556000:   Batch Loss = 11.991181, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.12622356414795, Accuracy = 0.8450528979301453
Training iter #558000:   Batch Loss = 11.964867, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.846564292907715, Accuracy = 0.8438083529472351
Training iter #560000:   Batch Loss = 11.116254, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.532732963562012, Accuracy = 0.8450528979301453
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1400
Training iter #562000:   Batch Loss = 11.633800, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.52299690246582, Accuracy = 0.8537647724151611
Training iter #564000:   Batch Loss = 11.284287, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.28587532043457, Accuracy = 0.8543870449066162
Training iter #566000:   Batch Loss = 10.763037, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.363330841064453, Accuracy = 0.8543870449066162
Training iter #568000:   Batch Loss = 11.432035, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.227455139160156, Accuracy = 0.8587430119514465
Training iter #570000:   Batch Loss = 10.605633, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.004355430603027, Accuracy = 0.8630989193916321
Training iter #572000:   Batch Loss = 10.739574, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.040889739990234, Accuracy = 0.8494088649749756
Training iter #574000:   Batch Loss = 10.774374, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.798356056213379, Accuracy = 0.8649657964706421
Training iter #576000:   Batch Loss = 10.429492, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.65176773071289, Accuracy = 0.8637211918830872
Training iter #578000:   Batch Loss = 9.678078, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.53553581237793, Accuracy = 0.8606098294258118
Training iter #580000:   Batch Loss = 10.503023, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.959367752075195, Accuracy = 0.8655880689620972
Training iter #582000:   Batch Loss = 10.406905, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.806873321533203, Accuracy = 0.8487865328788757
Training iter #584000:   Batch Loss = 9.687984, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.617523193359375, Accuracy = 0.852520227432251
Training iter #586000:   Batch Loss = 10.542430, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.713201522827148, Accuracy = 0.8581207394599915
Training iter #588000:   Batch Loss = 10.445728, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.60125732421875, Accuracy = 0.8581207394599915
Training iter #590000:   Batch Loss = 10.280079, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.467151641845703, Accuracy = 0.8612321019172668
Training iter #592000:   Batch Loss = 10.671519, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.646050453186035, Accuracy = 0.8568761944770813
Training iter #594000:   Batch Loss = 10.126578, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.58530330657959, Accuracy = 0.8562538623809814
Training iter #596000:   Batch Loss = 9.731164, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.517946243286133, Accuracy = 0.8612321019172668
Training iter #598000:   Batch Loss = 10.037989, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.399892807006836, Accuracy = 0.8550093173980713
Training iter #600000:   Batch Loss = 10.041270, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.670254707336426, Accuracy = 0.8556315898895264
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1500
Training iter #602000:   Batch Loss = 9.437232, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.304115295410156, Accuracy = 0.8655880689620972
Training iter #604000:   Batch Loss = 10.450974, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.344980239868164, Accuracy = 0.864343523979187
Training iter #606000:   Batch Loss = 10.101860, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.428821563720703, Accuracy = 0.8587430119514465
Training iter #608000:   Batch Loss = 9.743951, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.349441528320312, Accuracy = 0.8612321019172668
Training iter #610000:   Batch Loss = 10.566306, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.297019004821777, Accuracy = 0.862476646900177
Training iter #612000:   Batch Loss = 10.298983, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.28622055053711, Accuracy = 0.8718108534812927
Training iter #614000:   Batch Loss = 10.070218, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.526508331298828, Accuracy = 0.8630989193916321
Training iter #616000:   Batch Loss = 10.095124, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.465620994567871, Accuracy = 0.8668326139450073
Training iter #618000:   Batch Loss = 10.104958, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.365118980407715, Accuracy = 0.8674548864364624
Training iter #620000:   Batch Loss = 10.104292, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.43755054473877, Accuracy = 0.8618543744087219
Training iter #622000:   Batch Loss = 10.070446, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.331445693969727, Accuracy = 0.8686994314193726
Training iter #624000:   Batch Loss = 9.660096, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.272208213806152, Accuracy = 0.8711885213851929
Training iter #626000:   Batch Loss = 10.048672, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.2684907913208, Accuracy = 0.874299943447113
Training iter #628000:   Batch Loss = 10.030866, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.567392349243164, Accuracy = 0.8705662488937378
Training iter #630000:   Batch Loss = 10.355308, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.662643432617188, Accuracy = 0.8718108534812927
Training iter #632000:   Batch Loss = 10.159858, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.503777503967285, Accuracy = 0.8711885213851929
Training iter #634000:   Batch Loss = 10.156935, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.47115707397461, Accuracy = 0.8767890334129333
Training iter #636000:   Batch Loss = 10.304423, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.376687049865723, Accuracy = 0.8680771589279175
Training iter #638000:   Batch Loss = 9.778055, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.187240600585938, Accuracy = 0.8755444884300232
Training iter #640000:   Batch Loss = 9.728012, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.130794525146484, Accuracy = 0.864343523979187
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1600
Training iter #642000:   Batch Loss = 9.448223, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.946080207824707, Accuracy = 0.8693217039108276
Training iter #644000:   Batch Loss = 9.742337, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.074677467346191, Accuracy = 0.8730553984642029
Training iter #646000:   Batch Loss = 10.944538, Accuracy = 0.8695651888847351
PERFORMANCE ON TEST SET: Batch Loss = 11.10191535949707, Accuracy = 0.8599875569343567
Training iter #648000:   Batch Loss = 9.420907, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.093154907226562, Accuracy = 0.8693217039108276
Training iter #650000:   Batch Loss = 9.727815, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.003328323364258, Accuracy = 0.8730553984642029
Training iter #652000:   Batch Loss = 9.506099, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.066576957702637, Accuracy = 0.8680771589279175
Training iter #654000:   Batch Loss = 9.337160, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.986001014709473, Accuracy = 0.8761667609214783
Training iter #656000:   Batch Loss = 9.427893, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.923739433288574, Accuracy = 0.8786558508872986
Training iter #658000:   Batch Loss = 9.634547, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.840581893920898, Accuracy = 0.8799004554748535
Training iter #660000:   Batch Loss = 9.537488, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.99223804473877, Accuracy = 0.8730553984642029
Training iter #662000:   Batch Loss = 9.138112, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.854269981384277, Accuracy = 0.8817672729492188
Training iter #664000:   Batch Loss = 9.341675, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.73621940612793, Accuracy = 0.8774113059043884
Training iter #666000:   Batch Loss = 9.560687, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.008008003234863, Accuracy = 0.8767890334129333
Training iter #668000:   Batch Loss = 9.912435, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.309863090515137, Accuracy = 0.8655880689620972
Training iter #670000:   Batch Loss = 9.012106, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.97252082824707, Accuracy = 0.883634090423584
Training iter #672000:   Batch Loss = 9.578376, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.19204044342041, Accuracy = 0.8774113059043884
Training iter #674000:   Batch Loss = 9.475080, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.445141792297363, Accuracy = 0.8711885213851929
Training iter #676000:   Batch Loss = 9.878355, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.064886093139648, Accuracy = 0.8686994314193726
Training iter #678000:   Batch Loss = 9.743290, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.907098770141602, Accuracy = 0.8786558508872986
Training iter #680000:   Batch Loss = 10.448219, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.354206085205078, Accuracy = 0.874299943447113
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1700
Training iter #682000:   Batch Loss = 9.914111, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.271566390991211, Accuracy = 0.8774113059043884
Training iter #684000:   Batch Loss = 9.958600, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.112044334411621, Accuracy = 0.8693217039108276
Training iter #686000:   Batch Loss = 9.312006, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.986174583435059, Accuracy = 0.8724331259727478
Training iter #688000:   Batch Loss = 9.435820, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.178865432739258, Accuracy = 0.8755444884300232
Training iter #690000:   Batch Loss = 9.646402, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.015970230102539, Accuracy = 0.8718108534812927
Training iter #692000:   Batch Loss = 9.939505, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.977798461914062, Accuracy = 0.8693217039108276
Training iter #694000:   Batch Loss = 9.723543, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.126187324523926, Accuracy = 0.8699439764022827
Training iter #696000:   Batch Loss = 9.579903, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.043426513671875, Accuracy = 0.8680771589279175
Training iter #698000:   Batch Loss = 9.506078, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.80514144897461, Accuracy = 0.8811450004577637
Training iter #700000:   Batch Loss = 9.758451, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.820958137512207, Accuracy = 0.8674548864364624
Training iter #702000:   Batch Loss = 9.966404, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.935391426086426, Accuracy = 0.8761667609214783
Training iter #704000:   Batch Loss = 9.957407, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.588592529296875, Accuracy = 0.8662103414535522
Training iter #706000:   Batch Loss = 9.625857, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.23013973236084, Accuracy = 0.874299943447113
Training iter #708000:   Batch Loss = 9.591959, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.057291984558105, Accuracy = 0.873677670955658
Training iter #710000:   Batch Loss = 9.461576, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.795119285583496, Accuracy = 0.8805227279663086
Training iter #712000:   Batch Loss = 9.462048, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.91683578491211, Accuracy = 0.8792781829833984
Training iter #714000:   Batch Loss = 9.926470, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 10.729768753051758, Accuracy = 0.8680771589279175
Training iter #716000:   Batch Loss = 9.955155, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.112422943115234, Accuracy = 0.8693217039108276
Training iter #718000:   Batch Loss = 9.138397, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.771679878234863, Accuracy = 0.8674548864364624
Training iter #720000:   Batch Loss = 9.196034, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.64975357055664, Accuracy = 0.874299943447113
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1800
Training iter #722000:   Batch Loss = 9.410345, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.027095794677734, Accuracy = 0.8718108534812927
Training iter #724000:   Batch Loss = 9.406092, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.671525001525879, Accuracy = 0.874299943447113
Training iter #726000:   Batch Loss = 9.092161, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.598576545715332, Accuracy = 0.8749222159385681
Training iter #728000:   Batch Loss = 9.179106, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.566987991333008, Accuracy = 0.8730553984642029
Training iter #730000:   Batch Loss = 9.009711, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.686479568481445, Accuracy = 0.8855009078979492
Training iter #732000:   Batch Loss = 9.032154, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.485442161560059, Accuracy = 0.8693217039108276
Training iter #734000:   Batch Loss = 8.428432, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.384686470031738, Accuracy = 0.8805227279663086
Training iter #736000:   Batch Loss = 8.964619, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.678670883178711, Accuracy = 0.8686994314193726
Training iter #738000:   Batch Loss = 9.035210, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.697309494018555, Accuracy = 0.8686994314193726
Training iter #740000:   Batch Loss = 8.940088, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.844801902770996, Accuracy = 0.8686994314193726
Training iter #742000:   Batch Loss = 8.942047, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.64557933807373, Accuracy = 0.8755444884300232
Training iter #744000:   Batch Loss = 8.833719, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.568557739257812, Accuracy = 0.8767890334129333
Training iter #746000:   Batch Loss = 9.007731, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.532090187072754, Accuracy = 0.8767890334129333
Training iter #748000:   Batch Loss = 8.550018, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 10.458667755126953, Accuracy = 0.8805227279663086
Training iter #750000:   Batch Loss = 8.787424, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.474512100219727, Accuracy = 0.8755444884300232
Training iter #752000:   Batch Loss = 8.733616, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.348161697387695, Accuracy = 0.8935905694961548
Training iter #754000:   Batch Loss = 9.535128, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.986241340637207, Accuracy = 0.8749222159385681
Training iter #756000:   Batch Loss = 10.912086, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.794240951538086, Accuracy = 0.8574984669685364
Training iter #758000:   Batch Loss = 10.473671, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.423957824707031, Accuracy = 0.874299943447113
Training iter #760000:   Batch Loss = 9.843163, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.144551277160645, Accuracy = 0.8774113059043884
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-1900
Training iter #762000:   Batch Loss = 9.734097, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.154897689819336, Accuracy = 0.8761667609214783
Training iter #764000:   Batch Loss = 9.556793, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.892096519470215, Accuracy = 0.8730553984642029
Training iter #766000:   Batch Loss = 9.761211, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.8797025680542, Accuracy = 0.8780335783958435
Training iter #768000:   Batch Loss = 9.288595, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.016063690185547, Accuracy = 0.8805227279663086
Training iter #770000:   Batch Loss = 9.614545, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.532111167907715, Accuracy = 0.8618543744087219
Training iter #772000:   Batch Loss = 12.184667, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.959688186645508, Accuracy = 0.8574984669685364
Training iter #774000:   Batch Loss = 10.764843, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.037712097167969, Accuracy = 0.8612321019172668
Training iter #776000:   Batch Loss = 10.234451, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.086374282836914, Accuracy = 0.8749222159385681
Training iter #778000:   Batch Loss = 10.629823, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.812192916870117, Accuracy = 0.873677670955658
Training iter #780000:   Batch Loss = 10.126457, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.546981811523438, Accuracy = 0.8767890334129333
Training iter #782000:   Batch Loss = 9.587513, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.509591102600098, Accuracy = 0.8711885213851929
Training iter #784000:   Batch Loss = 10.042337, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.590466499328613, Accuracy = 0.8786558508872986
Training iter #786000:   Batch Loss = 10.094175, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.393804550170898, Accuracy = 0.8892346024513245
Training iter #788000:   Batch Loss = 9.486836, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.297682762145996, Accuracy = 0.8817672729492188
Training iter #790000:   Batch Loss = 9.676162, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.27085018157959, Accuracy = 0.8823895454406738
Training iter #792000:   Batch Loss = 9.818444, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.17841911315918, Accuracy = 0.8879900574684143
Training iter #794000:   Batch Loss = 9.898602, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.151651382446289, Accuracy = 0.8842563629150391
Training iter #796000:   Batch Loss = 8.923574, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.161046028137207, Accuracy = 0.89545738697052
Training iter #798000:   Batch Loss = 10.255898, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.622946739196777, Accuracy = 0.883634090423584
Training iter #800000:   Batch Loss = 10.161157, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.559592247009277, Accuracy = 0.8786558508872986
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2000
Training iter #802000:   Batch Loss = 9.970812, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.362234115600586, Accuracy = 0.8786558508872986
Training iter #804000:   Batch Loss = 9.527020, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.264776229858398, Accuracy = 0.874299943447113
Training iter #806000:   Batch Loss = 9.587316, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.141469955444336, Accuracy = 0.8774113059043884
Training iter #808000:   Batch Loss = 9.653876, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.029327392578125, Accuracy = 0.8792781829833984
Training iter #810000:   Batch Loss = 9.676369, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.808961868286133, Accuracy = 0.8724331259727478
Training iter #812000:   Batch Loss = 9.899090, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.612643241882324, Accuracy = 0.8711885213851929
Training iter #814000:   Batch Loss = 10.409843, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.306132316589355, Accuracy = 0.8780335783958435
Training iter #816000:   Batch Loss = 10.256941, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.290626525878906, Accuracy = 0.8761667609214783
Training iter #818000:   Batch Loss = 9.695189, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.32891845703125, Accuracy = 0.8761667609214783
Training iter #820000:   Batch Loss = 9.668095, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.149621963500977, Accuracy = 0.8898568749427795
Training iter #822000:   Batch Loss = 9.449262, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.192011833190918, Accuracy = 0.8867455124855042
Training iter #824000:   Batch Loss = 9.621936, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.246665000915527, Accuracy = 0.8799004554748535
Training iter #826000:   Batch Loss = 8.603322, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.046548843383789, Accuracy = 0.8830118179321289
Training iter #828000:   Batch Loss = 9.402317, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.002574920654297, Accuracy = 0.8873677849769592
Training iter #830000:   Batch Loss = 10.026481, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.764932632446289, Accuracy = 0.864343523979187
Training iter #832000:   Batch Loss = 9.990925, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.369682312011719, Accuracy = 0.8917236924171448
Training iter #834000:   Batch Loss = 9.285894, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.334856033325195, Accuracy = 0.883634090423584
Training iter #836000:   Batch Loss = 10.020563, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.2622652053833, Accuracy = 0.8873677849769592
Training iter #838000:   Batch Loss = 9.773426, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.239328384399414, Accuracy = 0.8855009078979492
Training iter #840000:   Batch Loss = 9.516686, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.152996063232422, Accuracy = 0.89545738697052
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2100
Training iter #842000:   Batch Loss = 9.553862, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.009557723999023, Accuracy = 0.8848786354064941
Training iter #844000:   Batch Loss = 9.842775, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.357230186462402, Accuracy = 0.8767890334129333
Training iter #846000:   Batch Loss = 9.467827, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.09644889831543, Accuracy = 0.874299943447113
Training iter #848000:   Batch Loss = 9.462317, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.393861770629883, Accuracy = 0.8780335783958435
Training iter #850000:   Batch Loss = 10.301552, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.310837745666504, Accuracy = 0.874299943447113
Training iter #852000:   Batch Loss = 9.785379, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.149271011352539, Accuracy = 0.8855009078979492
Training iter #854000:   Batch Loss = 9.207562, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.090923309326172, Accuracy = 0.8830118179321289
Training iter #856000:   Batch Loss = 9.429471, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.785208702087402, Accuracy = 0.8892346024513245
Training iter #858000:   Batch Loss = 10.036799, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.668889999389648, Accuracy = 0.8705662488937378
Training iter #860000:   Batch Loss = 9.868656, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.465949058532715, Accuracy = 0.8786558508872986
Training iter #862000:   Batch Loss = 10.161366, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.773192405700684, Accuracy = 0.8680771589279175
Training iter #864000:   Batch Loss = 9.552024, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.670083045959473, Accuracy = 0.8705662488937378
Training iter #866000:   Batch Loss = 10.889887, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.492355346679688, Accuracy = 0.8686994314193726
Training iter #868000:   Batch Loss = 10.515795, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.125886917114258, Accuracy = 0.8680771589279175
Training iter #870000:   Batch Loss = 10.314713, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.828926086425781, Accuracy = 0.8792781829833984
Training iter #872000:   Batch Loss = 10.085007, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.615042686462402, Accuracy = 0.8761667609214783
Training iter #874000:   Batch Loss = 10.312468, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.616646766662598, Accuracy = 0.8674548864364624
Training iter #876000:   Batch Loss = 9.755188, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.38115406036377, Accuracy = 0.8811450004577637
Training iter #878000:   Batch Loss = 15.978741, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 16.943836212158203, Accuracy = 0.8587430119514465
Training iter #880000:   Batch Loss = 14.314562, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 15.664765357971191, Accuracy = 0.8562538623809814
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2200
Training iter #882000:   Batch Loss = 14.098004, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 14.949708938598633, Accuracy = 0.8550093173980713
Training iter #884000:   Batch Loss = 13.498416, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 14.271780967712402, Accuracy = 0.8581207394599915
Training iter #886000:   Batch Loss = 12.554230, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.00394344329834, Accuracy = 0.8506534099578857
Training iter #888000:   Batch Loss = 13.101549, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.255882263183594, Accuracy = 0.8674548864364624
Training iter #890000:   Batch Loss = 12.431181, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 13.515759468078613, Accuracy = 0.8693217039108276
Training iter #892000:   Batch Loss = 11.945248, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.6013822555542, Accuracy = 0.8630989193916321
Training iter #894000:   Batch Loss = 12.138844, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.429269790649414, Accuracy = 0.8612321019172668
Training iter #896000:   Batch Loss = 11.973779, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.097236633300781, Accuracy = 0.8543870449066162
Training iter #898000:   Batch Loss = 11.918026, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.108100891113281, Accuracy = 0.8662103414535522
Training iter #900000:   Batch Loss = 11.290950, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.77685546875, Accuracy = 0.8587430119514465
Training iter #902000:   Batch Loss = 11.478096, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.747096061706543, Accuracy = 0.8749222159385681
Training iter #904000:   Batch Loss = 11.644011, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.689800262451172, Accuracy = 0.883634090423584
Training iter #906000:   Batch Loss = 11.333799, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.886482238769531, Accuracy = 0.8823895454406738
Training iter #908000:   Batch Loss = 11.081840, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.546441078186035, Accuracy = 0.8774113059043884
Training iter #910000:   Batch Loss = 11.343352, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.547783851623535, Accuracy = 0.8718108534812927
Training iter #912000:   Batch Loss = 11.248959, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.425070762634277, Accuracy = 0.8873677849769592
Training iter #914000:   Batch Loss = 10.960069, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.588057518005371, Accuracy = 0.8805227279663086
Training iter #916000:   Batch Loss = 10.926868, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.645804405212402, Accuracy = 0.8711885213851929
Training iter #918000:   Batch Loss = 10.679144, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 12.515427589416504, Accuracy = 0.8606098294258118
Training iter #920000:   Batch Loss = 11.048676, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.61870002746582, Accuracy = 0.8693217039108276
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2300
Training iter #922000:   Batch Loss = 10.945808, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.319317817687988, Accuracy = 0.864343523979187
Training iter #924000:   Batch Loss = 11.253472, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.073928833007812, Accuracy = 0.8730553984642029
Training iter #926000:   Batch Loss = 10.771940, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.138540267944336, Accuracy = 0.874299943447113
Training iter #928000:   Batch Loss = 10.737940, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.051321983337402, Accuracy = 0.8680771589279175
Training iter #930000:   Batch Loss = 10.666293, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.007247924804688, Accuracy = 0.8799004554748535
Training iter #932000:   Batch Loss = 10.581923, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.208207130432129, Accuracy = 0.8774113059043884
Training iter #934000:   Batch Loss = 11.039845, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.275276184082031, Accuracy = 0.8799004554748535
Training iter #936000:   Batch Loss = 10.994875, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.719475746154785, Accuracy = 0.8662103414535522
Training iter #938000:   Batch Loss = 11.225897, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.68264389038086, Accuracy = 0.8724331259727478
Training iter #940000:   Batch Loss = 11.596968, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.759542465209961, Accuracy = 0.8655880689620972
Training iter #942000:   Batch Loss = 11.527439, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.63748550415039, Accuracy = 0.8792781829833984
Training iter #944000:   Batch Loss = 11.250823, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.585002899169922, Accuracy = 0.8711885213851929
Training iter #946000:   Batch Loss = 10.968306, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.685432434082031, Accuracy = 0.8630989193916321
Training iter #948000:   Batch Loss = 10.758906, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.385980606079102, Accuracy = 0.8761667609214783
Training iter #950000:   Batch Loss = 10.971887, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.153865814208984, Accuracy = 0.8767890334129333
Training iter #952000:   Batch Loss = 11.263973, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 12.00040054321289, Accuracy = 0.8767890334129333
Training iter #954000:   Batch Loss = 10.472557, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.997310638427734, Accuracy = 0.8686994314193726
Training iter #956000:   Batch Loss = 10.187005, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.89273738861084, Accuracy = 0.8811450004577637
Training iter #958000:   Batch Loss = 10.347083, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.847146034240723, Accuracy = 0.8724331259727478
Training iter #960000:   Batch Loss = 10.506510, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.867203712463379, Accuracy = 0.8674548864364624
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2400
Training iter #962000:   Batch Loss = 9.791403, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.73770809173584, Accuracy = 0.8749222159385681
Training iter #964000:   Batch Loss = 10.230250, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.573179244995117, Accuracy = 0.8817672729492188
Training iter #966000:   Batch Loss = 10.347996, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.565924644470215, Accuracy = 0.8786558508872986
Training iter #968000:   Batch Loss = 9.827273, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.408714294433594, Accuracy = 0.8861232399940491
Training iter #970000:   Batch Loss = 9.442425, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.244277954101562, Accuracy = 0.8761667609214783
Training iter #972000:   Batch Loss = 9.748216, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.147825241088867, Accuracy = 0.8811450004577637
Training iter #974000:   Batch Loss = 9.409319, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.21222972869873, Accuracy = 0.883634090423584
Training iter #976000:   Batch Loss = 9.582723, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.197796821594238, Accuracy = 0.8805227279663086
Training iter #978000:   Batch Loss = 9.364416, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.024517059326172, Accuracy = 0.8780335783958435
Training iter #980000:   Batch Loss = 10.102123, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.991750717163086, Accuracy = 0.8780335783958435
Training iter #982000:   Batch Loss = 9.957531, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.606827735900879, Accuracy = 0.8761667609214783
Training iter #984000:   Batch Loss = 9.644157, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.260515213012695, Accuracy = 0.883634090423584
Training iter #986000:   Batch Loss = 8.971929, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.329242706298828, Accuracy = 0.8898568749427795
Training iter #988000:   Batch Loss = 9.190285, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.188310623168945, Accuracy = 0.8873677849769592
Training iter #990000:   Batch Loss = 10.464089, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.645011901855469, Accuracy = 0.8630989193916321
Training iter #992000:   Batch Loss = 9.781556, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.393301010131836, Accuracy = 0.8823895454406738
Training iter #994000:   Batch Loss = 9.553518, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.353031158447266, Accuracy = 0.873677670955658
Training iter #996000:   Batch Loss = 9.719999, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.267950057983398, Accuracy = 0.8792781829833984
Training iter #998000:   Batch Loss = 9.550941, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.191784858703613, Accuracy = 0.873677670955658
Training iter #1000000:   Batch Loss = 9.279461, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.371813774108887, Accuracy = 0.8842563629150391
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2500
Training iter #1002000:   Batch Loss = 9.543697, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.156539916992188, Accuracy = 0.8718108534812927
Training iter #1004000:   Batch Loss = 9.406240, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.042684555053711, Accuracy = 0.8855009078979492
Training iter #1006000:   Batch Loss = 9.807758, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.104909896850586, Accuracy = 0.89545738697052
Training iter #1008000:   Batch Loss = 9.700384, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.06443977355957, Accuracy = 0.8817672729492188
Training iter #1010000:   Batch Loss = 9.536385, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.01534366607666, Accuracy = 0.8842563629150391
Training iter #1012000:   Batch Loss = 9.108747, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.13599681854248, Accuracy = 0.8786558508872986
Training iter #1014000:   Batch Loss = 10.193279, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.684364318847656, Accuracy = 0.8668326139450073
Training iter #1016000:   Batch Loss = 9.837828, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.22988510131836, Accuracy = 0.8767890334129333
Training iter #1018000:   Batch Loss = 9.679170, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.227272033691406, Accuracy = 0.8761667609214783
Training iter #1020000:   Batch Loss = 9.195909, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.554028511047363, Accuracy = 0.8786558508872986
Training iter #1022000:   Batch Loss = 10.575472, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.804760932922363, Accuracy = 0.8811450004577637
Training iter #1024000:   Batch Loss = 11.122477, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.817436218261719, Accuracy = 0.8811450004577637
Training iter #1026000:   Batch Loss = 10.219435, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.631966590881348, Accuracy = 0.8817672729492188
Training iter #1028000:   Batch Loss = 10.279495, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.421059608459473, Accuracy = 0.8730553984642029
Training iter #1030000:   Batch Loss = 10.157622, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.418228149414062, Accuracy = 0.874299943447113
Training iter #1032000:   Batch Loss = 9.888448, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.178333282470703, Accuracy = 0.8873677849769592
Training iter #1034000:   Batch Loss = 10.351090, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.226751327514648, Accuracy = 0.8892346024513245
Training iter #1036000:   Batch Loss = 9.104571, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.022422790527344, Accuracy = 0.8848786354064941
Training iter #1038000:   Batch Loss = 9.378769, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.92099380493164, Accuracy = 0.8774113059043884
Training iter #1040000:   Batch Loss = 9.694935, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.758790016174316, Accuracy = 0.8892346024513245
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2600
Training iter #1042000:   Batch Loss = 9.619923, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.07290267944336, Accuracy = 0.883634090423584
Training iter #1044000:   Batch Loss = 9.501693, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.889389038085938, Accuracy = 0.8892346024513245
Training iter #1046000:   Batch Loss = 9.943680, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.00406265258789, Accuracy = 0.8823895454406738
Training iter #1048000:   Batch Loss = 13.107298, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 14.083337783813477, Accuracy = 0.8630989193916321
Training iter #1050000:   Batch Loss = 11.365807, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.145231246948242, Accuracy = 0.8655880689620972
Training iter #1052000:   Batch Loss = 11.482239, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.922541618347168, Accuracy = 0.8630989193916321
Training iter #1054000:   Batch Loss = 10.837776, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 13.038833618164062, Accuracy = 0.8755444884300232
Training iter #1056000:   Batch Loss = 11.036866, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.676546096801758, Accuracy = 0.8649657964706421
Training iter #1058000:   Batch Loss = 11.013371, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.528212547302246, Accuracy = 0.8774113059043884
Training iter #1060000:   Batch Loss = 10.595600, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.366153717041016, Accuracy = 0.8786558508872986
Training iter #1062000:   Batch Loss = 10.860494, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.361183166503906, Accuracy = 0.8662103414535522
Training iter #1064000:   Batch Loss = 10.773315, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.296575546264648, Accuracy = 0.8792781829833984
Training iter #1066000:   Batch Loss = 10.595220, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.25429630279541, Accuracy = 0.8649657964706421
Training iter #1068000:   Batch Loss = 10.576992, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.250848770141602, Accuracy = 0.8705662488937378
Training iter #1070000:   Batch Loss = 10.524527, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.126707077026367, Accuracy = 0.8693217039108276
Training iter #1072000:   Batch Loss = 10.277457, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.915105819702148, Accuracy = 0.8637211918830872
Training iter #1074000:   Batch Loss = 10.822168, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.88465404510498, Accuracy = 0.8823895454406738
Training iter #1076000:   Batch Loss = 10.006708, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.858196258544922, Accuracy = 0.873677670955658
Training iter #1078000:   Batch Loss = 10.812220, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.6951322555542, Accuracy = 0.8699439764022827
Training iter #1080000:   Batch Loss = 9.970580, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.684088706970215, Accuracy = 0.8680771589279175
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2700
Training iter #1082000:   Batch Loss = 10.653156, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.633423805236816, Accuracy = 0.8761667609214783
Training iter #1084000:   Batch Loss = 10.332495, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.741684913635254, Accuracy = 0.8755444884300232
Training iter #1086000:   Batch Loss = 10.090817, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.650951385498047, Accuracy = 0.873677670955658
Training iter #1088000:   Batch Loss = 10.113374, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.682872772216797, Accuracy = 0.8718108534812927
Training iter #1090000:   Batch Loss = 9.613496, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.635396957397461, Accuracy = 0.8792781829833984
Training iter #1092000:   Batch Loss = 9.973777, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.665884017944336, Accuracy = 0.8805227279663086
Training iter #1094000:   Batch Loss = 10.743337, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.442830085754395, Accuracy = 0.8718108534812927
Training iter #1096000:   Batch Loss = 11.395535, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.626731872558594, Accuracy = 0.8749222159385681
Training iter #1098000:   Batch Loss = 11.081207, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.42970085144043, Accuracy = 0.8662103414535522
Training iter #1100000:   Batch Loss = 10.799658, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.746745109558105, Accuracy = 0.8618543744087219
Training iter #1102000:   Batch Loss = 11.315365, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.71650218963623, Accuracy = 0.8599875569343567
Training iter #1104000:   Batch Loss = 11.588152, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.57902717590332, Accuracy = 0.8718108534812927
Training iter #1106000:   Batch Loss = 10.909863, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.447669982910156, Accuracy = 0.8655880689620972
Training iter #1108000:   Batch Loss = 10.798398, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.524036407470703, Accuracy = 0.8662103414535522
Training iter #1110000:   Batch Loss = 11.059330, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.817886352539062, Accuracy = 0.853142499923706
Training iter #1112000:   Batch Loss = 10.992449, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.394145965576172, Accuracy = 0.8612321019172668
Training iter #1114000:   Batch Loss = 10.689760, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.422911643981934, Accuracy = 0.8761667609214783
Training iter #1116000:   Batch Loss = 10.953837, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.484086990356445, Accuracy = 0.8618543744087219
Training iter #1118000:   Batch Loss = 10.663817, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.16899299621582, Accuracy = 0.8581207394599915
Training iter #1120000:   Batch Loss = 10.456660, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.120450019836426, Accuracy = 0.8730553984642029
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2800
Training iter #1122000:   Batch Loss = 11.015121, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.965126037597656, Accuracy = 0.8680771589279175
Training iter #1124000:   Batch Loss = 10.377610, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.052088737487793, Accuracy = 0.8674548864364624
Training iter #1126000:   Batch Loss = 10.142923, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.916801452636719, Accuracy = 0.864343523979187
Training iter #1128000:   Batch Loss = 9.975105, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.670150756835938, Accuracy = 0.8786558508872986
Training iter #1130000:   Batch Loss = 10.009619, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.713968276977539, Accuracy = 0.8630989193916321
Training iter #1132000:   Batch Loss = 9.851457, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.011143684387207, Accuracy = 0.8699439764022827
Training iter #1134000:   Batch Loss = 10.364861, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.32967472076416, Accuracy = 0.8711885213851929
Training iter #1136000:   Batch Loss = 10.075838, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.06644344329834, Accuracy = 0.8649657964706421
Training iter #1138000:   Batch Loss = 9.799395, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.992021560668945, Accuracy = 0.8668326139450073
Training iter #1140000:   Batch Loss = 10.204844, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.722884178161621, Accuracy = 0.8711885213851929
Training iter #1142000:   Batch Loss = 10.323404, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.858155250549316, Accuracy = 0.8668326139450073
Training iter #1144000:   Batch Loss = 9.757650, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.878005981445312, Accuracy = 0.8817672729492188
Training iter #1146000:   Batch Loss = 10.029576, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.933815002441406, Accuracy = 0.8711885213851929
Training iter #1148000:   Batch Loss = 10.867652, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.99828052520752, Accuracy = 0.8699439764022827
Training iter #1150000:   Batch Loss = 10.441513, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.911616325378418, Accuracy = 0.8730553984642029
Training iter #1152000:   Batch Loss = 11.796140, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.292387008666992, Accuracy = 0.8693217039108276
Training iter #1154000:   Batch Loss = 11.194283, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.974649429321289, Accuracy = 0.8799004554748535
Training iter #1156000:   Batch Loss = 10.609716, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 12.626989364624023, Accuracy = 0.8637211918830872
Training iter #1158000:   Batch Loss = 13.540168, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 14.647170066833496, Accuracy = 0.864343523979187
Training iter #1160000:   Batch Loss = 12.115652, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 13.548660278320312, Accuracy = 0.8749222159385681
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-2900
Training iter #1162000:   Batch Loss = 11.988843, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.635412216186523, Accuracy = 0.8680771589279175
Training iter #1164000:   Batch Loss = 11.975214, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.457735061645508, Accuracy = 0.862476646900177
Training iter #1166000:   Batch Loss = 11.403869, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.309440612792969, Accuracy = 0.8599875569343567
Training iter #1168000:   Batch Loss = 11.676775, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.113264083862305, Accuracy = 0.8680771589279175
Training iter #1170000:   Batch Loss = 11.567080, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.142830848693848, Accuracy = 0.8680771589279175
Training iter #1172000:   Batch Loss = 12.214739, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 14.291101455688477, Accuracy = 0.8587430119514465
Training iter #1174000:   Batch Loss = 12.111526, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.827690124511719, Accuracy = 0.8724331259727478
Training iter #1176000:   Batch Loss = 11.856240, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.646921157836914, Accuracy = 0.8718108534812927
Training iter #1178000:   Batch Loss = 11.927287, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.436541557312012, Accuracy = 0.8755444884300232
Training iter #1180000:   Batch Loss = 11.603255, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.417435646057129, Accuracy = 0.8799004554748535
Training iter #1182000:   Batch Loss = 11.957482, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.731001853942871, Accuracy = 0.8730553984642029
Training iter #1184000:   Batch Loss = 12.228685, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.711410522460938, Accuracy = 0.8730553984642029
Training iter #1186000:   Batch Loss = 12.021913, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.471956253051758, Accuracy = 0.8755444884300232
Training iter #1188000:   Batch Loss = 11.918884, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.387085914611816, Accuracy = 0.8680771589279175
Training iter #1190000:   Batch Loss = 11.298046, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 13.31182861328125, Accuracy = 0.8686994314193726
Training iter #1192000:   Batch Loss = 11.458417, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 13.238155364990234, Accuracy = 0.8724331259727478
Training iter #1194000:   Batch Loss = 11.119696, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.946700096130371, Accuracy = 0.8767890334129333
Training iter #1196000:   Batch Loss = 11.095358, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.913511276245117, Accuracy = 0.873677670955658
Training iter #1198000:   Batch Loss = 10.839670, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.008756637573242, Accuracy = 0.8786558508872986
Training iter #1200000:   Batch Loss = 11.069330, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.976602554321289, Accuracy = 0.8724331259727478
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3000
Training iter #1202000:   Batch Loss = 11.985332, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.657989501953125, Accuracy = 0.8693217039108276
Training iter #1204000:   Batch Loss = 11.587926, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.432076454162598, Accuracy = 0.8730553984642029
Training iter #1206000:   Batch Loss = 11.255651, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.261693000793457, Accuracy = 0.8786558508872986
Training iter #1208000:   Batch Loss = 11.184410, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.147642135620117, Accuracy = 0.8686994314193726
Training iter #1210000:   Batch Loss = 11.360176, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.998188972473145, Accuracy = 0.8780335783958435
Training iter #1212000:   Batch Loss = 11.205657, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.97030258178711, Accuracy = 0.8705662488937378
Training iter #1214000:   Batch Loss = 11.545809, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.999706268310547, Accuracy = 0.8711885213851929
Training iter #1216000:   Batch Loss = 12.316651, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 14.035848617553711, Accuracy = 0.8730553984642029
Training iter #1218000:   Batch Loss = 12.405829, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 14.420015335083008, Accuracy = 0.8537647724151611
Training iter #1220000:   Batch Loss = 12.593546, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 14.076852798461914, Accuracy = 0.8518979549407959
Training iter #1222000:   Batch Loss = 12.224951, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.761709213256836, Accuracy = 0.8543870449066162
Training iter #1224000:   Batch Loss = 11.402298, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 13.506105422973633, Accuracy = 0.8618543744087219
Training iter #1226000:   Batch Loss = 12.055967, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.291149139404297, Accuracy = 0.8693217039108276
Training iter #1228000:   Batch Loss = 11.685303, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 13.056203842163086, Accuracy = 0.8817672729492188
Training iter #1230000:   Batch Loss = 11.542064, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.886191368103027, Accuracy = 0.8792781829833984
Training iter #1232000:   Batch Loss = 11.359593, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 13.088749885559082, Accuracy = 0.8599875569343567
Training iter #1234000:   Batch Loss = 11.561390, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.820243835449219, Accuracy = 0.8574984669685364
Training iter #1236000:   Batch Loss = 11.219517, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.69495964050293, Accuracy = 0.8662103414535522
Training iter #1238000:   Batch Loss = 10.883028, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.693750381469727, Accuracy = 0.8699439764022827
Training iter #1240000:   Batch Loss = 11.424662, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.937983512878418, Accuracy = 0.8612321019172668
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3100
Training iter #1242000:   Batch Loss = 11.269645, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.597928047180176, Accuracy = 0.8606098294258118
Training iter #1244000:   Batch Loss = 10.875703, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.41088581085205, Accuracy = 0.8574984669685364
Training iter #1246000:   Batch Loss = 10.930971, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.598701477050781, Accuracy = 0.8606098294258118
Training iter #1248000:   Batch Loss = 12.598475, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 14.268291473388672, Accuracy = 0.8556315898895264
Training iter #1250000:   Batch Loss = 12.265068, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.889907836914062, Accuracy = 0.8556315898895264
Training iter #1252000:   Batch Loss = 12.927877, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 14.400094032287598, Accuracy = 0.8556315898895264
Training iter #1254000:   Batch Loss = 12.461198, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 14.15697193145752, Accuracy = 0.8543870449066162
Training iter #1256000:   Batch Loss = 12.725109, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.948272705078125, Accuracy = 0.8556315898895264
Training iter #1258000:   Batch Loss = 12.965364, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 13.903203964233398, Accuracy = 0.8506534099578857
Training iter #1260000:   Batch Loss = 12.143157, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 13.682825088500977, Accuracy = 0.8537647724151611
Training iter #1262000:   Batch Loss = 12.516916, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.620863914489746, Accuracy = 0.8487865328788757
Training iter #1264000:   Batch Loss = 12.175855, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.658472061157227, Accuracy = 0.8655880689620972
Training iter #1266000:   Batch Loss = 11.953245, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.462827682495117, Accuracy = 0.8550093173980713
Training iter #1268000:   Batch Loss = 11.713647, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 13.320672988891602, Accuracy = 0.8562538623809814
Training iter #1270000:   Batch Loss = 11.556166, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.165422439575195, Accuracy = 0.8649657964706421
Training iter #1272000:   Batch Loss = 12.032583, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.304441452026367, Accuracy = 0.8655880689620972
Training iter #1274000:   Batch Loss = 11.521117, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 13.062154769897461, Accuracy = 0.8705662488937378
Training iter #1276000:   Batch Loss = 10.799668, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.98045825958252, Accuracy = 0.8606098294258118
Training iter #1278000:   Batch Loss = 11.868131, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.96346664428711, Accuracy = 0.8612321019172668
Training iter #1280000:   Batch Loss = 12.676387, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 14.22007942199707, Accuracy = 0.8724331259727478
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3200
Training iter #1282000:   Batch Loss = 12.263186, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.931402206420898, Accuracy = 0.8655880689620972
Training iter #1284000:   Batch Loss = 12.882061, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 14.222312927246094, Accuracy = 0.8581207394599915
Training iter #1286000:   Batch Loss = 12.244102, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 13.953662872314453, Accuracy = 0.8481642603874207
Training iter #1288000:   Batch Loss = 12.169135, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.823376655578613, Accuracy = 0.8562538623809814
Training iter #1290000:   Batch Loss = 12.320961, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.885175704956055, Accuracy = 0.852520227432251
Training iter #1292000:   Batch Loss = 13.459728, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 13.664652824401855, Accuracy = 0.8581207394599915
Training iter #1294000:   Batch Loss = 11.572553, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.605428695678711, Accuracy = 0.8581207394599915
Training iter #1296000:   Batch Loss = 11.980534, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 13.52069091796875, Accuracy = 0.8637211918830872
Training iter #1298000:   Batch Loss = 12.035062, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.614921569824219, Accuracy = 0.8581207394599915
Training iter #1300000:   Batch Loss = 11.700852, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 13.332188606262207, Accuracy = 0.8568761944770813
Training iter #1302000:   Batch Loss = 11.984071, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.976632118225098, Accuracy = 0.8618543744087219
Training iter #1304000:   Batch Loss = 11.667580, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.865461349487305, Accuracy = 0.8711885213851929
Training iter #1306000:   Batch Loss = 10.903677, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.065350532531738, Accuracy = 0.862476646900177
Training iter #1308000:   Batch Loss = 11.362138, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.9212646484375, Accuracy = 0.8581207394599915
Training iter #1310000:   Batch Loss = 11.191562, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.806452751159668, Accuracy = 0.8581207394599915
Training iter #1312000:   Batch Loss = 11.504104, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.707368850708008, Accuracy = 0.8568761944770813
Training iter #1314000:   Batch Loss = 12.326341, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.416311264038086, Accuracy = 0.8587430119514465
Training iter #1316000:   Batch Loss = 11.727647, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.15172004699707, Accuracy = 0.8649657964706421
Training iter #1318000:   Batch Loss = 11.231252, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.903976440429688, Accuracy = 0.8693217039108276
Training iter #1320000:   Batch Loss = 11.109396, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.03770637512207, Accuracy = 0.8581207394599915
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3300
Training iter #1322000:   Batch Loss = 10.683610, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.787779808044434, Accuracy = 0.8606098294258118
Training iter #1324000:   Batch Loss = 10.961973, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.675814628601074, Accuracy = 0.8724331259727478
Training iter #1326000:   Batch Loss = 12.496170, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 13.201716423034668, Accuracy = 0.8655880689620972
Training iter #1328000:   Batch Loss = 11.247283, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.966238021850586, Accuracy = 0.8680771589279175
Training iter #1330000:   Batch Loss = 11.016546, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.731470108032227, Accuracy = 0.8668326139450073
Training iter #1332000:   Batch Loss = 10.370518, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.60422420501709, Accuracy = 0.8637211918830872
Training iter #1334000:   Batch Loss = 10.644229, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.731375694274902, Accuracy = 0.8550093173980713
Training iter #1336000:   Batch Loss = 11.179344, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.532878875732422, Accuracy = 0.8686994314193726
Training iter #1338000:   Batch Loss = 11.141110, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.632024765014648, Accuracy = 0.8724331259727478
Training iter #1340000:   Batch Loss = 10.830687, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.813760757446289, Accuracy = 0.8568761944770813
Training iter #1342000:   Batch Loss = 10.868006, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.859886169433594, Accuracy = 0.8724331259727478
Training iter #1344000:   Batch Loss = 11.173131, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.787168502807617, Accuracy = 0.8674548864364624
Training iter #1346000:   Batch Loss = 11.082609, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.651357650756836, Accuracy = 0.8674548864364624
Training iter #1348000:   Batch Loss = 11.256364, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.64560604095459, Accuracy = 0.8724331259727478
Training iter #1350000:   Batch Loss = 10.951118, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.464899063110352, Accuracy = 0.8686994314193726
Training iter #1352000:   Batch Loss = 11.007584, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.534357070922852, Accuracy = 0.8655880689620972
Training iter #1354000:   Batch Loss = 10.546637, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.529231071472168, Accuracy = 0.8711885213851929
Training iter #1356000:   Batch Loss = 10.521808, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.426996231079102, Accuracy = 0.8655880689620972
Training iter #1358000:   Batch Loss = 10.370584, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.184581756591797, Accuracy = 0.8805227279663086
Training iter #1360000:   Batch Loss = 11.791871, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 12.302225112915039, Accuracy = 0.8680771589279175
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3400
Training iter #1362000:   Batch Loss = 10.670579, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.396587371826172, Accuracy = 0.8612321019172668
Training iter #1364000:   Batch Loss = 10.541121, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.243980407714844, Accuracy = 0.8655880689620972
Training iter #1366000:   Batch Loss = 10.646175, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.041194915771484, Accuracy = 0.8799004554748535
Training iter #1368000:   Batch Loss = 10.266949, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.163726806640625, Accuracy = 0.873677670955658
Training iter #1370000:   Batch Loss = 10.739512, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.5679349899292, Accuracy = 0.8774113059043884
Training iter #1372000:   Batch Loss = 10.574318, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.556244850158691, Accuracy = 0.8550093173980713
Training iter #1374000:   Batch Loss = 10.974427, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.672348976135254, Accuracy = 0.8637211918830872
Training iter #1376000:   Batch Loss = 11.026320, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.25352668762207, Accuracy = 0.8599875569343567
Training iter #1378000:   Batch Loss = 10.825820, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.115711212158203, Accuracy = 0.8699439764022827
Training iter #1380000:   Batch Loss = 10.497774, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.162410736083984, Accuracy = 0.8774113059043884
Training iter #1382000:   Batch Loss = 10.260112, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.076973915100098, Accuracy = 0.8755444884300232
Training iter #1384000:   Batch Loss = 10.404093, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.096656799316406, Accuracy = 0.864343523979187
Training iter #1386000:   Batch Loss = 10.179902, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.047527313232422, Accuracy = 0.8705662488937378
Training iter #1388000:   Batch Loss = 10.664505, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.318109512329102, Accuracy = 0.8668326139450073
Training iter #1390000:   Batch Loss = 10.183481, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.131574630737305, Accuracy = 0.8674548864364624
Training iter #1392000:   Batch Loss = 10.084908, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.921143531799316, Accuracy = 0.8711885213851929
Training iter #1394000:   Batch Loss = 10.647945, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 12.143630027770996, Accuracy = 0.8749222159385681
Training iter #1396000:   Batch Loss = 10.266010, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.117717742919922, Accuracy = 0.8730553984642029
Training iter #1398000:   Batch Loss = 10.031989, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.006767272949219, Accuracy = 0.8767890334129333
Training iter #1400000:   Batch Loss = 10.198937, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.07588005065918, Accuracy = 0.8686994314193726
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3500
Training iter #1402000:   Batch Loss = 10.796800, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.57784366607666, Accuracy = 0.8699439764022827
Training iter #1404000:   Batch Loss = 10.233799, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.252620697021484, Accuracy = 0.8780335783958435
Training iter #1406000:   Batch Loss = 10.389744, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.159873962402344, Accuracy = 0.8662103414535522
Training iter #1408000:   Batch Loss = 10.253886, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.296979904174805, Accuracy = 0.8767890334129333
Training iter #1410000:   Batch Loss = 10.312674, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.199930191040039, Accuracy = 0.8699439764022827
Training iter #1412000:   Batch Loss = 10.506487, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.004107475280762, Accuracy = 0.8761667609214783
Training iter #1414000:   Batch Loss = 10.118735, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.981657981872559, Accuracy = 0.8805227279663086
Training iter #1416000:   Batch Loss = 10.775661, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.227766036987305, Accuracy = 0.8655880689620972
Training iter #1418000:   Batch Loss = 10.266954, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.11492919921875, Accuracy = 0.8668326139450073
Training iter #1420000:   Batch Loss = 10.603852, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.077987670898438, Accuracy = 0.8668326139450073
Training iter #1422000:   Batch Loss = 10.172981, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.969268798828125, Accuracy = 0.864343523979187
Training iter #1424000:   Batch Loss = 10.541414, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.938035011291504, Accuracy = 0.8668326139450073
Training iter #1426000:   Batch Loss = 10.247866, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.795056343078613, Accuracy = 0.8749222159385681
Training iter #1428000:   Batch Loss = 10.189618, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 12.161893844604492, Accuracy = 0.8705662488937378
Training iter #1430000:   Batch Loss = 10.510058, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.58898639678955, Accuracy = 0.8649657964706421
Training iter #1432000:   Batch Loss = 12.151213, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.188349723815918, Accuracy = 0.8649657964706421
Training iter #1434000:   Batch Loss = 11.123312, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.78156566619873, Accuracy = 0.8543870449066162
Training iter #1436000:   Batch Loss = 11.090996, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.76003360748291, Accuracy = 0.8506534099578857
Training iter #1438000:   Batch Loss = 10.896278, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.506600379943848, Accuracy = 0.8518979549407959
Training iter #1440000:   Batch Loss = 11.121579, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.423494338989258, Accuracy = 0.8581207394599915
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3600
Training iter #1442000:   Batch Loss = 10.422235, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.314308166503906, Accuracy = 0.8568761944770813
Training iter #1444000:   Batch Loss = 10.808880, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.147627830505371, Accuracy = 0.8630989193916321
Training iter #1446000:   Batch Loss = 10.024715, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.019852638244629, Accuracy = 0.8649657964706421
Training iter #1448000:   Batch Loss = 9.965835, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.891178131103516, Accuracy = 0.8686994314193726
Training iter #1450000:   Batch Loss = 10.264396, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.280338287353516, Accuracy = 0.8693217039108276
Training iter #1452000:   Batch Loss = 10.613453, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.154075622558594, Accuracy = 0.8649657964706421
Training iter #1454000:   Batch Loss = 10.793179, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.969673156738281, Accuracy = 0.8705662488937378
Training iter #1456000:   Batch Loss = 10.951764, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.150508880615234, Accuracy = 0.8662103414535522
Training iter #1458000:   Batch Loss = 10.065704, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.124364852905273, Accuracy = 0.8680771589279175
Training iter #1460000:   Batch Loss = 10.806780, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.263845443725586, Accuracy = 0.873677670955658
Training iter #1462000:   Batch Loss = 10.683067, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 12.121009826660156, Accuracy = 0.862476646900177
Training iter #1464000:   Batch Loss = 9.895808, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.111531257629395, Accuracy = 0.8817672729492188
Training iter #1466000:   Batch Loss = 10.114264, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.910819053649902, Accuracy = 0.8693217039108276
Training iter #1468000:   Batch Loss = 10.007881, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.02037239074707, Accuracy = 0.864343523979187
Training iter #1470000:   Batch Loss = 10.364397, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.085136413574219, Accuracy = 0.8686994314193726
Training iter #1472000:   Batch Loss = 10.441931, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.386700630187988, Accuracy = 0.8649657964706421
Training iter #1474000:   Batch Loss = 10.531341, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.222665786743164, Accuracy = 0.8587430119514465
Training iter #1476000:   Batch Loss = 10.056673, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.321647644042969, Accuracy = 0.853142499923706
Training iter #1478000:   Batch Loss = 10.757454, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.01730728149414, Accuracy = 0.8655880689620972
Training iter #1480000:   Batch Loss = 9.638062, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.043503761291504, Accuracy = 0.8730553984642029
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3700
Training iter #1482000:   Batch Loss = 10.440230, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.920228958129883, Accuracy = 0.8693217039108276
Training iter #1484000:   Batch Loss = 10.462041, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.40545654296875, Accuracy = 0.862476646900177
Training iter #1486000:   Batch Loss = 10.746706, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.053966522216797, Accuracy = 0.8668326139450073
Training iter #1488000:   Batch Loss = 10.518978, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.008026123046875, Accuracy = 0.864343523979187
Training iter #1490000:   Batch Loss = 10.640606, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.322929382324219, Accuracy = 0.8674548864364624
Training iter #1492000:   Batch Loss = 10.405943, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.923810005187988, Accuracy = 0.8749222159385681
Training iter #1494000:   Batch Loss = 9.976114, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.955354690551758, Accuracy = 0.8705662488937378
Training iter #1496000:   Batch Loss = 11.308540, Accuracy = 0.8913043737411499
PERFORMANCE ON TEST SET: Batch Loss = 12.19012451171875, Accuracy = 0.8730553984642029
Training iter #1498000:   Batch Loss = 10.019521, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.13447093963623, Accuracy = 0.8630989193916321
Training iter #1500000:   Batch Loss = 10.602549, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.948915481567383, Accuracy = 0.8630989193916321
Training iter #1502000:   Batch Loss = 10.604372, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.198202133178711, Accuracy = 0.8612321019172668
Training iter #1504000:   Batch Loss = 10.065437, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.14270305633545, Accuracy = 0.8686994314193726
Training iter #1506000:   Batch Loss = 9.909583, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.039626121520996, Accuracy = 0.874299943447113
Training iter #1508000:   Batch Loss = 10.232521, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.933691024780273, Accuracy = 0.8655880689620972
Training iter #1510000:   Batch Loss = 9.854016, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.728906631469727, Accuracy = 0.8767890334129333
Training iter #1512000:   Batch Loss = 9.937339, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.72256851196289, Accuracy = 0.8674548864364624
Training iter #1514000:   Batch Loss = 10.231272, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.963546752929688, Accuracy = 0.873677670955658
Training iter #1516000:   Batch Loss = 10.026873, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.065982818603516, Accuracy = 0.864343523979187
Training iter #1518000:   Batch Loss = 10.066381, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.026972770690918, Accuracy = 0.862476646900177
Training iter #1520000:   Batch Loss = 10.212112, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.154101371765137, Accuracy = 0.8668326139450073
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3800
Training iter #1522000:   Batch Loss = 10.220520, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.021453857421875, Accuracy = 0.8711885213851929
Training iter #1524000:   Batch Loss = 10.563241, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.126812934875488, Accuracy = 0.8680771589279175
Training iter #1526000:   Batch Loss = 10.933810, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 13.276973724365234, Accuracy = 0.8543870449066162
Training iter #1528000:   Batch Loss = 11.177537, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.073629379272461, Accuracy = 0.864343523979187
Training iter #1530000:   Batch Loss = 11.206047, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 13.02062702178955, Accuracy = 0.8581207394599915
Training iter #1532000:   Batch Loss = 10.978088, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.789800643920898, Accuracy = 0.8481642603874207
Training iter #1534000:   Batch Loss = 11.048242, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.645500183105469, Accuracy = 0.8593652844429016
Training iter #1536000:   Batch Loss = 10.879313, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.514784812927246, Accuracy = 0.8587430119514465
Training iter #1538000:   Batch Loss = 11.015589, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.50372314453125, Accuracy = 0.8662103414535522
Training iter #1540000:   Batch Loss = 10.580633, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.320989608764648, Accuracy = 0.8705662488937378
Training iter #1542000:   Batch Loss = 10.629887, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.205673217773438, Accuracy = 0.8780335783958435
Training iter #1544000:   Batch Loss = 10.473966, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.550165176391602, Accuracy = 0.874299943447113
Training iter #1546000:   Batch Loss = 10.846920, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.81789779663086, Accuracy = 0.862476646900177
Training iter #1548000:   Batch Loss = 10.821520, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.488574981689453, Accuracy = 0.8686994314193726
Training iter #1550000:   Batch Loss = 10.335781, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.164639472961426, Accuracy = 0.8724331259727478
Training iter #1552000:   Batch Loss = 10.492205, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.230754852294922, Accuracy = 0.8693217039108276
Training iter #1554000:   Batch Loss = 10.916553, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.132486343383789, Accuracy = 0.8612321019172668
Training iter #1556000:   Batch Loss = 10.037716, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.950401306152344, Accuracy = 0.8811450004577637
Training iter #1558000:   Batch Loss = 10.121606, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.131587982177734, Accuracy = 0.8668326139450073
Training iter #1560000:   Batch Loss = 10.948208, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.418512344360352, Accuracy = 0.8562538623809814
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-3900
Training iter #1562000:   Batch Loss = 10.750347, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.175192832946777, Accuracy = 0.8618543744087219
Training iter #1564000:   Batch Loss = 9.726732, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 12.048562049865723, Accuracy = 0.8718108534812927
Training iter #1566000:   Batch Loss = 10.229695, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.083073616027832, Accuracy = 0.864343523979187
Training iter #1568000:   Batch Loss = 10.244747, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.077095985412598, Accuracy = 0.8562538623809814
Training iter #1570000:   Batch Loss = 9.863584, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.860572814941406, Accuracy = 0.8649657964706421
Training iter #1572000:   Batch Loss = 10.053444, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.787940979003906, Accuracy = 0.8649657964706421
Training iter #1574000:   Batch Loss = 10.007824, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.95926284790039, Accuracy = 0.8705662488937378
Training iter #1576000:   Batch Loss = 10.000988, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.68283462524414, Accuracy = 0.8637211918830872
Training iter #1578000:   Batch Loss = 10.129565, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.253562927246094, Accuracy = 0.864343523979187
Training iter #1580000:   Batch Loss = 10.564279, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.114533424377441, Accuracy = 0.8612321019172668
Training iter #1582000:   Batch Loss = 10.336870, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.102160453796387, Accuracy = 0.8543870449066162
Training iter #1584000:   Batch Loss = 10.623147, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.88215446472168, Accuracy = 0.8674548864364624
Training iter #1586000:   Batch Loss = 10.106277, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.73119068145752, Accuracy = 0.8668326139450073
Training iter #1588000:   Batch Loss = 9.963652, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.679696083068848, Accuracy = 0.8693217039108276
Training iter #1590000:   Batch Loss = 9.781963, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.588142395019531, Accuracy = 0.8767890334129333
Training iter #1592000:   Batch Loss = 9.481352, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.751340866088867, Accuracy = 0.8705662488937378
Training iter #1594000:   Batch Loss = 9.788987, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.53253173828125, Accuracy = 0.8686994314193726
Training iter #1596000:   Batch Loss = 9.161495, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.374178886413574, Accuracy = 0.8755444884300232
Training iter #1598000:   Batch Loss = 8.609094, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 11.699541091918945, Accuracy = 0.8823895454406738
Training iter #1600000:   Batch Loss = 9.995255, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.788820266723633, Accuracy = 0.8662103414535522
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4000
Training iter #1602000:   Batch Loss = 10.376797, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.519550323486328, Accuracy = 0.8674548864364624
Training iter #1604000:   Batch Loss = 9.705736, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.380450248718262, Accuracy = 0.8693217039108276
Training iter #1606000:   Batch Loss = 9.693817, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.545733451843262, Accuracy = 0.8674548864364624
Training iter #1608000:   Batch Loss = 10.236511, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.819038391113281, Accuracy = 0.8724331259727478
Training iter #1610000:   Batch Loss = 9.919176, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.400568008422852, Accuracy = 0.8792781829833984
Training iter #1612000:   Batch Loss = 9.855748, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.726072311401367, Accuracy = 0.8749222159385681
Training iter #1614000:   Batch Loss = 9.718287, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.449365615844727, Accuracy = 0.8718108534812927
Training iter #1616000:   Batch Loss = 9.491318, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.374422073364258, Accuracy = 0.8767890334129333
Training iter #1618000:   Batch Loss = 9.783352, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.201300621032715, Accuracy = 0.8811450004577637
Training iter #1620000:   Batch Loss = 9.743511, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.092084884643555, Accuracy = 0.8855009078979492
Training iter #1622000:   Batch Loss = 9.844732, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.088214874267578, Accuracy = 0.8805227279663086
Training iter #1624000:   Batch Loss = 9.324057, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.071389198303223, Accuracy = 0.8811450004577637
Training iter #1626000:   Batch Loss = 9.878372, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.572189331054688, Accuracy = 0.8873677849769592
Training iter #1628000:   Batch Loss = 10.047572, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.44184684753418, Accuracy = 0.8755444884300232
Training iter #1630000:   Batch Loss = 10.902023, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.36313533782959, Accuracy = 0.8612321019172668
Training iter #1632000:   Batch Loss = 11.165466, Accuracy = 0.9130434989929199
PERFORMANCE ON TEST SET: Batch Loss = 12.608475685119629, Accuracy = 0.8718108534812927
Training iter #1634000:   Batch Loss = 10.335733, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.20247745513916, Accuracy = 0.8711885213851929
Training iter #1636000:   Batch Loss = 10.215352, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.036351203918457, Accuracy = 0.873677670955658
Training iter #1638000:   Batch Loss = 10.306232, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.877833366394043, Accuracy = 0.8830118179321289
Training iter #1640000:   Batch Loss = 10.029032, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.729390144348145, Accuracy = 0.8786558508872986
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4100
Training iter #1642000:   Batch Loss = 10.030838, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.580381393432617, Accuracy = 0.8711885213851929
Training iter #1644000:   Batch Loss = 9.695356, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.63081169128418, Accuracy = 0.8668326139450073
Training iter #1646000:   Batch Loss = 9.607145, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.70969295501709, Accuracy = 0.8755444884300232
Training iter #1648000:   Batch Loss = 9.862951, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.49091911315918, Accuracy = 0.8780335783958435
Training iter #1650000:   Batch Loss = 9.312179, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.500328063964844, Accuracy = 0.8799004554748535
Training iter #1652000:   Batch Loss = 9.569459, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.409492492675781, Accuracy = 0.8718108534812927
Training iter #1654000:   Batch Loss = 9.881431, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.384819030761719, Accuracy = 0.883634090423584
Training iter #1656000:   Batch Loss = 9.304413, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.31267261505127, Accuracy = 0.8842563629150391
Training iter #1658000:   Batch Loss = 9.720715, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.254810333251953, Accuracy = 0.8724331259727478
Training iter #1660000:   Batch Loss = 9.367361, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.305784225463867, Accuracy = 0.8711885213851929
Training iter #1662000:   Batch Loss = 9.577279, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.297772407531738, Accuracy = 0.8724331259727478
Training iter #1664000:   Batch Loss = 9.561618, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.618573188781738, Accuracy = 0.8873677849769592
Training iter #1666000:   Batch Loss = 9.013494, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.559541702270508, Accuracy = 0.874299943447113
Training iter #1668000:   Batch Loss = 9.592905, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.594202041625977, Accuracy = 0.8755444884300232
Training iter #1670000:   Batch Loss = 9.596656, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.558324813842773, Accuracy = 0.8817672729492188
Training iter #1672000:   Batch Loss = 9.638584, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.257211685180664, Accuracy = 0.8823895454406738
Training iter #1674000:   Batch Loss = 10.079500, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.65595531463623, Accuracy = 0.8780335783958435
Training iter #1676000:   Batch Loss = 9.663778, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.51008415222168, Accuracy = 0.8755444884300232
Training iter #1678000:   Batch Loss = 10.106363, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.449827194213867, Accuracy = 0.883634090423584
Training iter #1680000:   Batch Loss = 9.507593, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.427370071411133, Accuracy = 0.8718108534812927
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4200
Training iter #1682000:   Batch Loss = 9.507067, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.280529022216797, Accuracy = 0.874299943447113
Training iter #1684000:   Batch Loss = 9.210969, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.251487731933594, Accuracy = 0.8761667609214783
Training iter #1686000:   Batch Loss = 9.431623, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.14726734161377, Accuracy = 0.873677670955658
Training iter #1688000:   Batch Loss = 9.411209, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.13497257232666, Accuracy = 0.8842563629150391
Training iter #1690000:   Batch Loss = 9.209414, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.137470245361328, Accuracy = 0.8817672729492188
Training iter #1692000:   Batch Loss = 9.400980, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.070169448852539, Accuracy = 0.8898568749427795
Training iter #1694000:   Batch Loss = 9.577404, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.834360122680664, Accuracy = 0.8668326139450073
Training iter #1696000:   Batch Loss = 9.710849, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.44149398803711, Accuracy = 0.8898568749427795
Training iter #1698000:   Batch Loss = 9.334272, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.376919746398926, Accuracy = 0.8799004554748535
Training iter #1700000:   Batch Loss = 8.965471, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.40507698059082, Accuracy = 0.8767890334129333
Training iter #1702000:   Batch Loss = 10.941839, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.957684516906738, Accuracy = 0.8755444884300232
Training iter #1704000:   Batch Loss = 10.973030, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.560209274291992, Accuracy = 0.8730553984642029
Training iter #1706000:   Batch Loss = 10.298189, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.960018157958984, Accuracy = 0.8792781829833984
Training iter #1708000:   Batch Loss = 9.701806, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.913125991821289, Accuracy = 0.8568761944770813
Training iter #1710000:   Batch Loss = 10.001850, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.662216186523438, Accuracy = 0.8593652844429016
Training iter #1712000:   Batch Loss = 10.177958, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.485666275024414, Accuracy = 0.8718108534812927
Training iter #1714000:   Batch Loss = 9.472342, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.36556339263916, Accuracy = 0.8711885213851929
Training iter #1716000:   Batch Loss = 9.647567, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.219915390014648, Accuracy = 0.8805227279663086
Training iter #1718000:   Batch Loss = 9.211960, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.220172882080078, Accuracy = 0.8780335783958435
Training iter #1720000:   Batch Loss = 9.319667, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.128907203674316, Accuracy = 0.8774113059043884
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4300
Training iter #1722000:   Batch Loss = 9.168694, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.194366455078125, Accuracy = 0.8811450004577637
Training iter #1724000:   Batch Loss = 9.512851, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.104731559753418, Accuracy = 0.8811450004577637
Training iter #1726000:   Batch Loss = 9.509127, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.137173652648926, Accuracy = 0.883634090423584
Training iter #1728000:   Batch Loss = 9.553666, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.213096618652344, Accuracy = 0.8786558508872986
Training iter #1730000:   Batch Loss = 9.214288, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.190794944763184, Accuracy = 0.8855009078979492
Training iter #1732000:   Batch Loss = 9.661526, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.357099533081055, Accuracy = 0.8780335783958435
Training iter #1734000:   Batch Loss = 10.223907, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.527242660522461, Accuracy = 0.8612321019172668
Training iter #1736000:   Batch Loss = 9.856964, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.661301612854004, Accuracy = 0.8686994314193726
Training iter #1738000:   Batch Loss = 9.779498, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.507081031799316, Accuracy = 0.8755444884300232
Training iter #1740000:   Batch Loss = 9.773609, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.443459510803223, Accuracy = 0.883634090423584
Training iter #1742000:   Batch Loss = 9.989844, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.694527626037598, Accuracy = 0.8780335783958435
Training iter #1744000:   Batch Loss = 9.686813, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.382576942443848, Accuracy = 0.8699439764022827
Training iter #1746000:   Batch Loss = 11.317568, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.585400581359863, Accuracy = 0.8718108534812927
Training iter #1748000:   Batch Loss = 11.159472, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.703311920166016, Accuracy = 0.874299943447113
Training iter #1750000:   Batch Loss = 10.463169, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.161881446838379, Accuracy = 0.8668326139450073
Training iter #1752000:   Batch Loss = 10.373577, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.968758583068848, Accuracy = 0.8612321019172668
Training iter #1754000:   Batch Loss = 10.089353, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.671499252319336, Accuracy = 0.8842563629150391
Training iter #1756000:   Batch Loss = 10.207091, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.89223575592041, Accuracy = 0.8786558508872986
Training iter #1758000:   Batch Loss = 10.245434, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.844878196716309, Accuracy = 0.8786558508872986
Training iter #1760000:   Batch Loss = 10.061793, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.864082336425781, Accuracy = 0.8711885213851929
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4400
Training iter #1762000:   Batch Loss = 9.934598, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.719812393188477, Accuracy = 0.873677670955658
Training iter #1764000:   Batch Loss = 9.418983, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.536112785339355, Accuracy = 0.8792781829833984
Training iter #1766000:   Batch Loss = 9.844340, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.424456596374512, Accuracy = 0.874299943447113
Training iter #1768000:   Batch Loss = 9.968455, Accuracy = 0.9347826242446899
PERFORMANCE ON TEST SET: Batch Loss = 11.379047393798828, Accuracy = 0.8830118179321289
Training iter #1770000:   Batch Loss = 9.689051, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.292978286743164, Accuracy = 0.8855009078979492
Training iter #1772000:   Batch Loss = 9.351097, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.356654167175293, Accuracy = 0.8873677849769592
Training iter #1774000:   Batch Loss = 9.195551, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.132431030273438, Accuracy = 0.8892346024513245
Training iter #1776000:   Batch Loss = 9.704446, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.475290298461914, Accuracy = 0.8811450004577637
Training iter #1778000:   Batch Loss = 9.570002, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.285435676574707, Accuracy = 0.8761667609214783
Training iter #1780000:   Batch Loss = 9.158956, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.180646896362305, Accuracy = 0.8886123299598694
Training iter #1782000:   Batch Loss = 9.257374, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.056222915649414, Accuracy = 0.8830118179321289
Training iter #1784000:   Batch Loss = 9.528120, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.115316390991211, Accuracy = 0.8830118179321289
Training iter #1786000:   Batch Loss = 9.353624, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.227067947387695, Accuracy = 0.8805227279663086
Training iter #1788000:   Batch Loss = 9.316890, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.373947143554688, Accuracy = 0.8855009078979492
Training iter #1790000:   Batch Loss = 9.614437, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.464414596557617, Accuracy = 0.8855009078979492
Training iter #1792000:   Batch Loss = 9.872287, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.569446563720703, Accuracy = 0.8730553984642029
Training iter #1794000:   Batch Loss = 9.614408, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.31082534790039, Accuracy = 0.8755444884300232
Training iter #1796000:   Batch Loss = 9.724167, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.249979019165039, Accuracy = 0.8817672729492188
Training iter #1798000:   Batch Loss = 9.285440, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.033097267150879, Accuracy = 0.8823895454406738
Training iter #1800000:   Batch Loss = 8.910854, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.272390365600586, Accuracy = 0.883634090423584
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4500
Training iter #1802000:   Batch Loss = 8.342681, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.194483757019043, Accuracy = 0.8755444884300232
Training iter #1804000:   Batch Loss = 8.988939, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.116987228393555, Accuracy = 0.8879900574684143
Training iter #1806000:   Batch Loss = 8.839521, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.99321460723877, Accuracy = 0.8898568749427795
Training iter #1808000:   Batch Loss = 9.225259, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.912851333618164, Accuracy = 0.8886123299598694
Training iter #1810000:   Batch Loss = 9.619658, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.086759567260742, Accuracy = 0.8929682374000549
Training iter #1812000:   Batch Loss = 9.132463, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.18610954284668, Accuracy = 0.883634090423584
Training iter #1814000:   Batch Loss = 9.127918, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.129539489746094, Accuracy = 0.8805227279663086
Training iter #1816000:   Batch Loss = 9.292970, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.037342071533203, Accuracy = 0.8923459649085999
Training iter #1818000:   Batch Loss = 9.210869, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.079822540283203, Accuracy = 0.8898568749427795
Training iter #1820000:   Batch Loss = 9.189320, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.018027305603027, Accuracy = 0.883634090423584
Training iter #1822000:   Batch Loss = 8.900027, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.917471885681152, Accuracy = 0.8861232399940491
Training iter #1824000:   Batch Loss = 8.628319, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.03495979309082, Accuracy = 0.8942128419876099
Training iter #1826000:   Batch Loss = 8.763656, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.164188385009766, Accuracy = 0.8799004554748535
Training iter #1828000:   Batch Loss = 9.139368, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.869370460510254, Accuracy = 0.8942128419876099
Training iter #1830000:   Batch Loss = 10.588881, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.023971557617188, Accuracy = 0.8649657964706421
Training iter #1832000:   Batch Loss = 9.569868, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.529986381530762, Accuracy = 0.8786558508872986
Training iter #1834000:   Batch Loss = 9.482759, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.092135429382324, Accuracy = 0.8861232399940491
Training iter #1836000:   Batch Loss = 10.145163, Accuracy = 0.95652174949646
PERFORMANCE ON TEST SET: Batch Loss = 11.195842742919922, Accuracy = 0.8718108534812927
Training iter #1838000:   Batch Loss = 9.972564, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.329087257385254, Accuracy = 0.8693217039108276
Training iter #1840000:   Batch Loss = 9.123207, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.35529613494873, Accuracy = 0.8761667609214783
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4600
Training iter #1842000:   Batch Loss = 9.724783, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.037202835083008, Accuracy = 0.8767890334129333
Training iter #1844000:   Batch Loss = 9.467848, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.164015769958496, Accuracy = 0.8774113059043884
Training iter #1846000:   Batch Loss = 9.191935, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.184471130371094, Accuracy = 0.8886123299598694
Training iter #1848000:   Batch Loss = 9.349806, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.131889343261719, Accuracy = 0.8786558508872986
Training iter #1850000:   Batch Loss = 10.222140, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.997516632080078, Accuracy = 0.8830118179321289
Training iter #1852000:   Batch Loss = 9.727115, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.682626724243164, Accuracy = 0.873677670955658
Training iter #1854000:   Batch Loss = 9.657161, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.714313507080078, Accuracy = 0.8830118179321289
Training iter #1856000:   Batch Loss = 10.395394, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.773990631103516, Accuracy = 0.8637211918830872
Training iter #1858000:   Batch Loss = 11.227752, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.696701049804688, Accuracy = 0.8606098294258118
Training iter #1860000:   Batch Loss = 10.808419, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.324846267700195, Accuracy = 0.8649657964706421
Training iter #1862000:   Batch Loss = 10.414731, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.903583526611328, Accuracy = 0.8730553984642029
Training iter #1864000:   Batch Loss = 10.333034, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.88295841217041, Accuracy = 0.8674548864364624
Training iter #1866000:   Batch Loss = 9.940910, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.622095108032227, Accuracy = 0.8780335783958435
Training iter #1868000:   Batch Loss = 9.853549, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.602054595947266, Accuracy = 0.8780335783958435
Training iter #1870000:   Batch Loss = 9.683450, Accuracy = 0.8695651888847351
PERFORMANCE ON TEST SET: Batch Loss = 11.439826011657715, Accuracy = 0.8805227279663086
Training iter #1872000:   Batch Loss = 9.855598, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.33405876159668, Accuracy = 0.8823895454406738
Training iter #1874000:   Batch Loss = 9.580231, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.287261962890625, Accuracy = 0.8823895454406738
Training iter #1876000:   Batch Loss = 9.875501, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.131482124328613, Accuracy = 0.8823895454406738
Training iter #1878000:   Batch Loss = 9.676121, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.164140701293945, Accuracy = 0.8817672729492188
Training iter #1880000:   Batch Loss = 9.048660, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.115715026855469, Accuracy = 0.8780335783958435
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4700
Training iter #1882000:   Batch Loss = 9.617949, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.257083892822266, Accuracy = 0.8830118179321289
Training iter #1884000:   Batch Loss = 9.660680, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.128379821777344, Accuracy = 0.8867455124855042
Training iter #1886000:   Batch Loss = 9.395717, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.292725563049316, Accuracy = 0.8799004554748535
Training iter #1888000:   Batch Loss = 9.289669, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.985265731811523, Accuracy = 0.8830118179321289
Training iter #1890000:   Batch Loss = 8.946815, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.32229995727539, Accuracy = 0.8886123299598694
Training iter #1892000:   Batch Loss = 9.902853, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.92430591583252, Accuracy = 0.8811450004577637
Training iter #1894000:   Batch Loss = 9.444531, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.518915176391602, Accuracy = 0.883634090423584
Training iter #1896000:   Batch Loss = 9.292744, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.158586502075195, Accuracy = 0.8861232399940491
Training iter #1898000:   Batch Loss = 9.045736, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.177742004394531, Accuracy = 0.8929682374000549
Training iter #1900000:   Batch Loss = 8.935692, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.154154777526855, Accuracy = 0.8873677849769592
Training iter #1902000:   Batch Loss = 9.453586, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.945950508117676, Accuracy = 0.8855009078979492
Training iter #1904000:   Batch Loss = 9.168331, Accuracy = 0.97826087474823
PERFORMANCE ON TEST SET: Batch Loss = 12.15184211730957, Accuracy = 0.8699439764022827
Training iter #1906000:   Batch Loss = 10.346212, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.91030216217041, Accuracy = 0.8879900574684143
Training iter #1908000:   Batch Loss = 9.723922, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.781795501708984, Accuracy = 0.8767890334129333
Training iter #1910000:   Batch Loss = 9.536610, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.721238136291504, Accuracy = 0.8848786354064941
Training iter #1912000:   Batch Loss = 9.592618, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.7379732131958, Accuracy = 0.8848786354064941
Training iter #1914000:   Batch Loss = 9.402528, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.392928123474121, Accuracy = 0.883634090423584
Training iter #1916000:   Batch Loss = 10.119071, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.464736938476562, Accuracy = 0.8780335783958435
Training iter #1918000:   Batch Loss = 9.690487, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.415711402893066, Accuracy = 0.8842563629150391
Training iter #1920000:   Batch Loss = 9.986702, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.20417308807373, Accuracy = 0.8991910219192505
Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-4800
Training iter #1922000:   Batch Loss = 10.435852, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.517067909240723, Accuracy = 0.8867455124855042
Training iter #1924000:   Batch Loss = 10.972080, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.248208045959473, Accuracy = 0.8805227279663086
Training iter #1926000:   Batch Loss = 11.267996, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.77324104309082, Accuracy = 0.874299943447113
Training iter #1928000:   Batch Loss = 11.433547, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.718564987182617, Accuracy = 0.8686994314193726
Training iter #1930000:   Batch Loss = 11.026072, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.48987102508545, Accuracy = 0.8780335783958435
Training iter #1932000:   Batch Loss = 10.877009, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.581727981567383, Accuracy = 0.8711885213851929
Optimization Finished!
FINAL RESULT: Batch Loss = 12.575441360473633, Accuracy = 0.8630989193916321
All train time = 17195.160686969757
Final Model saved in file: ./lstm2/model_selftest_kfold3.ckpt-final
Precision: 86.4798615017476%
Recall: 86.30989421281892%
f1_score: 86.3102853178041%

Confusion Matrix:
[[146   1   3   0   0   4   0   0   2   5]
 [  1 151   0   0   0   0   0   0   8   1]
 [  3   0 130   1   0   1   5   1  18   2]
 [  0   1   4 132   5   0   0  18   0   2]
 [  1   0   0   3 156   0   0   0   0   0]
 [  0   0   0   0   0 161   0   0   2   0]
 [  5   1   4   3   0   0 132  13   2   1]
 [  3   0   7   8   0   0  28 115   0   0]
 [  0   2  36   0   0   0   2   1 115   0]
 [  9   1   0   0   0   1   1   0   0 149]]

Confusion matrix (normalised to % of total test data):
[[ 9.085252    0.06222776  0.18668327  0.          0.          0.24891102
   0.          0.          0.12445551  0.31113875]
 [ 0.06222776  9.396391    0.          0.          0.          0.
   0.          0.          0.49782205  0.06222776]
 [ 0.18668327  0.          8.089608    0.06222776  0.          0.06222776
   0.31113875  0.06222776  1.1200995   0.12445551]
 [ 0.          0.06222776  0.24891102  8.214064    0.31113875  0.
   0.          1.1200995   0.          0.12445551]
 [ 0.06222776  0.          0.          0.18668327  9.70753     0.
   0.          0.          0.          0.        ]
 [ 0.          0.          0.          0.          0.         10.018668
   0.          0.          0.12445551  0.        ]
 [ 0.31113875  0.06222776  0.24891102  0.18668327  0.          0.
   8.214064    0.80896074  0.12445551  0.06222776]
 [ 0.18668327  0.          0.4355943   0.49782205  0.          0.
   1.7423772   7.156192    0.          0.        ]
 [ 0.          0.12445551  2.240199    0.          0.          0.
   0.12445551  0.06222776  7.156192    0.        ]
 [ 0.5600498   0.06222776  0.          0.          0.          0.06222776
   0.06222776  0.          0.          9.271935  ]]/home/sunrepe/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))

Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.

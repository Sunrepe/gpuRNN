WARNING:tensorflow:From all_three_lstm.py:91: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:93: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From all_three_lstm.py:94: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From all_three_lstm.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From all_three_lstm.py:97: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
loading data...
train: 6458 test: 1595
load data time: 191.45445442199707
Start train!
Training iter #400:   Batch Loss = 26.867498, Accuracy = 0.07999999821186066
PERFORMANCE ON TEST SET: Batch Loss = 26.41322898864746, Accuracy = 0.1573667675256729
Training iter #2000:   Batch Loss = 25.399176, Accuracy = 0.27250000834465027
PERFORMANCE ON TEST SET: Batch Loss = 25.071882247924805, Accuracy = 0.29592475295066833
Training iter #4000:   Batch Loss = 23.775869, Accuracy = 0.36000001430511475
PERFORMANCE ON TEST SET: Batch Loss = 23.419300079345703, Accuracy = 0.3874608278274536
Training iter #6000:   Batch Loss = 22.015093, Accuracy = 0.41749998927116394
PERFORMANCE ON TEST SET: Batch Loss = 21.95159912109375, Accuracy = 0.41567397117614746
Training iter #8000:   Batch Loss = 21.085384, Accuracy = 0.4050000011920929
PERFORMANCE ON TEST SET: Batch Loss = 20.766904830932617, Accuracy = 0.42131662368774414
Training iter #10000:   Batch Loss = 20.078651, Accuracy = 0.4325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 19.570234298706055, Accuracy = 0.5047022104263306
Training iter #12000:   Batch Loss = 18.906183, Accuracy = 0.5350000262260437
PERFORMANCE ON TEST SET: Batch Loss = 18.58592987060547, Accuracy = 0.5579937100410461
Training iter #14000:   Batch Loss = 17.545166, Accuracy = 0.6324999928474426
PERFORMANCE ON TEST SET: Batch Loss = 17.68950843811035, Accuracy = 0.5949843525886536
Training iter #16000:   Batch Loss = 17.568373, Accuracy = 0.5625
PERFORMANCE ON TEST SET: Batch Loss = 16.97275733947754, Accuracy = 0.615047037601471
Training iter #18000:   Batch Loss = 16.402304, Accuracy = 0.6800000071525574
PERFORMANCE ON TEST SET: Batch Loss = 16.226232528686523, Accuracy = 0.6507837176322937
Training iter #20000:   Batch Loss = 15.572433, Accuracy = 0.6600000262260437
PERFORMANCE ON TEST SET: Batch Loss = 15.702858924865723, Accuracy = 0.6652037501335144
Training iter #22000:   Batch Loss = 15.006404, Accuracy = 0.6899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 15.375596046447754, Accuracy = 0.6683385372161865
Training iter #24000:   Batch Loss = 14.760846, Accuracy = 0.6675000190734863
PERFORMANCE ON TEST SET: Batch Loss = 14.789420127868652, Accuracy = 0.6796238422393799
Training iter #26000:   Batch Loss = 14.392980, Accuracy = 0.6924999952316284
PERFORMANCE ON TEST SET: Batch Loss = 14.365821838378906, Accuracy = 0.6971786618232727
Training iter #28000:   Batch Loss = 14.322235, Accuracy = 0.737500011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.345074653625488, Accuracy = 0.7003135085105896
Training iter #30000:   Batch Loss = 14.193612, Accuracy = 0.737500011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.092042922973633, Accuracy = 0.7122256755828857
Training iter #32000:   Batch Loss = 14.175412, Accuracy = 0.6850000023841858
PERFORMANCE ON TEST SET: Batch Loss = 13.923410415649414, Accuracy = 0.702194333076477
Training iter #34000:   Batch Loss = 12.559443, Accuracy = 0.7241379022598267
PERFORMANCE ON TEST SET: Batch Loss = 13.83212661743164, Accuracy = 0.7266457676887512
Training iter #36000:   Batch Loss = 13.231709, Accuracy = 0.7574999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.51061725616455, Accuracy = 0.73542320728302
Training iter #38000:   Batch Loss = 12.686245, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.250275611877441, Accuracy = 0.7329153418540955
Training iter #40000:   Batch Loss = 12.863930, Accuracy = 0.7250000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.007144927978516, Accuracy = 0.739811897277832
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-100
Training iter #42000:   Batch Loss = 12.159510, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.875710487365723, Accuracy = 0.7673981189727783
Training iter #44000:   Batch Loss = 12.766058, Accuracy = 0.7699999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.760616302490234, Accuracy = 0.751724123954773
Training iter #46000:   Batch Loss = 12.796696, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.772754669189453, Accuracy = 0.7498432397842407
Training iter #48000:   Batch Loss = 12.410002, Accuracy = 0.7825000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.70670223236084, Accuracy = 0.7617554664611816
Training iter #50000:   Batch Loss = 11.926912, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.391291618347168, Accuracy = 0.757993757724762
Training iter #52000:   Batch Loss = 12.243606, Accuracy = 0.7549999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.146097183227539, Accuracy = 0.7786833643913269
Training iter #54000:   Batch Loss = 11.499688, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 12.1487455368042, Accuracy = 0.773040771484375
Training iter #56000:   Batch Loss = 11.650274, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.317031860351562, Accuracy = 0.7567397952079773
Training iter #58000:   Batch Loss = 12.202611, Accuracy = 0.7699999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.127702713012695, Accuracy = 0.7724137902259827
Training iter #60000:   Batch Loss = 11.129072, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.97165584564209, Accuracy = 0.7749216556549072
Training iter #62000:   Batch Loss = 11.459515, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.465421676635742, Accuracy = 0.7717868089675903
Training iter #64000:   Batch Loss = 11.888149, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.281773567199707, Accuracy = 0.7793103456497192
Training iter #66000:   Batch Loss = 11.600235, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.833775520324707, Accuracy = 0.7949843406677246
Training iter #68000:   Batch Loss = 12.443159, Accuracy = 0.7931034564971924
PERFORMANCE ON TEST SET: Batch Loss = 11.799993515014648, Accuracy = 0.8037617802619934
Training iter #70000:   Batch Loss = 11.335524, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.700401306152344, Accuracy = 0.7943573594093323
Training iter #72000:   Batch Loss = 11.664586, Accuracy = 0.7799999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.601759910583496, Accuracy = 0.8131661415100098
Training iter #74000:   Batch Loss = 11.827575, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.951866149902344, Accuracy = 0.7968652248382568
Training iter #76000:   Batch Loss = 11.153667, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.76099967956543, Accuracy = 0.7993730306625366
Training iter #78000:   Batch Loss = 11.743520, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.093902587890625, Accuracy = 0.8018808960914612
Training iter #80000:   Batch Loss = 11.627047, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.80736255645752, Accuracy = 0.7874608039855957
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-200
Training iter #82000:   Batch Loss = 11.598613, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.549126625061035, Accuracy = 0.8163009285926819
Training iter #84000:   Batch Loss = 10.814042, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.499849319458008, Accuracy = 0.8094043731689453
Training iter #86000:   Batch Loss = 11.021530, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.5047025680542, Accuracy = 0.8075234889984131
Training iter #88000:   Batch Loss = 10.608444, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.472694396972656, Accuracy = 0.8119122385978699
Training iter #90000:   Batch Loss = 10.868759, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.532304763793945, Accuracy = 0.8137931227684021
Training iter #92000:   Batch Loss = 10.933772, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.404163360595703, Accuracy = 0.8062695860862732
Training iter #94000:   Batch Loss = 11.408010, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.76081371307373, Accuracy = 0.8050156831741333
Training iter #96000:   Batch Loss = 11.840664, Accuracy = 0.8050000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.839242935180664, Accuracy = 0.8050156831741333
Training iter #98000:   Batch Loss = 11.041510, Accuracy = 0.8224999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.985715866088867, Accuracy = 0.8062695860862732
Training iter #100000:   Batch Loss = 11.307486, Accuracy = 0.8299999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.780461311340332, Accuracy = 0.815047025680542
Training iter #102000:   Batch Loss = 12.094110, Accuracy = 0.7931034564971924
PERFORMANCE ON TEST SET: Batch Loss = 12.035325050354004, Accuracy = 0.7968652248382568
Training iter #104000:   Batch Loss = 12.400719, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.607489585876465, Accuracy = 0.8012539148330688
Training iter #106000:   Batch Loss = 11.874398, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.216655731201172, Accuracy = 0.7974921464920044
Training iter #108000:   Batch Loss = 11.332708, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.04240608215332, Accuracy = 0.8100313544273376
Training iter #110000:   Batch Loss = 10.894030, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.029230117797852, Accuracy = 0.81065833568573
Training iter #112000:   Batch Loss = 11.064493, Accuracy = 0.8125
PERFORMANCE ON TEST SET: Batch Loss = 11.650128364562988, Accuracy = 0.8213165998458862
Training iter #114000:   Batch Loss = 11.034760, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.45740032196045, Accuracy = 0.8163009285926819
Training iter #116000:   Batch Loss = 10.911017, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.495681762695312, Accuracy = 0.8206896781921387
Training iter #118000:   Batch Loss = 11.452627, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.465490341186523, Accuracy = 0.8156740069389343
Training iter #120000:   Batch Loss = 10.690219, Accuracy = 0.8374999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.375299453735352, Accuracy = 0.8100313544273376
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-300
Training iter #122000:   Batch Loss = 10.939245, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.34644889831543, Accuracy = 0.8188087940216064
Training iter #124000:   Batch Loss = 10.013647, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.240286827087402, Accuracy = 0.825705349445343
Training iter #126000:   Batch Loss = 10.966375, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.407220840454102, Accuracy = 0.825705349445343
Training iter #128000:   Batch Loss = 10.608795, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.245739936828613, Accuracy = 0.8351097106933594
Training iter #130000:   Batch Loss = 10.602539, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.452247619628906, Accuracy = 0.8275862336158752
Training iter #132000:   Batch Loss = 10.139391, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.11532974243164, Accuracy = 0.8357366919517517
Training iter #134000:   Batch Loss = 10.638943, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.982603073120117, Accuracy = 0.8282131552696228
Training iter #136000:   Batch Loss = 10.566135, Accuracy = 0.8965517282485962
PERFORMANCE ON TEST SET: Batch Loss = 10.946091651916504, Accuracy = 0.8369905948638916
Training iter #138000:   Batch Loss = 9.990515, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.025357246398926, Accuracy = 0.8357366919517517
Training iter #140000:   Batch Loss = 10.154516, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.012251853942871, Accuracy = 0.8288401365280151
Training iter #142000:   Batch Loss = 10.684082, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.030927658081055, Accuracy = 0.8332288265228271
Training iter #144000:   Batch Loss = 9.893107, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.91899299621582, Accuracy = 0.8401253819465637
Training iter #146000:   Batch Loss = 9.597243, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.843866348266602, Accuracy = 0.8451410531997681
Training iter #148000:   Batch Loss = 10.174945, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.684700965881348, Accuracy = 0.8369905948638916
Training iter #150000:   Batch Loss = 9.623766, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.828758239746094, Accuracy = 0.8388714790344238
Training iter #152000:   Batch Loss = 9.873264, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.549159049987793, Accuracy = 0.8413792848587036
Training iter #154000:   Batch Loss = 9.836148, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.587584495544434, Accuracy = 0.8369905948638916
Training iter #156000:   Batch Loss = 9.415092, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.68118667602539, Accuracy = 0.8432601690292358
Training iter #158000:   Batch Loss = 9.451026, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.736310005187988, Accuracy = 0.8394984602928162
Training iter #160000:   Batch Loss = 9.908042, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.549736976623535, Accuracy = 0.8520376086235046
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-400
Training iter #162000:   Batch Loss = 9.556980, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.635388374328613, Accuracy = 0.846394956111908
Training iter #164000:   Batch Loss = 10.217340, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.57475471496582, Accuracy = 0.8495298027992249
Training iter #166000:   Batch Loss = 9.024570, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.55112361907959, Accuracy = 0.840752363204956
Training iter #168000:   Batch Loss = 9.414930, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.397605895996094, Accuracy = 0.8507837057113647
Training iter #170000:   Batch Loss = 8.119674, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 10.515216827392578, Accuracy = 0.8457680344581604
Training iter #172000:   Batch Loss = 9.604187, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.459644317626953, Accuracy = 0.857053279876709
Training iter #174000:   Batch Loss = 9.197374, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.448776245117188, Accuracy = 0.8514106869697571
Training iter #176000:   Batch Loss = 9.753654, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.399356842041016, Accuracy = 0.8564263582229614
Training iter #178000:   Batch Loss = 9.633243, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.640953063964844, Accuracy = 0.8482758402824402
Training iter #180000:   Batch Loss = 9.406539, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.547893524169922, Accuracy = 0.8539184927940369
Training iter #182000:   Batch Loss = 9.909279, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.202436447143555, Accuracy = 0.8564263582229614
Training iter #184000:   Batch Loss = 10.766444, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.539739608764648, Accuracy = 0.8288401365280151
Training iter #186000:   Batch Loss = 11.014102, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.737350463867188, Accuracy = 0.8413792848587036
Training iter #188000:   Batch Loss = 10.796379, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.548517227172852, Accuracy = 0.8495298027992249
Training iter #190000:   Batch Loss = 10.964723, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.557787895202637, Accuracy = 0.8451410531997681
Training iter #192000:   Batch Loss = 10.645023, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.651323318481445, Accuracy = 0.8382444977760315
Training iter #194000:   Batch Loss = 10.715317, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.264336585998535, Accuracy = 0.8401253819465637
Training iter #196000:   Batch Loss = 10.079990, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.098669052124023, Accuracy = 0.846394956111908
Training iter #198000:   Batch Loss = 9.923820, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.035384178161621, Accuracy = 0.8501567244529724
Training iter #200000:   Batch Loss = 9.926552, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.802849769592285, Accuracy = 0.8501567244529724
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-500
Training iter #202000:   Batch Loss = 10.106196, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 10.742391586303711, Accuracy = 0.8601880669593811
Training iter #204000:   Batch Loss = 9.823746, Accuracy = 0.9137930870056152
PERFORMANCE ON TEST SET: Batch Loss = 10.87287712097168, Accuracy = 0.8589341640472412
Training iter #206000:   Batch Loss = 10.682169, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.990878105163574, Accuracy = 0.8470219373703003
Training iter #208000:   Batch Loss = 10.808438, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.558767318725586, Accuracy = 0.8394984602928162
Training iter #210000:   Batch Loss = 10.686502, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.417823791503906, Accuracy = 0.8532915115356445
Training iter #212000:   Batch Loss = 10.709149, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.288036346435547, Accuracy = 0.8445141315460205
Training iter #214000:   Batch Loss = 10.015512, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.162189483642578, Accuracy = 0.8476489186286926
Training iter #216000:   Batch Loss = 9.888855, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.092111587524414, Accuracy = 0.8482758402824402
Training iter #218000:   Batch Loss = 10.153563, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.078095436096191, Accuracy = 0.8532915115356445
Training iter #220000:   Batch Loss = 9.809038, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.056707382202148, Accuracy = 0.8539184927940369
Training iter #222000:   Batch Loss = 9.821320, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.91657829284668, Accuracy = 0.8589341640472412
Training iter #224000:   Batch Loss = 10.477045, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.94068717956543, Accuracy = 0.8608150482177734
Training iter #226000:   Batch Loss = 9.783789, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.931297302246094, Accuracy = 0.8576802611351013
Training iter #228000:   Batch Loss = 9.841295, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.857470512390137, Accuracy = 0.8639498353004456
Training iter #230000:   Batch Loss = 10.332577, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.164034843444824, Accuracy = 0.8595611453056335
Training iter #232000:   Batch Loss = 10.097261, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.08464241027832, Accuracy = 0.8608150482177734
Training iter #234000:   Batch Loss = 9.505550, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.820817947387695, Accuracy = 0.863322913646698
Training iter #236000:   Batch Loss = 9.582293, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.842816352844238, Accuracy = 0.8557993769645691
Training iter #238000:   Batch Loss = 8.798706, Accuracy = 0.9137930870056152
PERFORMANCE ON TEST SET: Batch Loss = 10.706018447875977, Accuracy = 0.857053279876709
Training iter #240000:   Batch Loss = 9.668121, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.68698501586914, Accuracy = 0.8626959323883057
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-600
Training iter #242000:   Batch Loss = 9.413420, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.5518798828125, Accuracy = 0.86771160364151
Training iter #244000:   Batch Loss = 9.573561, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.535725593566895, Accuracy = 0.8652037382125854
Training iter #246000:   Batch Loss = 9.815933, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.737897872924805, Accuracy = 0.8557993769645691
Training iter #248000:   Batch Loss = 9.830983, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.766307830810547, Accuracy = 0.8608150482177734
Training iter #250000:   Batch Loss = 9.557065, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.56262493133545, Accuracy = 0.8595611453056335
Training iter #252000:   Batch Loss = 9.459164, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.498488426208496, Accuracy = 0.8626959323883057
Training iter #254000:   Batch Loss = 9.432631, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.504827499389648, Accuracy = 0.8689655065536499
Training iter #256000:   Batch Loss = 9.282665, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.522231101989746, Accuracy = 0.8670846223831177
Training iter #258000:   Batch Loss = 9.702488, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.5564603805542, Accuracy = 0.8702194094657898
Training iter #260000:   Batch Loss = 9.643581, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.351237297058105, Accuracy = 0.8752350807189941
Training iter #262000:   Batch Loss = 9.254385, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.378478050231934, Accuracy = 0.8670846223831177
Training iter #264000:   Batch Loss = 8.791938, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.308631896972656, Accuracy = 0.8683385848999023
Training iter #266000:   Batch Loss = 9.154541, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.524505615234375, Accuracy = 0.8714733719825745
Training iter #268000:   Batch Loss = 9.390481, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.413457870483398, Accuracy = 0.8746081590652466
Training iter #270000:   Batch Loss = 8.858624, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.278402328491211, Accuracy = 0.86771160364151
Training iter #272000:   Batch Loss = 10.010052, Accuracy = 0.8793103694915771
PERFORMANCE ON TEST SET: Batch Loss = 10.294952392578125, Accuracy = 0.8758620619773865
Training iter #274000:   Batch Loss = 9.317730, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.98572063446045, Accuracy = 0.8739811778068542
Training iter #276000:   Batch Loss = 10.738979, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.481193542480469, Accuracy = 0.8702194094657898
Training iter #278000:   Batch Loss = 10.387020, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.393433570861816, Accuracy = 0.8746081590652466
Training iter #280000:   Batch Loss = 10.255797, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.38185977935791, Accuracy = 0.8727272748947144
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-700
Training iter #282000:   Batch Loss = 10.141620, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.11760425567627, Accuracy = 0.8727272748947144
Training iter #284000:   Batch Loss = 10.133477, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.939401626586914, Accuracy = 0.8739811778068542
Training iter #286000:   Batch Loss = 9.511011, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.791542053222656, Accuracy = 0.8777429461479187
Training iter #288000:   Batch Loss = 9.791079, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.042226791381836, Accuracy = 0.8802508115768433
Training iter #290000:   Batch Loss = 9.770382, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.774097442626953, Accuracy = 0.8714733719825745
Training iter #292000:   Batch Loss = 9.256155, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.870964050292969, Accuracy = 0.8739811778068542
Training iter #294000:   Batch Loss = 10.054062, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.954174995422363, Accuracy = 0.8670846223831177
Training iter #296000:   Batch Loss = 9.628738, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.745951652526855, Accuracy = 0.8752350807189941
Training iter #298000:   Batch Loss = 9.214803, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.698431015014648, Accuracy = 0.8695924878120422
Training iter #300000:   Batch Loss = 9.948027, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.131056785583496, Accuracy = 0.8658307194709778
Training iter #302000:   Batch Loss = 9.482697, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.949471473693848, Accuracy = 0.8645768165588379
Training iter #304000:   Batch Loss = 9.034534, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.802096366882324, Accuracy = 0.8652037382125854
Training iter #306000:   Batch Loss = 11.255434, Accuracy = 0.8620689511299133
PERFORMANCE ON TEST SET: Batch Loss = 10.761331558227539, Accuracy = 0.8695924878120422
Training iter #308000:   Batch Loss = 9.824590, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.922945976257324, Accuracy = 0.8645768165588379
Training iter #310000:   Batch Loss = 9.444991, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.976895332336426, Accuracy = 0.8658307194709778
Training iter #312000:   Batch Loss = 9.515831, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.743179321289062, Accuracy = 0.8739811778068542
Training iter #314000:   Batch Loss = 10.297853, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.22794246673584, Accuracy = 0.8476489186286926
Training iter #316000:   Batch Loss = 9.826572, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.876726150512695, Accuracy = 0.8695924878120422
Training iter #318000:   Batch Loss = 9.669641, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.767219543457031, Accuracy = 0.8695924878120422
Training iter #320000:   Batch Loss = 10.080263, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.687111854553223, Accuracy = 0.8658307194709778
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-800
Training iter #322000:   Batch Loss = 9.655737, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.888565063476562, Accuracy = 0.8796238303184509
Training iter #324000:   Batch Loss = 9.310823, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.762762069702148, Accuracy = 0.8746081590652466
Training iter #326000:   Batch Loss = 9.548695, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.723005294799805, Accuracy = 0.8695924878120422
Training iter #328000:   Batch Loss = 9.276912, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.92754077911377, Accuracy = 0.8746081590652466
Training iter #330000:   Batch Loss = 9.390910, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.735071182250977, Accuracy = 0.8708463907241821
Training iter #332000:   Batch Loss = 9.257060, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.717042922973633, Accuracy = 0.8689655065536499
Training iter #334000:   Batch Loss = 9.364929, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.88210391998291, Accuracy = 0.8733542561531067
Training iter #336000:   Batch Loss = 9.721935, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.853215217590332, Accuracy = 0.8589341640472412
Training iter #338000:   Batch Loss = 9.768203, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.926298141479492, Accuracy = 0.8608150482177734
Training iter #340000:   Batch Loss = 8.665979, Accuracy = 0.8965517282485962
PERFORMANCE ON TEST SET: Batch Loss = 10.701387405395508, Accuracy = 0.8708463907241821
Training iter #342000:   Batch Loss = 10.040483, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.290596008300781, Accuracy = 0.8796238303184509
Training iter #344000:   Batch Loss = 9.566040, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.956700325012207, Accuracy = 0.86771160364151
Training iter #346000:   Batch Loss = 9.344980, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.708277702331543, Accuracy = 0.8708463907241821
Training iter #348000:   Batch Loss = 9.375802, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.627853393554688, Accuracy = 0.8758620619773865
Training iter #350000:   Batch Loss = 9.024158, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.497441291809082, Accuracy = 0.8764890432357788
Training iter #352000:   Batch Loss = 9.316103, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.440593719482422, Accuracy = 0.8815047144889832
Training iter #354000:   Batch Loss = 9.003508, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.487886428833008, Accuracy = 0.8796238303184509
Training iter #356000:   Batch Loss = 9.183002, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.487943649291992, Accuracy = 0.8802508115768433
Training iter #358000:   Batch Loss = 9.502060, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.5535306930542, Accuracy = 0.8771159648895264
Training iter #360000:   Batch Loss = 9.200794, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.47374153137207, Accuracy = 0.8821316361427307
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-900
Training iter #362000:   Batch Loss = 9.390536, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.505176544189453, Accuracy = 0.8833855986595154
Training iter #364000:   Batch Loss = 8.721797, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.330034255981445, Accuracy = 0.8808777332305908
Training iter #366000:   Batch Loss = 8.947121, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.306743621826172, Accuracy = 0.8821316361427307
Training iter #368000:   Batch Loss = 9.045578, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.458261489868164, Accuracy = 0.8746081590652466
Training iter #370000:   Batch Loss = 8.889993, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.227489471435547, Accuracy = 0.8877742886543274
Training iter #372000:   Batch Loss = 8.865847, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.318074226379395, Accuracy = 0.8815047144889832
Training iter #374000:   Batch Loss = 7.545419, Accuracy = 0.8793103694915771
PERFORMANCE ON TEST SET: Batch Loss = 10.283082962036133, Accuracy = 0.8840125203132629
Training iter #376000:   Batch Loss = 8.722193, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.149275779724121, Accuracy = 0.8915360569953918
Training iter #378000:   Batch Loss = 9.080992, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.125970840454102, Accuracy = 0.8796238303184509
Training iter #380000:   Batch Loss = 8.682994, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.150849342346191, Accuracy = 0.8777429461479187
Training iter #382000:   Batch Loss = 8.903466, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.98572826385498, Accuracy = 0.8758620619773865
Training iter #384000:   Batch Loss = 9.888295, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.904289245605469, Accuracy = 0.8871473073959351
Training iter #386000:   Batch Loss = 9.480749, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.905750274658203, Accuracy = 0.8852664828300476
Training iter #388000:   Batch Loss = 9.108799, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.73635482788086, Accuracy = 0.8890281915664673
Training iter #390000:   Batch Loss = 9.319628, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.93349838256836, Accuracy = 0.8789968490600586
Training iter #392000:   Batch Loss = 9.304939, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.666915893554688, Accuracy = 0.8846395015716553
Training iter #394000:   Batch Loss = 9.356920, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.472335815429688, Accuracy = 0.8840125203132629
Training iter #396000:   Batch Loss = 9.096606, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.58927059173584, Accuracy = 0.8871473073959351
Training iter #398000:   Batch Loss = 8.849587, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.519482612609863, Accuracy = 0.8896551728248596
Training iter #400000:   Batch Loss = 9.278316, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.486176490783691, Accuracy = 0.8852664828300476
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1000
Training iter #402000:   Batch Loss = 8.879532, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.697214126586914, Accuracy = 0.8808777332305908
Training iter #404000:   Batch Loss = 9.289547, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.463668823242188, Accuracy = 0.8802508115768433
Training iter #406000:   Batch Loss = 9.212527, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.518899917602539, Accuracy = 0.8802508115768433
Training iter #408000:   Batch Loss = 8.268263, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 10.606106758117676, Accuracy = 0.8815047144889832
Training iter #410000:   Batch Loss = 8.872484, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.965932846069336, Accuracy = 0.8865203857421875
Training iter #412000:   Batch Loss = 9.871446, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.195852279663086, Accuracy = 0.882758617401123
Training iter #414000:   Batch Loss = 10.094619, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.157413482666016, Accuracy = 0.8808777332305908
Training iter #416000:   Batch Loss = 10.486567, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.853172302246094, Accuracy = 0.8915360569953918
Training iter #418000:   Batch Loss = 9.274330, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.76450252532959, Accuracy = 0.8846395015716553
Training iter #420000:   Batch Loss = 9.306200, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.781685829162598, Accuracy = 0.8896551728248596
Training iter #422000:   Batch Loss = 9.569167, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.783536911010742, Accuracy = 0.8865203857421875
Training iter #424000:   Batch Loss = 8.732749, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.487802505493164, Accuracy = 0.8858934044837952
Training iter #426000:   Batch Loss = 9.086417, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.431045532226562, Accuracy = 0.8871473073959351
Training iter #428000:   Batch Loss = 8.726902, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.307999610900879, Accuracy = 0.8865203857421875
Training iter #430000:   Batch Loss = 8.990974, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.266256332397461, Accuracy = 0.8915360569953918
Training iter #432000:   Batch Loss = 8.911101, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.152099609375, Accuracy = 0.8934169411659241
Training iter #434000:   Batch Loss = 8.671564, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.314550399780273, Accuracy = 0.8909090757369995
Training iter #436000:   Batch Loss = 9.127723, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.367280960083008, Accuracy = 0.8877742886543274
Training iter #438000:   Batch Loss = 8.896381, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.329989433288574, Accuracy = 0.8909090757369995
Training iter #440000:   Batch Loss = 8.871029, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.280659675598145, Accuracy = 0.8921630382537842
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1100
Training iter #442000:   Batch Loss = 9.447617, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 11.687914848327637, Accuracy = 0.8846395015716553
Training iter #444000:   Batch Loss = 9.835279, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.288848876953125, Accuracy = 0.8927899599075317
Training iter #446000:   Batch Loss = 10.294041, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.259408950805664, Accuracy = 0.8877742886543274
Training iter #448000:   Batch Loss = 9.965807, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.981425285339355, Accuracy = 0.8896551728248596
Training iter #450000:   Batch Loss = 9.834749, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.420547485351562, Accuracy = 0.8858934044837952
Training iter #452000:   Batch Loss = 9.790195, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.125255584716797, Accuracy = 0.8865203857421875
Training iter #454000:   Batch Loss = 9.618212, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.97204875946045, Accuracy = 0.8877742886543274
Training iter #456000:   Batch Loss = 9.600267, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.98614501953125, Accuracy = 0.8890281915664673
Training iter #458000:   Batch Loss = 9.552044, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.857529640197754, Accuracy = 0.8852664828300476
Training iter #460000:   Batch Loss = 9.491169, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.744858741760254, Accuracy = 0.8858934044837952
Training iter #462000:   Batch Loss = 8.916680, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.773090362548828, Accuracy = 0.8890281915664673
Training iter #464000:   Batch Loss = 9.170067, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.614728927612305, Accuracy = 0.8858934044837952
Training iter #466000:   Batch Loss = 9.051015, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.496826171875, Accuracy = 0.8940438628196716
Training iter #468000:   Batch Loss = 9.256162, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.47554874420166, Accuracy = 0.8940438628196716
Training iter #470000:   Batch Loss = 8.865042, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.624754905700684, Accuracy = 0.8877742886543274
Training iter #472000:   Batch Loss = 8.770012, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.574397087097168, Accuracy = 0.8821316361427307
Training iter #474000:   Batch Loss = 8.975920, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.562980651855469, Accuracy = 0.882758617401123
Training iter #476000:   Batch Loss = 9.057837, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.622795104980469, Accuracy = 0.8915360569953918
Training iter #478000:   Batch Loss = 9.838694, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.98090648651123, Accuracy = 0.8871473073959351
Training iter #480000:   Batch Loss = 8.816311, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.69396686553955, Accuracy = 0.8927899599075317
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1200
Training iter #482000:   Batch Loss = 8.735042, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.60078239440918, Accuracy = 0.8884012699127197
Training iter #484000:   Batch Loss = 9.150710, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.752294540405273, Accuracy = 0.882758617401123
Training iter #486000:   Batch Loss = 8.867112, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.437199592590332, Accuracy = 0.8884012699127197WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.

Training iter #488000:   Batch Loss = 8.812999, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.506808280944824, Accuracy = 0.8896551728248596
Training iter #490000:   Batch Loss = 8.730755, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.440603256225586, Accuracy = 0.8840125203132629
Training iter #492000:   Batch Loss = 8.951380, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.613541603088379, Accuracy = 0.8877742886543274
Training iter #494000:   Batch Loss = 9.392302, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.2711763381958, Accuracy = 0.8884012699127197
Training iter #496000:   Batch Loss = 9.085647, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.353645324707031, Accuracy = 0.8984326124191284
Training iter #498000:   Batch Loss = 8.328278, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.177692413330078, Accuracy = 0.8877742886543274
Training iter #500000:   Batch Loss = 8.674279, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.028412818908691, Accuracy = 0.894670844078064
Training iter #502000:   Batch Loss = 9.552346, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.224981307983398, Accuracy = 0.8952978253364563
Training iter #504000:   Batch Loss = 8.789206, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.210746765136719, Accuracy = 0.8971787095069885
Training iter #506000:   Batch Loss = 8.611362, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.345327377319336, Accuracy = 0.894670844078064
Training iter #508000:   Batch Loss = 8.345016, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.29382610321045, Accuracy = 0.8952978253364563
Training iter #510000:   Batch Loss = 8.281227, Accuracy = 0.9137930870056152
PERFORMANCE ON TEST SET: Batch Loss = 10.337849617004395, Accuracy = 0.8896551728248596
Training iter #512000:   Batch Loss = 8.491887, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.218064308166504, Accuracy = 0.8927899599075317
Training iter #514000:   Batch Loss = 8.199704, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.465703964233398, Accuracy = 0.894670844078064
Training iter #516000:   Batch Loss = 8.673415, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.245772361755371, Accuracy = 0.894670844078064
Training iter #518000:   Batch Loss = 9.240217, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.597770690917969, Accuracy = 0.8952978253364563
Training iter #520000:   Batch Loss = 9.503559, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.971063613891602, Accuracy = 0.8833855986595154
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1300
Training iter #522000:   Batch Loss = 9.490273, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.43022632598877, Accuracy = 0.8952978253364563
Training iter #524000:   Batch Loss = 9.609070, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.882669448852539, Accuracy = 0.890282154083252
Training iter #526000:   Batch Loss = 9.494005, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.610054016113281, Accuracy = 0.8852664828300476
Training iter #528000:   Batch Loss = 9.263590, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.438592910766602, Accuracy = 0.8959247469902039
Training iter #530000:   Batch Loss = 8.696974, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.373310089111328, Accuracy = 0.8846395015716553
Training iter #532000:   Batch Loss = 8.813420, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.330808639526367, Accuracy = 0.8865203857421875
Training iter #534000:   Batch Loss = 9.118883, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.291898727416992, Accuracy = 0.8921630382537842
Training iter #536000:   Batch Loss = 8.429039, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.330583572387695, Accuracy = 0.8877742886543274
Training iter #538000:   Batch Loss = 8.793247, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.182565689086914, Accuracy = 0.8934169411659241
Training iter #540000:   Batch Loss = 8.829426, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.166135787963867, Accuracy = 0.8865203857421875
Training iter #542000:   Batch Loss = 9.163646, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.291274070739746, Accuracy = 0.899059534072876
Training iter #544000:   Batch Loss = 9.601954, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.80021858215332, Accuracy = 0.8890281915664673
Training iter #546000:   Batch Loss = 9.305130, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.04948616027832, Accuracy = 0.8796238303184509
Training iter #548000:   Batch Loss = 9.620993, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.762690544128418, Accuracy = 0.8808777332305908
Training iter #550000:   Batch Loss = 9.175567, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.508746147155762, Accuracy = 0.8890281915664673
Training iter #552000:   Batch Loss = 9.368546, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.750415802001953, Accuracy = 0.894670844078064
Training iter #554000:   Batch Loss = 9.405844, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.480060577392578, Accuracy = 0.8978056311607361
Training iter #556000:   Batch Loss = 8.757100, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.230972290039062, Accuracy = 0.8915360569953918
Training iter #558000:   Batch Loss = 8.823343, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.497962951660156, Accuracy = 0.8877742886543274
Training iter #560000:   Batch Loss = 9.103770, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.199162483215332, Accuracy = 0.8971787095069885
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1400
Training iter #562000:   Batch Loss = 8.809782, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.3353853225708, Accuracy = 0.8927899599075317
Training iter #564000:   Batch Loss = 8.826333, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.016390800476074, Accuracy = 0.8940438628196716
Training iter #566000:   Batch Loss = 8.707710, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.09717845916748, Accuracy = 0.8909090757369995
Training iter #568000:   Batch Loss = 8.354197, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.019170761108398, Accuracy = 0.8984326124191284
Training iter #570000:   Batch Loss = 8.681943, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.081219673156738, Accuracy = 0.8971787095069885
Training iter #572000:   Batch Loss = 9.383224, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.691781997680664, Accuracy = 0.8984326124191284
Training iter #574000:   Batch Loss = 9.093684, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.672809600830078, Accuracy = 0.9034482836723328
Training iter #576000:   Batch Loss = 9.068035, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.563763618469238, Accuracy = 0.8978056311607361
Training iter #578000:   Batch Loss = 8.950056, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.715232849121094, Accuracy = 0.905329167842865
Training iter #580000:   Batch Loss = 9.642375, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.935656547546387, Accuracy = 0.894670844078064
Training iter #582000:   Batch Loss = 8.618656, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.362004280090332, Accuracy = 0.9003134965896606
Training iter #584000:   Batch Loss = 9.295938, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.0781831741333, Accuracy = 0.8984326124191284
Training iter #586000:   Batch Loss = 9.636576, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.87946605682373, Accuracy = 0.8846395015716553
Training iter #588000:   Batch Loss = 9.121420, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.833855628967285, Accuracy = 0.8934169411659241
Training iter #590000:   Batch Loss = 9.239322, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.150213241577148, Accuracy = 0.8877742886543274
Training iter #592000:   Batch Loss = 8.974512, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.08666706085205, Accuracy = 0.8846395015716553
Training iter #594000:   Batch Loss = 9.581492, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.857090950012207, Accuracy = 0.8884012699127197
Training iter #596000:   Batch Loss = 9.640696, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.92003059387207, Accuracy = 0.8915360569953918
Training iter #598000:   Batch Loss = 8.940274, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.779942512512207, Accuracy = 0.8865203857421875
Training iter #600000:   Batch Loss = 8.967882, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.73483657836914, Accuracy = 0.8884012699127197
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1500
Training iter #602000:   Batch Loss = 8.788731, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.40068531036377, Accuracy = 0.8927899599075317
Training iter #604000:   Batch Loss = 9.087883, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.34006404876709, Accuracy = 0.8965517282485962
Training iter #606000:   Batch Loss = 9.467350, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.486776351928711, Accuracy = 0.8821316361427307
Training iter #608000:   Batch Loss = 8.686932, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.375314712524414, Accuracy = 0.8959247469902039
Training iter #610000:   Batch Loss = 8.743501, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.21843147277832, Accuracy = 0.8996865153312683
Training iter #612000:   Batch Loss = 8.932884, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.512377738952637, Accuracy = 0.8971787095069885
Training iter #614000:   Batch Loss = 9.639275, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.041097640991211, Accuracy = 0.8877742886543274
Training iter #616000:   Batch Loss = 9.159823, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.912531852722168, Accuracy = 0.8877742886543274
Training iter #618000:   Batch Loss = 9.039109, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.595029830932617, Accuracy = 0.8915360569953918
Training iter #620000:   Batch Loss = 8.512687, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.590280532836914, Accuracy = 0.8952978253364563
Training iter #622000:   Batch Loss = 8.716652, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.324007034301758, Accuracy = 0.899059534072876
Training iter #624000:   Batch Loss = 8.941401, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.207347869873047, Accuracy = 0.899059534072876
Training iter #626000:   Batch Loss = 8.684178, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.716670989990234, Accuracy = 0.8846395015716553
Training iter #628000:   Batch Loss = 8.810278, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.64285659790039, Accuracy = 0.8952978253364563
Training iter #630000:   Batch Loss = 8.695175, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.639257431030273, Accuracy = 0.894670844078064
Training iter #632000:   Batch Loss = 8.914837, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.885658264160156, Accuracy = 0.8846395015716553
Training iter #634000:   Batch Loss = 9.240709, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.835176467895508, Accuracy = 0.8852664828300476
Training iter #636000:   Batch Loss = 9.374854, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.672303199768066, Accuracy = 0.8934169411659241
Training iter #638000:   Batch Loss = 8.907409, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.969161987304688, Accuracy = 0.8965517282485962
Training iter #640000:   Batch Loss = 9.616074, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.042988777160645, Accuracy = 0.899059534072876
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1600
Training iter #642000:   Batch Loss = 9.918554, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.829375267028809, Accuracy = 0.8771159648895264
Training iter #644000:   Batch Loss = 9.572007, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.066780090332031, Accuracy = 0.8921630382537842
Training iter #646000:   Batch Loss = 8.468477, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 11.0626802444458, Accuracy = 0.8934169411659241
Training iter #648000:   Batch Loss = 9.354319, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.987205505371094, Accuracy = 0.8896551728248596
Training iter #650000:   Batch Loss = 9.254173, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.683259963989258, Accuracy = 0.894670844078064
Training iter #652000:   Batch Loss = 9.840459, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.364749908447266, Accuracy = 0.8971787095069885
Training iter #654000:   Batch Loss = 10.347310, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.02607536315918, Accuracy = 0.8752350807189941
Training iter #656000:   Batch Loss = 10.141554, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.59334945678711, Accuracy = 0.8871473073959351
Training iter #658000:   Batch Loss = 10.323323, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.349595069885254, Accuracy = 0.8877742886543274
Training iter #660000:   Batch Loss = 9.434155, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.164373397827148, Accuracy = 0.8940438628196716
Training iter #662000:   Batch Loss = 9.384195, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.134000778198242, Accuracy = 0.8884012699127197
Training iter #664000:   Batch Loss = 9.755834, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.074833869934082, Accuracy = 0.8852664828300476
Training iter #666000:   Batch Loss = 9.428040, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.833959579467773, Accuracy = 0.890282154083252
Training iter #668000:   Batch Loss = 9.312486, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.778250694274902, Accuracy = 0.8909090757369995
Training iter #670000:   Batch Loss = 9.171080, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.823873519897461, Accuracy = 0.8909090757369995
Training iter #672000:   Batch Loss = 9.051050, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.735029220581055, Accuracy = 0.8959247469902039
Training iter #674000:   Batch Loss = 8.752174, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.840022087097168, Accuracy = 0.8952978253364563
Training iter #676000:   Batch Loss = 9.195087, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.845219612121582, Accuracy = 0.9040752053260803
Training iter #678000:   Batch Loss = 9.208755, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.580745697021484, Accuracy = 0.8921630382537842
Training iter #680000:   Batch Loss = 9.292810, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.648483276367188, Accuracy = 0.8940438628196716
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1700
Training iter #682000:   Batch Loss = 9.068471, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.424983978271484, Accuracy = 0.8984326124191284
Training iter #684000:   Batch Loss = 9.202272, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.506887435913086, Accuracy = 0.8852664828300476
Training iter #686000:   Batch Loss = 9.149722, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.535039901733398, Accuracy = 0.9078369736671448
Training iter #688000:   Batch Loss = 8.681530, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.598834991455078, Accuracy = 0.8996865153312683
Training iter #690000:   Batch Loss = 8.898318, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.529252052307129, Accuracy = 0.8965517282485962
Training iter #692000:   Batch Loss = 8.920659, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.364104270935059, Accuracy = 0.9040752053260803
Training iter #694000:   Batch Loss = 8.341118, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.412229537963867, Accuracy = 0.9040752053260803
Training iter #696000:   Batch Loss = 8.776226, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.319473266601562, Accuracy = 0.9034482836723328
Training iter #698000:   Batch Loss = 8.945734, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.581518173217773, Accuracy = 0.8940438628196716
Training iter #700000:   Batch Loss = 8.476126, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.280915260314941, Accuracy = 0.9021943807601929
Training iter #702000:   Batch Loss = 8.432983, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.511344909667969, Accuracy = 0.9003134965896606
Training iter #704000:   Batch Loss = 9.129396, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.55143928527832, Accuracy = 0.8996865153312683
Training iter #706000:   Batch Loss = 8.770103, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.537843704223633, Accuracy = 0.8934169411659241
Training iter #708000:   Batch Loss = 8.887754, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.659331321716309, Accuracy = 0.8934169411659241
Training iter #710000:   Batch Loss = 8.768844, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.384946823120117, Accuracy = 0.8971787095069885
Training iter #712000:   Batch Loss = 8.661924, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.177021026611328, Accuracy = 0.9015673995018005
Training iter #714000:   Batch Loss = 8.624929, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.530157089233398, Accuracy = 0.9015673995018005
Training iter #716000:   Batch Loss = 9.038456, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.825782775878906, Accuracy = 0.899059534072876
Training iter #718000:   Batch Loss = 8.748133, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.536152839660645, Accuracy = 0.899059534072876
Training iter #720000:   Batch Loss = 9.302062, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.402125358581543, Accuracy = 0.8940438628196716
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1800
Training iter #722000:   Batch Loss = 8.324667, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.433300971984863, Accuracy = 0.8896551728248596
Training iter #724000:   Batch Loss = 9.050325, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.290584564208984, Accuracy = 0.8984326124191284
Training iter #726000:   Batch Loss = 8.697088, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.286653518676758, Accuracy = 0.8996865153312683
Training iter #728000:   Batch Loss = 8.586831, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.532008171081543, Accuracy = 0.8890281915664673
Training iter #730000:   Batch Loss = 8.613619, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.211045265197754, Accuracy = 0.8940438628196716
Training iter #732000:   Batch Loss = 8.655301, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.223575592041016, Accuracy = 0.8978056311607361
Training iter #734000:   Batch Loss = 8.306776, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.225409507751465, Accuracy = 0.8965517282485962
Training iter #736000:   Batch Loss = 8.587193, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.229467391967773, Accuracy = 0.8952978253364563
Training iter #738000:   Batch Loss = 8.632847, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.162190437316895, Accuracy = 0.8978056311607361
Training iter #740000:   Batch Loss = 8.630265, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.304622650146484, Accuracy = 0.9047021865844727
Training iter #742000:   Batch Loss = 8.475445, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.33460807800293, Accuracy = 0.9034482836723328
Training iter #744000:   Batch Loss = 8.867684, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.166078567504883, Accuracy = 0.899059534072876
Training iter #746000:   Batch Loss = 8.743997, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.259099006652832, Accuracy = 0.9028213024139404
Training iter #748000:   Batch Loss = 9.035302, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.479571342468262, Accuracy = 0.8959247469902039
Training iter #750000:   Batch Loss = 8.648455, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.428468704223633, Accuracy = 0.8952978253364563
Training iter #752000:   Batch Loss = 8.656240, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.377653121948242, Accuracy = 0.8921630382537842
Training iter #754000:   Batch Loss = 8.786436, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.234637260437012, Accuracy = 0.9028213024139404
Training iter #756000:   Batch Loss = 8.848328, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.224993705749512, Accuracy = 0.9065830707550049
Training iter #758000:   Batch Loss = 8.373233, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.149553298950195, Accuracy = 0.8984326124191284
Training iter #760000:   Batch Loss = 8.229445, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.16402816772461, Accuracy = 0.8984326124191284
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-1900
Training iter #762000:   Batch Loss = 8.363679, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.015874862670898, Accuracy = 0.9015673995018005
Training iter #764000:   Batch Loss = 8.506031, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.098943710327148, Accuracy = 0.9034482836723328
Training iter #766000:   Batch Loss = 8.496446, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.3419189453125, Accuracy = 0.9028213024139404
Training iter #768000:   Batch Loss = 8.361137, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.111794471740723, Accuracy = 0.9059560894966125
Training iter #770000:   Batch Loss = 8.462957, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.360105514526367, Accuracy = 0.9028213024139404
Training iter #772000:   Batch Loss = 8.474276, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.096881866455078, Accuracy = 0.9047021865844727
Training iter #774000:   Batch Loss = 8.745584, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.081945419311523, Accuracy = 0.8996865153312683
Training iter #776000:   Batch Loss = 8.130154, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.002509117126465, Accuracy = 0.9040752053260803
Training iter #778000:   Batch Loss = 8.520106, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.916568756103516, Accuracy = 0.9021943807601929
Training iter #780000:   Batch Loss = 8.471699, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 9.954679489135742, Accuracy = 0.8996865153312683
Training iter #782000:   Batch Loss = 7.930495, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.402383804321289, Accuracy = 0.9015673995018005
Training iter #784000:   Batch Loss = 8.478334, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.216981887817383, Accuracy = 0.8996865153312683
Training iter #786000:   Batch Loss = 8.406118, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.142522811889648, Accuracy = 0.8996865153312683
Training iter #788000:   Batch Loss = 8.133284, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.109773635864258, Accuracy = 0.9034482836723328
Training iter #790000:   Batch Loss = 8.128994, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.105688095092773, Accuracy = 0.8978056311607361
Training iter #792000:   Batch Loss = 10.208747, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.737772941589355, Accuracy = 0.8858934044837952
Training iter #794000:   Batch Loss = 10.627800, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.338455200195312, Accuracy = 0.8959247469902039
Training iter #796000:   Batch Loss = 9.390796, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.331727027893066, Accuracy = 0.8978056311607361
Training iter #798000:   Batch Loss = 8.837567, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.903644561767578, Accuracy = 0.9040752053260803
Training iter #800000:   Batch Loss = 9.209217, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.820037841796875, Accuracy = 0.9059560894966125
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2000
Training iter #802000:   Batch Loss = 8.781289, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.633772850036621, Accuracy = 0.9084639549255371
Training iter #804000:   Batch Loss = 8.813150, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.54017162322998, Accuracy = 0.905329167842865
Training iter #806000:   Batch Loss = 8.485642, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.594026565551758, Accuracy = 0.8996865153312683
Training iter #808000:   Batch Loss = 8.715347, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.483293533325195, Accuracy = 0.9034482836723328
Training iter #810000:   Batch Loss = 8.829536, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.48190975189209, Accuracy = 0.9103448390960693
Training iter #812000:   Batch Loss = 8.637763, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.803037643432617, Accuracy = 0.9009404182434082
Training iter #814000:   Batch Loss = 9.757633, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.99380874633789, Accuracy = 0.8978056311607361
Training iter #816000:   Batch Loss = 10.081791, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 10.91830825805664, Accuracy = 0.9028213024139404
Training iter #818000:   Batch Loss = 9.365659, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.65747356414795, Accuracy = 0.9034482836723328
Training iter #820000:   Batch Loss = 8.612576, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.752861022949219, Accuracy = 0.9015673995018005
Training iter #822000:   Batch Loss = 8.964542, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.604071617126465, Accuracy = 0.9040752053260803
Training iter #824000:   Batch Loss = 8.695806, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.518439292907715, Accuracy = 0.9040752053260803
Training iter #826000:   Batch Loss = 8.859617, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.633395195007324, Accuracy = 0.9059560894966125
Training iter #828000:   Batch Loss = 9.109722, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.393126487731934, Accuracy = 0.905329167842865
Training iter #830000:   Batch Loss = 8.697738, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.755996704101562, Accuracy = 0.899059534072876
Training iter #832000:   Batch Loss = 8.907530, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.81361198425293, Accuracy = 0.8984326124191284
Training iter #834000:   Batch Loss = 8.660745, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.648811340332031, Accuracy = 0.9040752053260803
Training iter #836000:   Batch Loss = 9.145505, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.64028549194336, Accuracy = 0.8833855986595154
Training iter #838000:   Batch Loss = 8.536579, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.399042129516602, Accuracy = 0.8984326124191284
Training iter #840000:   Batch Loss = 8.603041, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.41150188446045, Accuracy = 0.8971787095069885
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2100
Training iter #842000:   Batch Loss = 9.095168, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.822782516479492, Accuracy = 0.9015673995018005
Training iter #844000:   Batch Loss = 9.998678, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.417200088500977, Accuracy = 0.882758617401123
Training iter #846000:   Batch Loss = 9.414263, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.071732521057129, Accuracy = 0.8927899599075317
Training iter #848000:   Batch Loss = 9.457731, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.880151748657227, Accuracy = 0.8909090757369995
Training iter #850000:   Batch Loss = 9.659966, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.96106243133545, Accuracy = 0.8971787095069885
Training iter #852000:   Batch Loss = 10.831530, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.279369354248047, Accuracy = 0.8683385848999023
Training iter #854000:   Batch Loss = 10.295238, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.659709930419922, Accuracy = 0.8884012699127197
Training iter #856000:   Batch Loss = 9.475412, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.2298583984375, Accuracy = 0.8927899599075317
Training iter #858000:   Batch Loss = 10.380396, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.432741165161133, Accuracy = 0.882758617401123
Training iter #860000:   Batch Loss = 9.806507, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.367419242858887, Accuracy = 0.8896551728248596
Training iter #862000:   Batch Loss = 9.787941, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.400116920471191, Accuracy = 0.8884012699127197
Training iter #864000:   Batch Loss = 9.159548, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.317840576171875, Accuracy = 0.8752350807189941
Training iter #866000:   Batch Loss = 9.671561, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.08004093170166, Accuracy = 0.8884012699127197
Training iter #868000:   Batch Loss = 9.527361, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.784554481506348, Accuracy = 0.894670844078064
Training iter #870000:   Batch Loss = 8.895576, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.633028030395508, Accuracy = 0.894670844078064
Training iter #872000:   Batch Loss = 9.035624, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.60771369934082, Accuracy = 0.8934169411659241
Training iter #874000:   Batch Loss = 8.863686, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.589698791503906, Accuracy = 0.9028213024139404
Training iter #876000:   Batch Loss = 8.925134, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.515424728393555, Accuracy = 0.9015673995018005
Training iter #878000:   Batch Loss = 11.297393, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.133110046386719, Accuracy = 0.8921630382537842
Training iter #880000:   Batch Loss = 10.648590, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.464250564575195, Accuracy = 0.8915360569953918
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2200
Training iter #882000:   Batch Loss = 10.989595, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.425813674926758, Accuracy = 0.8921630382537842
Training iter #884000:   Batch Loss = 10.342352, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 12.101119995117188, Accuracy = 0.8984326124191284
Training iter #886000:   Batch Loss = 11.006392, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.264116287231445, Accuracy = 0.8871473073959351
Training iter #888000:   Batch Loss = 10.582335, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.246658325195312, Accuracy = 0.8871473073959351
Training iter #890000:   Batch Loss = 10.278382, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.060973167419434, Accuracy = 0.8934169411659241
Training iter #892000:   Batch Loss = 9.731965, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.047822952270508, Accuracy = 0.8909090757369995
Training iter #894000:   Batch Loss = 10.274661, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.787495613098145, Accuracy = 0.8833855986595154
Training iter #896000:   Batch Loss = 9.733818, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.698835372924805, Accuracy = 0.8896551728248596
Training iter #898000:   Batch Loss = 9.894175, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.984691619873047, Accuracy = 0.8852664828300476
Training iter #900000:   Batch Loss = 10.510760, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.564737319946289, Accuracy = 0.8940438628196716
Training iter #902000:   Batch Loss = 9.819440, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.699766159057617, Accuracy = 0.8852664828300476
Training iter #904000:   Batch Loss = 9.719528, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.486945152282715, Accuracy = 0.8959247469902039
Training iter #906000:   Batch Loss = 9.655618, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.468999862670898, Accuracy = 0.9003134965896606
Training iter #908000:   Batch Loss = 9.174364, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.360562324523926, Accuracy = 0.9021943807601929
Training iter #910000:   Batch Loss = 10.024054, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.635327339172363, Accuracy = 0.8996865153312683
Training iter #912000:   Batch Loss = 10.618372, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.924911499023438, Accuracy = 0.8890281915664673
Training iter #914000:   Batch Loss = 10.483566, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.103839874267578, Accuracy = 0.9003134965896606
Training iter #916000:   Batch Loss = 9.724562, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.667978286743164, Accuracy = 0.9028213024139404
Training iter #918000:   Batch Loss = 9.654497, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 11.782759666442871, Accuracy = 0.9015673995018005
Training iter #920000:   Batch Loss = 9.708164, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.654226303100586, Accuracy = 0.8909090757369995
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2300
Training iter #922000:   Batch Loss = 9.500391, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.465651512145996, Accuracy = 0.9034482836723328
Training iter #924000:   Batch Loss = 9.796698, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.44564437866211, Accuracy = 0.899059534072876
Training iter #926000:   Batch Loss = 9.169835, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.332770347595215, Accuracy = 0.9059560894966125
Training iter #928000:   Batch Loss = 9.382987, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.141517639160156, Accuracy = 0.9028213024139404
Training iter #930000:   Batch Loss = 9.245739, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.211639404296875, Accuracy = 0.8984326124191284
Training iter #932000:   Batch Loss = 9.655398, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.955706596374512, Accuracy = 0.899059534072876
Training iter #934000:   Batch Loss = 9.345407, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.501814842224121, Accuracy = 0.9028213024139404
Training iter #936000:   Batch Loss = 9.954980, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.550481796264648, Accuracy = 0.8965517282485962
Training iter #938000:   Batch Loss = 9.697214, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.666736602783203, Accuracy = 0.8921630382537842
Training iter #940000:   Batch Loss = 9.618311, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.659842491149902, Accuracy = 0.894670844078064
Training iter #942000:   Batch Loss = 9.608624, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.53139591217041, Accuracy = 0.8877742886543274
Training iter #944000:   Batch Loss = 9.536137, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.363357543945312, Accuracy = 0.8909090757369995
Training iter #946000:   Batch Loss = 9.867762, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.902654647827148, Accuracy = 0.8840125203132629
Training iter #948000:   Batch Loss = 10.364810, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.868343353271484, Accuracy = 0.882758617401123
Training iter #950000:   Batch Loss = 10.330325, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.716242790222168, Accuracy = 0.8865203857421875
Training iter #952000:   Batch Loss = 10.067024, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 11.696402549743652, Accuracy = 0.8921630382537842
Training iter #954000:   Batch Loss = 10.041137, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.454675674438477, Accuracy = 0.9034482836723328
Training iter #956000:   Batch Loss = 9.658798, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.469928741455078, Accuracy = 0.8940438628196716
Training iter #958000:   Batch Loss = 9.896155, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.553459167480469, Accuracy = 0.8858934044837952
Training iter #960000:   Batch Loss = 9.635699, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.473191261291504, Accuracy = 0.890282154083252
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2400
Training iter #962000:   Batch Loss = 9.986440, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.378462791442871, Accuracy = 0.8884012699127197
Training iter #964000:   Batch Loss = 9.891594, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.677421569824219, Accuracy = 0.890282154083252
Training iter #966000:   Batch Loss = 9.935064, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.229528427124023, Accuracy = 0.894670844078064
Training iter #968000:   Batch Loss = 10.079259, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.039979934692383, Accuracy = 0.8909090757369995
Training iter #970000:   Batch Loss = 10.736479, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.777708053588867, Accuracy = 0.8921630382537842
Training iter #972000:   Batch Loss = 10.204477, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.77560806274414, Accuracy = 0.8865203857421875
Training iter #974000:   Batch Loss = 9.432678, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.907327651977539, Accuracy = 0.8971787095069885
Training iter #976000:   Batch Loss = 10.068451, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.647374153137207, Accuracy = 0.8984326124191284
Training iter #978000:   Batch Loss = 9.732105, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.797760009765625, Accuracy = 0.8959247469902039
Training iter #980000:   Batch Loss = 10.125821, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.774559020996094, Accuracy = 0.8840125203132629
Training iter #982000:   Batch Loss = 9.791975, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.488195419311523, Accuracy = 0.890282154083252
Training iter #984000:   Batch Loss = 10.273485, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.617681503295898, Accuracy = 0.8890281915664673
Training iter #986000:   Batch Loss = 9.254881, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 11.840351104736328, Accuracy = 0.8890281915664673
Training iter #988000:   Batch Loss = 10.995667, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.569278717041016, Accuracy = 0.8871473073959351
Training iter #990000:   Batch Loss = 10.370273, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.094099998474121, Accuracy = 0.8877742886543274
Training iter #992000:   Batch Loss = 10.123681, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.899945259094238, Accuracy = 0.8846395015716553
Training iter #994000:   Batch Loss = 10.307285, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.915103912353516, Accuracy = 0.8877742886543274
Training iter #996000:   Batch Loss = 9.846391, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.806242942810059, Accuracy = 0.894670844078064
Training iter #998000:   Batch Loss = 10.568791, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.93378734588623, Accuracy = 0.8884012699127197
Training iter #1000000:   Batch Loss = 10.100394, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.778149604797363, Accuracy = 0.8877742886543274
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2500
Training iter #1002000:   Batch Loss = 9.866622, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.657543182373047, Accuracy = 0.8934169411659241
Training iter #1004000:   Batch Loss = 9.588688, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.470620155334473, Accuracy = 0.8909090757369995
Training iter #1006000:   Batch Loss = 10.190962, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.470808029174805, Accuracy = 0.894670844078064
Training iter #1008000:   Batch Loss = 9.903304, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.551389694213867, Accuracy = 0.8865203857421875
Training iter #1010000:   Batch Loss = 10.216299, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.104454040527344, Accuracy = 0.8777429461479187
Training iter #1012000:   Batch Loss = 9.972292, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.588802337646484, Accuracy = 0.890282154083252
Training iter #1014000:   Batch Loss = 10.115079, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.68851089477539, Accuracy = 0.8852664828300476
Training iter #1016000:   Batch Loss = 9.989617, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.416295051574707, Accuracy = 0.8871473073959351
Training iter #1018000:   Batch Loss = 9.770931, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.338401794433594, Accuracy = 0.8865203857421875
Training iter #1020000:   Batch Loss = 10.556324, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 11.231689453125, Accuracy = 0.8927899599075317
Training iter #1022000:   Batch Loss = 10.158558, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.964654922485352, Accuracy = 0.8890281915664673
Training iter #1024000:   Batch Loss = 10.494777, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.770723342895508, Accuracy = 0.8940438628196716
Training iter #1026000:   Batch Loss = 10.504532, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.910167694091797, Accuracy = 0.8833855986595154
Training iter #1028000:   Batch Loss = 11.790397, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.807035446166992, Accuracy = 0.8871473073959351
Training iter #1030000:   Batch Loss = 10.375528, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.063064575195312, Accuracy = 0.882758617401123
Training iter #1032000:   Batch Loss = 10.485195, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.009819984436035, Accuracy = 0.8865203857421875
Training iter #1034000:   Batch Loss = 10.405142, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.307422637939453, Accuracy = 0.86771160364151
Training iter #1036000:   Batch Loss = 10.675870, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.08238410949707, Accuracy = 0.8884012699127197
Training iter #1038000:   Batch Loss = 10.672801, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.914363861083984, Accuracy = 0.8896551728248596
Training iter #1040000:   Batch Loss = 9.806817, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.691877365112305, Accuracy = 0.8865203857421875
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2600
Training iter #1042000:   Batch Loss = 9.993774, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.618670463562012, Accuracy = 0.8846395015716553
Training iter #1044000:   Batch Loss = 10.050445, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.646007537841797, Accuracy = 0.8952978253364563
Training iter #1046000:   Batch Loss = 9.646454, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.433137893676758, Accuracy = 0.8877742886543274
Training iter #1048000:   Batch Loss = 9.841916, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.52835750579834, Accuracy = 0.8777429461479187
Training iter #1050000:   Batch Loss = 9.479595, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.508633613586426, Accuracy = 0.8865203857421875
Training iter #1052000:   Batch Loss = 10.715053, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.983561515808105, Accuracy = 0.8921630382537842
Training iter #1054000:   Batch Loss = 9.651433, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 12.001819610595703, Accuracy = 0.8833855986595154
Training iter #1056000:   Batch Loss = 9.916430, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.825519561767578, Accuracy = 0.894670844078064
Training iter #1058000:   Batch Loss = 9.605198, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.722673416137695, Accuracy = 0.8921630382537842
Training iter #1060000:   Batch Loss = 9.761940, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.473250389099121, Accuracy = 0.8896551728248596
Training iter #1062000:   Batch Loss = 10.076647, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.687453269958496, Accuracy = 0.8852664828300476
Training iter #1064000:   Batch Loss = 10.089373, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.443174362182617, Accuracy = 0.899059534072876
Training iter #1066000:   Batch Loss = 9.535509, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.331673622131348, Accuracy = 0.8971787095069885
Training iter #1068000:   Batch Loss = 9.576729, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.273959159851074, Accuracy = 0.9021943807601929
Training iter #1070000:   Batch Loss = 10.379930, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.563215255737305, Accuracy = 0.8921630382537842
Training iter #1072000:   Batch Loss = 9.657847, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.643266677856445, Accuracy = 0.8971787095069885
Training iter #1074000:   Batch Loss = 10.138743, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.505851745605469, Accuracy = 0.8971787095069885
Training iter #1076000:   Batch Loss = 9.768585, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.465649604797363, Accuracy = 0.8840125203132629
Training iter #1078000:   Batch Loss = 9.302427, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.397253036499023, Accuracy = 0.8909090757369995
Training iter #1080000:   Batch Loss = 9.541993, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.18385124206543, Accuracy = 0.894670844078064
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2700
Training iter #1082000:   Batch Loss = 9.811601, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.097307205200195, Accuracy = 0.8890281915664673
Training iter #1084000:   Batch Loss = 10.298018, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.717669486999512, Accuracy = 0.8984326124191284
Training iter #1086000:   Batch Loss = 9.924153, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.42314338684082, Accuracy = 0.8934169411659241
Training iter #1088000:   Batch Loss = 9.408382, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 11.380228042602539, Accuracy = 0.8959247469902039
Training iter #1090000:   Batch Loss = 9.828798, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.440322875976562, Accuracy = 0.9015673995018005
Training iter #1092000:   Batch Loss = 9.982759, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.291704177856445, Accuracy = 0.9034482836723328
Training iter #1094000:   Batch Loss = 9.403749, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.126546859741211, Accuracy = 0.9009404182434082
Training iter #1096000:   Batch Loss = 9.560515, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.165239334106445, Accuracy = 0.9028213024139404
Training iter #1098000:   Batch Loss = 9.728543, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.023941040039062, Accuracy = 0.8996865153312683
Training iter #1100000:   Batch Loss = 9.142171, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.116403579711914, Accuracy = 0.9021943807601929
Training iter #1102000:   Batch Loss = 9.397352, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.206589698791504, Accuracy = 0.8984326124191284
Training iter #1104000:   Batch Loss = 9.155790, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.190901756286621, Accuracy = 0.8965517282485962
Training iter #1106000:   Batch Loss = 9.494902, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.215540885925293, Accuracy = 0.905329167842865
Training iter #1108000:   Batch Loss = 9.755012, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.071233749389648, Accuracy = 0.9003134965896606
Training iter #1110000:   Batch Loss = 9.155914, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.231809616088867, Accuracy = 0.8909090757369995
Training iter #1112000:   Batch Loss = 9.203972, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.097528457641602, Accuracy = 0.9047021865844727
Training iter #1114000:   Batch Loss = 9.284466, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.884763717651367, Accuracy = 0.8952978253364563
Training iter #1116000:   Batch Loss = 8.803587, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.94778060913086, Accuracy = 0.8996865153312683
Training iter #1118000:   Batch Loss = 8.929633, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.215109825134277, Accuracy = 0.9021943807601929
Training iter #1120000:   Batch Loss = 8.998316, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.005431175231934, Accuracy = 0.9028213024139404
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2800
Training iter #1122000:   Batch Loss = 10.041919, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 11.06387710571289, Accuracy = 0.9028213024139404
Training iter #1124000:   Batch Loss = 9.107498, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.8507719039917, Accuracy = 0.9084639549255371
Training iter #1126000:   Batch Loss = 8.736846, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.766422271728516, Accuracy = 0.9078369736671448
Training iter #1128000:   Batch Loss = 9.326780, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.014020919799805, Accuracy = 0.9047021865844727
Training iter #1130000:   Batch Loss = 9.247515, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.302740097045898, Accuracy = 0.8952978253364563
Training iter #1132000:   Batch Loss = 9.713449, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.330490112304688, Accuracy = 0.899059534072876
Training iter #1134000:   Batch Loss = 9.299879, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.072240829467773, Accuracy = 0.909717857837677
Training iter #1136000:   Batch Loss = 9.416664, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.053540229797363, Accuracy = 0.9040752053260803
Training iter #1138000:   Batch Loss = 9.001925, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.998689651489258, Accuracy = 0.909717857837677
Training iter #1140000:   Batch Loss = 8.620326, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.995710372924805, Accuracy = 0.9047021865844727
Training iter #1142000:   Batch Loss = 9.244714, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.965452194213867, Accuracy = 0.9065830707550049
Training iter #1144000:   Batch Loss = 9.143990, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.432695388793945, Accuracy = 0.8934169411659241
Training iter #1146000:   Batch Loss = 10.141024, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.93356704711914, Accuracy = 0.8952978253364563
Training iter #1148000:   Batch Loss = 9.837609, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.795478820800781, Accuracy = 0.8952978253364563
Training iter #1150000:   Batch Loss = 10.061862, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.711557388305664, Accuracy = 0.8890281915664673
Training iter #1152000:   Batch Loss = 9.543444, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.337743759155273, Accuracy = 0.8978056311607361
Training iter #1154000:   Batch Loss = 9.450080, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.481060981750488, Accuracy = 0.8896551728248596
Training iter #1156000:   Batch Loss = 10.703636, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 11.414409637451172, Accuracy = 0.894670844078064
Training iter #1158000:   Batch Loss = 9.783249, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.327157974243164, Accuracy = 0.8978056311607361
Training iter #1160000:   Batch Loss = 9.828550, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.101555824279785, Accuracy = 0.8909090757369995
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-2900
Training iter #1162000:   Batch Loss = 9.406160, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.15877914428711, Accuracy = 0.8934169411659241
Training iter #1164000:   Batch Loss = 9.160079, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.025443077087402, Accuracy = 0.9015673995018005
Training iter #1166000:   Batch Loss = 8.938000, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.000231742858887, Accuracy = 0.9065830707550049
Training iter #1168000:   Batch Loss = 9.271872, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.903635025024414, Accuracy = 0.9003134965896606
Training iter #1170000:   Batch Loss = 9.136383, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.950448989868164, Accuracy = 0.8971787095069885
Training iter #1172000:   Batch Loss = 9.097577, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.948814392089844, Accuracy = 0.8978056311607361
Training iter #1174000:   Batch Loss = 9.059292, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.779073715209961, Accuracy = 0.9090909361839294
Training iter #1176000:   Batch Loss = 9.052425, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.021361351013184, Accuracy = 0.9028213024139404
Training iter #1178000:   Batch Loss = 9.183416, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.04975700378418, Accuracy = 0.8965517282485962
Training iter #1180000:   Batch Loss = 8.937139, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.755596160888672, Accuracy = 0.9021943807601929
Training iter #1182000:   Batch Loss = 8.773473, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.729183197021484, Accuracy = 0.8952978253364563
Training iter #1184000:   Batch Loss = 8.900215, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.742366790771484, Accuracy = 0.8996865153312683
Training iter #1186000:   Batch Loss = 8.961656, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.659982681274414, Accuracy = 0.8978056311607361
Training iter #1188000:   Batch Loss = 8.638826, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.627215385437012, Accuracy = 0.9034482836723328
Training iter #1190000:   Batch Loss = 9.062822, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 10.490674018859863, Accuracy = 0.9065830707550049
Training iter #1192000:   Batch Loss = 8.404125, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.676177978515625, Accuracy = 0.9065830707550049
Training iter #1194000:   Batch Loss = 8.844223, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.532079696655273, Accuracy = 0.909717857837677
Training iter #1196000:   Batch Loss = 8.969922, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.83791446685791, Accuracy = 0.905329167842865
Training iter #1198000:   Batch Loss = 9.157500, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.979771614074707, Accuracy = 0.8971787095069885
Training iter #1200000:   Batch Loss = 9.073134, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.765129089355469, Accuracy = 0.9047021865844727
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3000
Training iter #1202000:   Batch Loss = 8.718500, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.808592796325684, Accuracy = 0.9009404182434082
Training iter #1204000:   Batch Loss = 8.329388, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.881196022033691, Accuracy = 0.9015673995018005
Training iter #1206000:   Batch Loss = 9.810618, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.61312484741211, Accuracy = 0.9021943807601929
Training iter #1208000:   Batch Loss = 9.074505, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.248714447021484, Accuracy = 0.905329167842865
Training iter #1210000:   Batch Loss = 9.338864, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.182193756103516, Accuracy = 0.9015673995018005
Training iter #1212000:   Batch Loss = 9.157221, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.162958145141602, Accuracy = 0.9059560894966125
Training iter #1214000:   Batch Loss = 9.017924, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.09454345703125, Accuracy = 0.9034482836723328
Training iter #1216000:   Batch Loss = 8.813730, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.820161819458008, Accuracy = 0.909717857837677
Training iter #1218000:   Batch Loss = 8.738354, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.948198318481445, Accuracy = 0.9034482836723328
Training iter #1220000:   Batch Loss = 8.939952, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.82536506652832, Accuracy = 0.9078369736671448
Training iter #1222000:   Batch Loss = 8.641031, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.793632507324219, Accuracy = 0.9040752053260803
Training iter #1224000:   Batch Loss = 8.873194, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 11.0712890625, Accuracy = 0.905329167842865
Training iter #1226000:   Batch Loss = 9.217561, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.934898376464844, Accuracy = 0.9015673995018005
Training iter #1228000:   Batch Loss = 8.737379, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.152461051940918, Accuracy = 0.8940438628196716
Training iter #1230000:   Batch Loss = 9.387364, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.911739349365234, Accuracy = 0.8996865153312683
Training iter #1232000:   Batch Loss = 8.983162, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.800287246704102, Accuracy = 0.8934169411659241
Training iter #1234000:   Batch Loss = 9.177235, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.703943252563477, Accuracy = 0.9003134965896606
Training iter #1236000:   Batch Loss = 8.612108, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.573165893554688, Accuracy = 0.9003134965896606
Training iter #1238000:   Batch Loss = 8.559059, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.676060676574707, Accuracy = 0.9040752053260803
Training iter #1240000:   Batch Loss = 8.787577, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.472320556640625, Accuracy = 0.9034482836723328
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3100
Training iter #1242000:   Batch Loss = 8.870522, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.465880393981934, Accuracy = 0.9028213024139404
Training iter #1244000:   Batch Loss = 8.381225, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.361635208129883, Accuracy = 0.9065830707550049
Training iter #1246000:   Batch Loss = 8.735419, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.654420852661133, Accuracy = 0.8959247469902039
Training iter #1248000:   Batch Loss = 8.650925, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.617132186889648, Accuracy = 0.9090909361839294
Training iter #1250000:   Batch Loss = 8.428258, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.484975814819336, Accuracy = 0.9084639549255371
Training iter #1252000:   Batch Loss = 8.746810, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.629251480102539, Accuracy = 0.9047021865844727
Training iter #1254000:   Batch Loss = 8.686329, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.556947708129883, Accuracy = 0.9034482836723328
Training iter #1256000:   Batch Loss = 8.407182, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.620172500610352, Accuracy = 0.9040752053260803
Training iter #1258000:   Batch Loss = 7.860058, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.542150497436523, Accuracy = 0.9040752053260803
Training iter #1260000:   Batch Loss = 8.528862, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.66601276397705, Accuracy = 0.9003134965896606
Training iter #1262000:   Batch Loss = 8.638674, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.974719047546387, Accuracy = 0.9040752053260803
Training iter #1264000:   Batch Loss = 8.820147, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.821077346801758, Accuracy = 0.8996865153312683
Training iter #1266000:   Batch Loss = 8.338620, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.855013847351074, Accuracy = 0.9009404182434082
Training iter #1268000:   Batch Loss = 8.755138, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.594855308532715, Accuracy = 0.9072100520133972
Training iter #1270000:   Batch Loss = 8.652656, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.585824012756348, Accuracy = 0.9003134965896606
Training iter #1272000:   Batch Loss = 8.447479, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.72565746307373, Accuracy = 0.9059560894966125
Training iter #1274000:   Batch Loss = 8.633595, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.487319946289062, Accuracy = 0.9047021865844727
Training iter #1276000:   Batch Loss = 8.399297, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.437153816223145, Accuracy = 0.9078369736671448
Training iter #1278000:   Batch Loss = 8.597536, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.432191848754883, Accuracy = 0.9047021865844727
Training iter #1280000:   Batch Loss = 8.747037, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.600835800170898, Accuracy = 0.9021943807601929
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3200
Training iter #1282000:   Batch Loss = 8.316044, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.533522605895996, Accuracy = 0.9103448390960693
Training iter #1284000:   Batch Loss = 8.638519, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.38387680053711, Accuracy = 0.9134796261787415
Training iter #1286000:   Batch Loss = 7.710244, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.368780136108398, Accuracy = 0.8978056311607361
Training iter #1288000:   Batch Loss = 8.155501, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.338961601257324, Accuracy = 0.9072100520133972
Training iter #1290000:   Batch Loss = 8.947380, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.587226867675781, Accuracy = 0.9009404182434082
Training iter #1292000:   Batch Loss = 8.293280, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.511163711547852, Accuracy = 0.9128526449203491
Training iter #1294000:   Batch Loss = 8.318099, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.394350051879883, Accuracy = 0.9172413945198059
Training iter #1296000:   Batch Loss = 8.483150, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.274624824523926, Accuracy = 0.9103448390960693
Training iter #1298000:   Batch Loss = 7.995085, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.327435493469238, Accuracy = 0.9159874320030212
Training iter #1300000:   Batch Loss = 8.363434, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.325108528137207, Accuracy = 0.9166144132614136
Training iter #1302000:   Batch Loss = 8.575827, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.436909675598145, Accuracy = 0.9078369736671448
Training iter #1304000:   Batch Loss = 8.079105, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.296470642089844, Accuracy = 0.9072100520133972
Training iter #1306000:   Batch Loss = 8.445330, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.749018669128418, Accuracy = 0.9034482836723328
Training iter #1308000:   Batch Loss = 8.568966, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.533037185668945, Accuracy = 0.9115987420082092
Training iter #1310000:   Batch Loss = 8.582111, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.371848106384277, Accuracy = 0.9141066074371338
Training iter #1312000:   Batch Loss = 8.961907, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.459427833557129, Accuracy = 0.9021943807601929
Training iter #1314000:   Batch Loss = 8.870022, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.887429237365723, Accuracy = 0.9003134965896606
Training iter #1316000:   Batch Loss = 9.626176, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.907740592956543, Accuracy = 0.905329167842865
Training iter #1318000:   Batch Loss = 9.134161, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.654158592224121, Accuracy = 0.9147335290908813
Training iter #1320000:   Batch Loss = 8.712737, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.954507827758789, Accuracy = 0.8934169411659241
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3300
Training iter #1322000:   Batch Loss = 8.861693, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.846630096435547, Accuracy = 0.9072100520133972
Training iter #1324000:   Batch Loss = 8.919225, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.630348205566406, Accuracy = 0.9103448390960693
Training iter #1326000:   Batch Loss = 8.645385, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.767565727233887, Accuracy = 0.9072100520133972
Training iter #1328000:   Batch Loss = 8.875973, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.801409721374512, Accuracy = 0.9084639549255371
Training iter #1330000:   Batch Loss = 8.747681, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.659750938415527, Accuracy = 0.9128526449203491
Training iter #1332000:   Batch Loss = 8.708505, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.720256805419922, Accuracy = 0.9115987420082092
Training iter #1334000:   Batch Loss = 8.553679, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.604811668395996, Accuracy = 0.9090909361839294
Training iter #1336000:   Batch Loss = 8.602837, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.529974937438965, Accuracy = 0.905329167842865
Training iter #1338000:   Batch Loss = 8.306258, Accuracy = 0.987500011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.438779830932617, Accuracy = 0.9115987420082092
Training iter #1340000:   Batch Loss = 8.172845, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.564970016479492, Accuracy = 0.905329167842865
Training iter #1342000:   Batch Loss = 8.434597, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.641921043395996, Accuracy = 0.9147335290908813
Training iter #1344000:   Batch Loss = 8.570805, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.379222869873047, Accuracy = 0.9172413945198059
Training iter #1346000:   Batch Loss = 8.423432, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.331159591674805, Accuracy = 0.9084639549255371
Training iter #1348000:   Batch Loss = 8.326124, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.37932014465332, Accuracy = 0.9134796261787415
Training iter #1350000:   Batch Loss = 8.388604, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.427454948425293, Accuracy = 0.9128526449203491
Training iter #1352000:   Batch Loss = 7.921310, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.247233390808105, Accuracy = 0.9166144132614136
Training iter #1354000:   Batch Loss = 7.741442, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.313190460205078, Accuracy = 0.9159874320030212
Training iter #1356000:   Batch Loss = 8.020550, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.319929122924805, Accuracy = 0.9153605103492737
Training iter #1358000:   Batch Loss = 7.992211, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.288338661193848, Accuracy = 0.9172413945198059
Training iter #1360000:   Batch Loss = 8.423807, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.181451797485352, Accuracy = 0.9159874320030212
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3400
Training iter #1362000:   Batch Loss = 7.861530, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.441095352172852, Accuracy = 0.909717857837677
Training iter #1364000:   Batch Loss = 8.240391, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.317686080932617, Accuracy = 0.9115987420082092
Training iter #1366000:   Batch Loss = 8.179767, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.529809951782227, Accuracy = 0.9147335290908813
Training iter #1368000:   Batch Loss = 8.641100, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.330945014953613, Accuracy = 0.9147335290908813
Training iter #1370000:   Batch Loss = 7.827718, Accuracy = 0.987500011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.156625747680664, Accuracy = 0.9172413945198059
Training iter #1372000:   Batch Loss = 8.447655, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.311604499816895, Accuracy = 0.9122257232666016
Training iter #1374000:   Batch Loss = 8.345098, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.337457656860352, Accuracy = 0.9159874320030212
Training iter #1376000:   Batch Loss = 8.398905, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.256342887878418, Accuracy = 0.9178683161735535
Training iter #1378000:   Batch Loss = 8.414528, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.31273365020752, Accuracy = 0.9178683161735535
Training iter #1380000:   Batch Loss = 8.007914, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.01506519317627, Accuracy = 0.9159874320030212
Training iter #1382000:   Batch Loss = 9.603041, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.344654083251953, Accuracy = 0.8978056311607361
Training iter #1384000:   Batch Loss = 9.680782, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.167391777038574, Accuracy = 0.9015673995018005
Training iter #1386000:   Batch Loss = 8.818246, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.792381286621094, Accuracy = 0.9072100520133972
Training iter #1388000:   Batch Loss = 8.622905, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.786624908447266, Accuracy = 0.9072100520133972
Training iter #1390000:   Batch Loss = 8.462627, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.448413848876953, Accuracy = 0.9059560894966125
Training iter #1392000:   Batch Loss = 8.192993, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.434523582458496, Accuracy = 0.9021943807601929
Training iter #1394000:   Batch Loss = 8.023003, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.374873161315918, Accuracy = 0.9128526449203491
Training iter #1396000:   Batch Loss = 8.169268, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.36790657043457, Accuracy = 0.9059560894966125
Training iter #1398000:   Batch Loss = 8.181194, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.24885082244873, Accuracy = 0.9078369736671448
Training iter #1400000:   Batch Loss = 8.017622, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.203478813171387, Accuracy = 0.9166144132614136
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3500
Training iter #1402000:   Batch Loss = 9.499326, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.822352409362793, Accuracy = 0.8996865153312683
Training iter #1404000:   Batch Loss = 9.639891, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.664201736450195, Accuracy = 0.8921630382537842
Training iter #1406000:   Batch Loss = 9.989247, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.203394889831543, Accuracy = 0.8996865153312683
Training iter #1408000:   Batch Loss = 9.500015, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.996241569519043, Accuracy = 0.9003134965896606
Training iter #1410000:   Batch Loss = 9.136645, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.785282135009766, Accuracy = 0.9065830707550049
Training iter #1412000:   Batch Loss = 8.982470, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.727921485900879, Accuracy = 0.9047021865844727
Training iter #1414000:   Batch Loss = 8.888810, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.673388481140137, Accuracy = 0.9103448390960693
Training iter #1416000:   Batch Loss = 8.754798, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.588443756103516, Accuracy = 0.9065830707550049
Training iter #1418000:   Batch Loss = 8.878494, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.659076690673828, Accuracy = 0.9065830707550049
Training iter #1420000:   Batch Loss = 8.798409, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.537419319152832, Accuracy = 0.9059560894966125
Training iter #1422000:   Batch Loss = 8.793333, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.434540748596191, Accuracy = 0.9078369736671448
Training iter #1424000:   Batch Loss = 8.974005, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.411870002746582, Accuracy = 0.9072100520133972
Training iter #1426000:   Batch Loss = 8.868336, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.358345985412598, Accuracy = 0.9078369736671448
Training iter #1428000:   Batch Loss = 7.619998, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.30447006225586, Accuracy = 0.909717857837677
Training iter #1430000:   Batch Loss = 8.547509, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.310873031616211, Accuracy = 0.9153605103492737
Training iter #1432000:   Batch Loss = 8.090276, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.234668731689453, Accuracy = 0.9159874320030212
Training iter #1434000:   Batch Loss = 8.237687, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.16190242767334, Accuracy = 0.909717857837677
Training iter #1436000:   Batch Loss = 8.762733, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.34290885925293, Accuracy = 0.9134796261787415
Training iter #1438000:   Batch Loss = 8.486511, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.205513954162598, Accuracy = 0.9122257232666016
Training iter #1440000:   Batch Loss = 7.921582, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.410938262939453, Accuracy = 0.9109717607498169
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3600
Training iter #1442000:   Batch Loss = 9.044588, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.127510070800781, Accuracy = 0.9072100520133972
Training iter #1444000:   Batch Loss = 9.153570, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.58906364440918, Accuracy = 0.9059560894966125
Training iter #1446000:   Batch Loss = 9.738420, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.734579086303711, Accuracy = 0.9034482836723328
Training iter #1448000:   Batch Loss = 9.780571, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.499305725097656, Accuracy = 0.9034482836723328
Training iter #1450000:   Batch Loss = 9.583611, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.320307731628418, Accuracy = 0.9122257232666016
Training iter #1452000:   Batch Loss = 9.185749, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.350942611694336, Accuracy = 0.9040752053260803
Training iter #1454000:   Batch Loss = 9.324071, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.295005798339844, Accuracy = 0.8996865153312683
Training iter #1456000:   Batch Loss = 8.993404, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.149620056152344, Accuracy = 0.9059560894966125
Training iter #1458000:   Batch Loss = 9.637086, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.27048397064209, Accuracy = 0.9078369736671448
Training iter #1460000:   Batch Loss = 9.161154, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.08231258392334, Accuracy = 0.9103448390960693
Training iter #1462000:   Batch Loss = 8.569536, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 11.295045852661133, Accuracy = 0.9021943807601929
Training iter #1464000:   Batch Loss = 8.927824, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.908990859985352, Accuracy = 0.909717857837677
Training iter #1466000:   Batch Loss = 9.137623, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.930532455444336, Accuracy = 0.9040752053260803
Training iter #1468000:   Batch Loss = 8.486454, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.777462005615234, Accuracy = 0.909717857837677
Training iter #1470000:   Batch Loss = 8.717272, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.763862609863281, Accuracy = 0.9128526449203491
Training iter #1472000:   Batch Loss = 8.935911, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.910966873168945, Accuracy = 0.9072100520133972
Training iter #1474000:   Batch Loss = 9.135729, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.883829116821289, Accuracy = 0.9047021865844727
Training iter #1476000:   Batch Loss = 8.574376, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.957525253295898, Accuracy = 0.9021943807601929
Training iter #1478000:   Batch Loss = 8.836254, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.802496910095215, Accuracy = 0.9047021865844727
Training iter #1480000:   Batch Loss = 8.451385, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.731462478637695, Accuracy = 0.9059560894966125
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3700
Training iter #1482000:   Batch Loss = 8.610025, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.678621292114258, Accuracy = 0.9047021865844727
Training iter #1484000:   Batch Loss = 9.454960, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.006275177001953, Accuracy = 0.899059534072876
Training iter #1486000:   Batch Loss = 8.734224, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.990788459777832, Accuracy = 0.9028213024139404
Training iter #1488000:   Batch Loss = 9.455164, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.911971092224121, Accuracy = 0.8965517282485962
Training iter #1490000:   Batch Loss = 9.527838, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.118247985839844, Accuracy = 0.8978056311607361
Training iter #1492000:   Batch Loss = 9.701560, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.072306632995605, Accuracy = 0.9072100520133972
Training iter #1494000:   Batch Loss = 9.459322, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.987826347351074, Accuracy = 0.9084639549255371
Training iter #1496000:   Batch Loss = 11.836066, Accuracy = 0.8793103694915771
PERFORMANCE ON TEST SET: Batch Loss = 10.91812515258789, Accuracy = 0.9003134965896606
Training iter #1498000:   Batch Loss = 8.956823, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.8223237991333, Accuracy = 0.9084639549255371
Training iter #1500000:   Batch Loss = 9.055819, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.799120903015137, Accuracy = 0.894670844078064
Training iter #1502000:   Batch Loss = 8.745475, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.44480037689209, Accuracy = 0.9090909361839294
Training iter #1504000:   Batch Loss = 8.453471, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.587931632995605, Accuracy = 0.9065830707550049
Training iter #1506000:   Batch Loss = 8.881137, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.688606262207031, Accuracy = 0.9078369736671448
Training iter #1508000:   Batch Loss = 8.872509, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.592610359191895, Accuracy = 0.9090909361839294
Training iter #1510000:   Batch Loss = 8.899245, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.452988624572754, Accuracy = 0.8909090757369995
Training iter #1512000:   Batch Loss = 9.773538, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.23820686340332, Accuracy = 0.8959247469902039
Training iter #1514000:   Batch Loss = 9.257984, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.976070404052734, Accuracy = 0.9009404182434082
Training iter #1516000:   Batch Loss = 9.122126, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.905054092407227, Accuracy = 0.8978056311607361
Training iter #1518000:   Batch Loss = 9.183632, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.806694030761719, Accuracy = 0.909717857837677
Training iter #1520000:   Batch Loss = 9.009462, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.673850059509277, Accuracy = 0.9047021865844727
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3800
Training iter #1522000:   Batch Loss = 8.847319, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.471672058105469, Accuracy = 0.9084639549255371
Training iter #1524000:   Batch Loss = 8.738335, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.488309860229492, Accuracy = 0.905329167842865
Training iter #1526000:   Batch Loss = 8.328840, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.44540023803711, Accuracy = 0.9015673995018005
Training iter #1528000:   Batch Loss = 8.346709, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.458932876586914, Accuracy = 0.9065830707550049
Training iter #1530000:   Batch Loss = 7.488503, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 10.526227951049805, Accuracy = 0.9153605103492737
Training iter #1532000:   Batch Loss = 8.501215, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.493680953979492, Accuracy = 0.9040752053260803
Training iter #1534000:   Batch Loss = 8.815832, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.45960807800293, Accuracy = 0.9009404182434082
Training iter #1536000:   Batch Loss = 8.218838, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.274592399597168, Accuracy = 0.9072100520133972
Training iter #1538000:   Batch Loss = 8.658943, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.339649200439453, Accuracy = 0.9072100520133972
Training iter #1540000:   Batch Loss = 8.650347, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.584512710571289, Accuracy = 0.9065830707550049
Training iter #1542000:   Batch Loss = 8.794066, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.395425796508789, Accuracy = 0.9090909361839294
Training iter #1544000:   Batch Loss = 8.454617, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.495043754577637, Accuracy = 0.9065830707550049
Training iter #1546000:   Batch Loss = 8.334397, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.444561004638672, Accuracy = 0.8996865153312683
Training iter #1548000:   Batch Loss = 8.419111, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.274778366088867, Accuracy = 0.9047021865844727
Training iter #1550000:   Batch Loss = 10.007772, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.387069702148438, Accuracy = 0.8959247469902039
Training iter #1552000:   Batch Loss = 10.344368, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.859709739685059, Accuracy = 0.8909090757369995
Training iter #1554000:   Batch Loss = 9.291183, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.520278930664062, Accuracy = 0.9090909361839294
Training iter #1556000:   Batch Loss = 9.482581, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.189810752868652, Accuracy = 0.9040752053260803
Training iter #1558000:   Batch Loss = 9.298798, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.21312141418457, Accuracy = 0.8978056311607361
Training iter #1560000:   Batch Loss = 9.281232, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.096817016601562, Accuracy = 0.9065830707550049
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-3900
Training iter #1562000:   Batch Loss = 9.282711, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.010595321655273, Accuracy = 0.9021943807601929
Training iter #1564000:   Batch Loss = 8.790661, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.878008842468262, Accuracy = 0.9009404182434082
Training iter #1566000:   Batch Loss = 8.843867, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.871511459350586, Accuracy = 0.9015673995018005
Training iter #1568000:   Batch Loss = 8.916368, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.550470352172852, Accuracy = 0.9028213024139404
Training iter #1570000:   Batch Loss = 8.519895, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.578231811523438, Accuracy = 0.9047021865844727
Training iter #1572000:   Batch Loss = 8.682909, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.620484352111816, Accuracy = 0.9115987420082092
Training iter #1574000:   Batch Loss = 8.239092, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.39852523803711, Accuracy = 0.9122257232666016
Training iter #1576000:   Batch Loss = 8.298384, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.462372779846191, Accuracy = 0.9128526449203491
Training iter #1578000:   Batch Loss = 8.262399, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.521673202514648, Accuracy = 0.9078369736671448
Training iter #1580000:   Batch Loss = 8.388144, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.477550506591797, Accuracy = 0.9103448390960693
Training iter #1582000:   Batch Loss = 8.270785, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.516430854797363, Accuracy = 0.9128526449203491
Training iter #1584000:   Batch Loss = 8.409300, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.463857650756836, Accuracy = 0.9109717607498169
Training iter #1586000:   Batch Loss = 8.414577, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.59394645690918, Accuracy = 0.9040752053260803
Training iter #1588000:   Batch Loss = 8.267956, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.372735977172852, Accuracy = 0.9090909361839294
Training iter #1590000:   Batch Loss = 8.424852, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.595956802368164, Accuracy = 0.909717857837677
Training iter #1592000:   Batch Loss = 8.663069, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.76512622833252, Accuracy = 0.9015673995018005
Training iter #1594000:   Batch Loss = 8.553505, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.684030532836914, Accuracy = 0.9028213024139404
Training iter #1596000:   Batch Loss = 8.745290, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.47340202331543, Accuracy = 0.909717857837677
Training iter #1598000:   Batch Loss = 8.393980, Accuracy = 0.982758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.435582160949707, Accuracy = 0.905329167842865
Training iter #1600000:   Batch Loss = 8.380037, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.450695037841797, Accuracy = 0.905329167842865
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4000
Training iter #1602000:   Batch Loss = 8.209860, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.302325248718262, Accuracy = 0.9141066074371338
Training iter #1604000:   Batch Loss = 8.140439, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.357486724853516, Accuracy = 0.9072100520133972
Training iter #1606000:   Batch Loss = 8.818473, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.666947364807129, Accuracy = 0.9084639549255371
Training iter #1608000:   Batch Loss = 7.953577, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.287435531616211, Accuracy = 0.9159874320030212
Training iter #1610000:   Batch Loss = 8.112900, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.212672233581543, Accuracy = 0.9159874320030212
Training iter #1612000:   Batch Loss = 8.017603, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.298348426818848, Accuracy = 0.9021943807601929
Training iter #1614000:   Batch Loss = 8.318305, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.322473526000977, Accuracy = 0.9021943807601929
Training iter #1616000:   Batch Loss = 7.938146, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.16968822479248, Accuracy = 0.9109717607498169
Training iter #1618000:   Batch Loss = 7.942578, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.185384750366211, Accuracy = 0.9159874320030212
Training iter #1620000:   Batch Loss = 8.090534, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.218517303466797, Accuracy = 0.9134796261787415
Training iter #1622000:   Batch Loss = 8.185762, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.198609352111816, Accuracy = 0.9159874320030212
Training iter #1624000:   Batch Loss = 7.976728, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.108949661254883, Accuracy = 0.9109717607498169
Training iter #1626000:   Batch Loss = 7.757576, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.239086151123047, Accuracy = 0.9103448390960693
Training iter #1628000:   Batch Loss = 8.453068, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.297324180603027, Accuracy = 0.9084639549255371
Training iter #1630000:   Batch Loss = 7.672534, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.19418716430664, Accuracy = 0.9178683161735535
Training iter #1632000:   Batch Loss = 8.094871, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 10.083685874938965, Accuracy = 0.9197492003440857
Training iter #1634000:   Batch Loss = 7.906963, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 9.988794326782227, Accuracy = 0.9159874320030212
Training iter #1636000:   Batch Loss = 7.697278, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.08480453491211, Accuracy = 0.9159874320030212
Training iter #1638000:   Batch Loss = 7.838509, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.069252014160156, Accuracy = 0.9153605103492737
Training iter #1640000:   Batch Loss = 7.591830, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.016767501831055, Accuracy = 0.9090909361839294
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4100
Training iter #1642000:   Batch Loss = 7.480485, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.039896965026855, Accuracy = 0.9134796261787415
Training iter #1644000:   Batch Loss = 7.597897, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 9.834402084350586, Accuracy = 0.9178683161735535
Training iter #1646000:   Batch Loss = 7.610520, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.124093055725098, Accuracy = 0.9178683161735535
Training iter #1648000:   Batch Loss = 7.782400, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.162761688232422, Accuracy = 0.9235109686851501
Training iter #1650000:   Batch Loss = 8.548475, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.387425422668457, Accuracy = 0.9153605103492737
Training iter #1652000:   Batch Loss = 8.372481, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.344015121459961, Accuracy = 0.9166144132614136
Training iter #1654000:   Batch Loss = 8.517550, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.513721466064453, Accuracy = 0.9128526449203491
Training iter #1656000:   Batch Loss = 9.239628, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.8413724899292, Accuracy = 0.9078369736671448
Training iter #1658000:   Batch Loss = 8.454106, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.73157024383545, Accuracy = 0.9153605103492737
Training iter #1660000:   Batch Loss = 8.581871, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.668251037597656, Accuracy = 0.9141066074371338
Training iter #1662000:   Batch Loss = 8.673189, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.469402313232422, Accuracy = 0.9109717607498169
Training iter #1664000:   Batch Loss = 8.383125, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.394573211669922, Accuracy = 0.9159874320030212
Training iter #1666000:   Batch Loss = 9.141623, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 10.468509674072266, Accuracy = 0.9134796261787415
Training iter #1668000:   Batch Loss = 8.858641, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.747961044311523, Accuracy = 0.9065830707550049
Training iter #1670000:   Batch Loss = 8.481485, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.362344741821289, Accuracy = 0.9159874320030212
Training iter #1672000:   Batch Loss = 8.891699, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.424737930297852, Accuracy = 0.9047021865844727
Training iter #1674000:   Batch Loss = 8.357178, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.38187313079834, Accuracy = 0.909717857837677
Training iter #1676000:   Batch Loss = 8.605669, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.339250564575195, Accuracy = 0.909717857837677
Training iter #1678000:   Batch Loss = 8.482283, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.289426803588867, Accuracy = 0.9028213024139404
Training iter #1680000:   Batch Loss = 8.214903, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.304704666137695, Accuracy = 0.9047021865844727
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4200
Training iter #1682000:   Batch Loss = 7.905583, Accuracy = 0.9925000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.136035919189453, Accuracy = 0.9065830707550049
Training iter #1684000:   Batch Loss = 7.952486, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.090127944946289, Accuracy = 0.920376181602478
Training iter #1686000:   Batch Loss = 8.219862, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.284923553466797, Accuracy = 0.9172413945198059
Training iter #1688000:   Batch Loss = 7.976892, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.178108215332031, Accuracy = 0.9228839874267578
Training iter #1690000:   Batch Loss = 8.092870, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.131860733032227, Accuracy = 0.9109717607498169
Training iter #1692000:   Batch Loss = 8.094800, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 9.909801483154297, Accuracy = 0.9172413945198059
Training iter #1694000:   Batch Loss = 7.988003, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.10635757446289, Accuracy = 0.9166144132614136
Training iter #1696000:   Batch Loss = 8.155441, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.10710334777832, Accuracy = 0.9115987420082092
Training iter #1698000:   Batch Loss = 7.443965, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 9.949808120727539, Accuracy = 0.9184952974319458
Training iter #1700000:   Batch Loss = 7.198579, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 9.94906997680664, Accuracy = 0.9159874320030212
Training iter #1702000:   Batch Loss = 7.963964, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 9.932412147521973, Accuracy = 0.9153605103492737
Training iter #1704000:   Batch Loss = 7.612926, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 9.888931274414062, Accuracy = 0.9172413945198059
Training iter #1706000:   Batch Loss = 7.892002, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 9.958099365234375, Accuracy = 0.9147335290908813
Training iter #1708000:   Batch Loss = 7.694471, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.17896556854248, Accuracy = 0.9153605103492737
Training iter #1710000:   Batch Loss = 8.013248, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.134471893310547, Accuracy = 0.9184952974319458
Training iter #1712000:   Batch Loss = 7.870921, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.033184051513672, Accuracy = 0.9134796261787415
Training iter #1714000:   Batch Loss = 8.066125, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.231032371520996, Accuracy = 0.9141066074371338
Training iter #1716000:   Batch Loss = 8.032829, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.220654487609863, Accuracy = 0.9166144132614136
Training iter #1718000:   Batch Loss = 7.893966, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.075308799743652, Accuracy = 0.9159874320030212
Training iter #1720000:   Batch Loss = 8.193765, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.360130310058594, Accuracy = 0.909717857837677
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4300
Training iter #1722000:   Batch Loss = 8.168816, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.070032119750977, Accuracy = 0.9128526449203491
Training iter #1724000:   Batch Loss = 8.248331, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.288324356079102, Accuracy = 0.9103448390960693
Training iter #1726000:   Batch Loss = 7.868947, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 9.973320960998535, Accuracy = 0.9159874320030212
Training iter #1728000:   Batch Loss = 7.894254, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.038098335266113, Accuracy = 0.9166144132614136
Training iter #1730000:   Batch Loss = 7.614105, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 9.922311782836914, Accuracy = 0.9122257232666016
Training iter #1732000:   Batch Loss = 7.890406, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 9.839017868041992, Accuracy = 0.9147335290908813
Training iter #1734000:   Batch Loss = 8.628465, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 9.957563400268555, Accuracy = 0.9122257232666016
Training iter #1736000:   Batch Loss = 7.965549, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.059250831604004, Accuracy = 0.909717857837677
Training iter #1738000:   Batch Loss = 7.807676, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.054386138916016, Accuracy = 0.9166144132614136
Training iter #1740000:   Batch Loss = 7.791111, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 9.929791450500488, Accuracy = 0.9147335290908813
Training iter #1742000:   Batch Loss = 7.581555, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.021562576293945, Accuracy = 0.9128526449203491
Training iter #1744000:   Batch Loss = 7.676888, Accuracy = 0.987500011920929
PERFORMANCE ON TEST SET: Batch Loss = 9.97230339050293, Accuracy = 0.9141066074371338
Training iter #1746000:   Batch Loss = 7.901066, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.132157325744629, Accuracy = 0.899059534072876
Training iter #1748000:   Batch Loss = 9.083271, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.053317070007324, Accuracy = 0.909717857837677
Training iter #1750000:   Batch Loss = 9.904225, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.877853393554688, Accuracy = 0.9072100520133972
Training iter #1752000:   Batch Loss = 9.837635, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.765068054199219, Accuracy = 0.9090909361839294
Training iter #1754000:   Batch Loss = 9.337167, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.406723976135254, Accuracy = 0.9040752053260803
Training iter #1756000:   Batch Loss = 9.085526, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.210614204406738, Accuracy = 0.905329167842865
Training iter #1758000:   Batch Loss = 9.171453, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.03225326538086, Accuracy = 0.9003134965896606
Training iter #1760000:   Batch Loss = 8.439723, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.870233535766602, Accuracy = 0.9122257232666016
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4400
Training iter #1762000:   Batch Loss = 9.552064, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.711788177490234, Accuracy = 0.905329167842865
Training iter #1764000:   Batch Loss = 9.162930, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.50885009765625, Accuracy = 0.9090909361839294
Training iter #1766000:   Batch Loss = 8.910749, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.288546562194824, Accuracy = 0.9128526449203491
Training iter #1768000:   Batch Loss = 9.965780, Accuracy = 0.931034505367279
PERFORMANCE ON TEST SET: Batch Loss = 11.44235610961914, Accuracy = 0.9166144132614136
Training iter #1770000:   Batch Loss = 9.501999, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.467700958251953, Accuracy = 0.9072100520133972
Training iter #1772000:   Batch Loss = 9.419630, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.343989372253418, Accuracy = 0.9028213024139404
Training iter #1774000:   Batch Loss = 9.260660, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.324262619018555, Accuracy = 0.9147335290908813
Training iter #1776000:   Batch Loss = 9.883874, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.380858421325684, Accuracy = 0.909717857837677
Training iter #1778000:   Batch Loss = 9.329079, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.331731796264648, Accuracy = 0.9040752053260803
Training iter #1780000:   Batch Loss = 9.096371, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.289854049682617, Accuracy = 0.9078369736671448
Training iter #1782000:   Batch Loss = 10.462875, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.175019264221191, Accuracy = 0.9028213024139404
Training iter #1784000:   Batch Loss = 9.808330, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.801193237304688, Accuracy = 0.9003134965896606
Training iter #1786000:   Batch Loss = 9.502941, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.661600112915039, Accuracy = 0.9090909361839294
Training iter #1788000:   Batch Loss = 9.368844, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.67896556854248, Accuracy = 0.9034482836723328
Training iter #1790000:   Batch Loss = 9.759193, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.920122146606445, Accuracy = 0.9009404182434082
Training iter #1792000:   Batch Loss = 9.559834, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.752955436706543, Accuracy = 0.8927899599075317
Training iter #1794000:   Batch Loss = 9.586141, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.701553344726562, Accuracy = 0.8934169411659241
Training iter #1796000:   Batch Loss = 9.543119, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.695333480834961, Accuracy = 0.8984326124191284
Training iter #1798000:   Batch Loss = 9.554614, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.731063842773438, Accuracy = 0.8940438628196716
Training iter #1800000:   Batch Loss = 9.670580, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.815543174743652, Accuracy = 0.8940438628196716
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4500
Training iter #1802000:   Batch Loss = 9.176568, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.497097969055176, Accuracy = 0.8996865153312683
Training iter #1804000:   Batch Loss = 9.337971, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.20250129699707, Accuracy = 0.905329167842865
Training iter #1806000:   Batch Loss = 9.135128, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.34146499633789, Accuracy = 0.9040752053260803
Training iter #1808000:   Batch Loss = 8.850875, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.21187973022461, Accuracy = 0.9103448390960693
Training iter #1810000:   Batch Loss = 9.369734, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.452618598937988, Accuracy = 0.9009404182434082
Training iter #1812000:   Batch Loss = 9.639020, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.253978729248047, Accuracy = 0.9090909361839294
Training iter #1814000:   Batch Loss = 9.161679, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.214767456054688, Accuracy = 0.9009404182434082
Training iter #1816000:   Batch Loss = 9.172576, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.216348648071289, Accuracy = 0.9115987420082092
Training iter #1818000:   Batch Loss = 9.728058, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.755990028381348, Accuracy = 0.8971787095069885
Training iter #1820000:   Batch Loss = 9.370972, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.59095573425293, Accuracy = 0.8996865153312683
Training iter #1822000:   Batch Loss = 9.240515, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.27045726776123, Accuracy = 0.8927899599075317
Training iter #1824000:   Batch Loss = 9.207619, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.19486141204834, Accuracy = 0.905329167842865
Training iter #1826000:   Batch Loss = 9.696737, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.246774673461914, Accuracy = 0.9047021865844727
Training iter #1828000:   Batch Loss = 9.209469, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.0980806350708, Accuracy = 0.9015673995018005
Training iter #1830000:   Batch Loss = 8.769507, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.085531234741211, Accuracy = 0.9028213024139404
Training iter #1832000:   Batch Loss = 8.600197, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.150383949279785, Accuracy = 0.9021943807601929
Training iter #1834000:   Batch Loss = 9.187480, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.21657943725586, Accuracy = 0.899059534072876
Training iter #1836000:   Batch Loss = 9.474332, Accuracy = 0.9655172228813171
PERFORMANCE ON TEST SET: Batch Loss = 11.051675796508789, Accuracy = 0.8996865153312683
Training iter #1838000:   Batch Loss = 9.100401, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.932114601135254, Accuracy = 0.9072100520133972
Training iter #1840000:   Batch Loss = 8.995667, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.890792846679688, Accuracy = 0.909717857837677
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4600
Training iter #1842000:   Batch Loss = 8.913566, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.89196491241455, Accuracy = 0.9047021865844727
Training iter #1844000:   Batch Loss = 8.672455, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.901729583740234, Accuracy = 0.9015673995018005
Training iter #1846000:   Batch Loss = 9.030214, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.87578010559082, Accuracy = 0.8877742886543274
Training iter #1848000:   Batch Loss = 11.260073, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.84150505065918, Accuracy = 0.8940438628196716
Training iter #1850000:   Batch Loss = 10.690546, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.494876861572266, Accuracy = 0.8952978253364563
Training iter #1852000:   Batch Loss = 10.410804, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.515307426452637, Accuracy = 0.8996865153312683
Training iter #1854000:   Batch Loss = 10.494517, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.570722579956055, Accuracy = 0.8996865153312683
Training iter #1856000:   Batch Loss = 10.440647, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.416596412658691, Accuracy = 0.909717857837677
Training iter #1858000:   Batch Loss = 10.527948, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.57355785369873, Accuracy = 0.8952978253364563
Training iter #1860000:   Batch Loss = 10.713764, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.497034072875977, Accuracy = 0.8984326124191284
Training iter #1862000:   Batch Loss = 10.885851, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.931629180908203, Accuracy = 0.894670844078064
Training iter #1864000:   Batch Loss = 11.085161, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 13.362506866455078, Accuracy = 0.8921630382537842
Training iter #1866000:   Batch Loss = 11.023550, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.105619430541992, Accuracy = 0.8871473073959351
Training iter #1868000:   Batch Loss = 11.185145, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.948686599731445, Accuracy = 0.8840125203132629
Training iter #1870000:   Batch Loss = 10.581386, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 13.012533187866211, Accuracy = 0.8921630382537842
Training iter #1872000:   Batch Loss = 11.286782, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 13.126423835754395, Accuracy = 0.8934169411659241
Training iter #1874000:   Batch Loss = 11.255804, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.980173110961914, Accuracy = 0.878369927406311
Training iter #1876000:   Batch Loss = 11.366339, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.820176124572754, Accuracy = 0.8871473073959351
Training iter #1878000:   Batch Loss = 10.445679, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.757339477539062, Accuracy = 0.8846395015716553
Training iter #1880000:   Batch Loss = 10.942082, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.013591766357422, Accuracy = 0.8952978253364563
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4700
Training iter #1882000:   Batch Loss = 11.304603, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.823609352111816, Accuracy = 0.8940438628196716
Training iter #1884000:   Batch Loss = 10.556934, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.608592987060547, Accuracy = 0.8934169411659241
Training iter #1886000:   Batch Loss = 10.759777, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.548349380493164, Accuracy = 0.8934169411659241
Training iter #1888000:   Batch Loss = 10.655219, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.45304012298584, Accuracy = 0.8915360569953918
Training iter #1890000:   Batch Loss = 10.326254, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.351619720458984, Accuracy = 0.894670844078064
Training iter #1892000:   Batch Loss = 10.555984, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.892313003540039, Accuracy = 0.8896551728248596
Training iter #1894000:   Batch Loss = 11.436119, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.103381156921387, Accuracy = 0.8915360569953918
Training iter #1896000:   Batch Loss = 11.059561, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.860233306884766, Accuracy = 0.8959247469902039
Training iter #1898000:   Batch Loss = 10.723466, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.830119132995605, Accuracy = 0.8833855986595154
Training iter #1900000:   Batch Loss = 10.938570, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.89238166809082, Accuracy = 0.8846395015716553
Training iter #1902000:   Batch Loss = 10.945921, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.70628547668457, Accuracy = 0.8821316361427307
Training iter #1904000:   Batch Loss = 11.628035, Accuracy = 0.9482758641242981
PERFORMANCE ON TEST SET: Batch Loss = 12.827829360961914, Accuracy = 0.8884012699127197
Training iter #1906000:   Batch Loss = 10.216459, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.58655834197998, Accuracy = 0.8877742886543274
Training iter #1908000:   Batch Loss = 10.114611, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.506961822509766, Accuracy = 0.8896551728248596
Training iter #1910000:   Batch Loss = 10.093939, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.338581085205078, Accuracy = 0.8833855986595154
Training iter #1912000:   Batch Loss = 10.215246, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.740832328796387, Accuracy = 0.8921630382537842
Training iter #1914000:   Batch Loss = 10.467913, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.519516944885254, Accuracy = 0.8934169411659241
Training iter #1916000:   Batch Loss = 10.291193, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.343825340270996, Accuracy = 0.8934169411659241
Training iter #1918000:   Batch Loss = 10.622923, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.312636375427246, Accuracy = 0.890282154083252
Training iter #1920000:   Batch Loss = 10.338009, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.50123119354248, Accuracy = 0.890282154083252
Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-4800
Training iter #1922000:   Batch Loss = 10.771209, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.819116592407227, Accuracy = 0.878369927406311
Training iter #1924000:   Batch Loss = 11.504318, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 13.265746116638184, Accuracy = 0.8877742886543274
Training iter #1926000:   Batch Loss = 11.628647, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 13.36590576171875, Accuracy = 0.8846395015716553
Training iter #1928000:   Batch Loss = 11.373760, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.010770797729492, Accuracy = 0.8909090757369995
Training iter #1930000:   Batch Loss = 11.122316, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.99310302734375, Accuracy = 0.8846395015716553
Training iter #1932000:   Batch Loss = 11.340610, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.296829223632812, Accuracy = 0.8796238303184509
Training iter #1934000:   Batch Loss = 10.997643, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.897660255432129, Accuracy = 0.8840125203132629
Training iter #1936000:   Batch Loss = 10.884291, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.804170608520508, Accuracy = 0.8896551728248596
Optimization Finished!
FINAL RESULT: Batch Loss = 12.620623588562012, Accuracy = 0.8909090757369995
All train time = 17199.164047956467
Final Model saved in file: ./lstm2/model_selftest_kfold0.ckpt-final
Precision: 89.2588377120448%
Recall: 89.0909090909091%
f1_score: 89.1331573997563%

Confusion Matrix:
[[143   0   1   1   0   0   6   4   0   5]
 [  1 155   0   0   0   0   1   0   3   0]
 [  2   0 133   3   0   0   4   5  14   0]
 [  1   0   1 135   4   0   1  16   0   1]
 [  0   2   0   0 157   0   0   0   0   0]
 [  0   0   5   0   2 152   0   0   2   0]
 [  3   1   5   1   0   0 137  11   2   0]
 [  2   0   0   8   0   0  18 131   0   1]
 [  0   1  26   3   0   1   0   1 123   0]
 [  5   0   0   0   0   0   0   0   0 155]]

Confusion matrix (normalised to % of total test data):
[[8.965518   0.         0.06269593 0.06269593 0.         0.
  0.37617555 0.2507837  0.         0.3134796 ]
 [0.06269593 9.717868   0.         0.         0.         0.
  0.06269593 0.         0.18808778 0.        ]
 [0.12539186 0.         8.338558   0.18808778 0.         0.
  0.2507837  0.3134796  0.87774295 0.        ]
 [0.06269593 0.         0.06269593 8.46395    0.2507837  0.
  0.06269593 1.0031348  0.         0.06269593]
 [0.         0.12539186 0.         0.         9.84326    0.
  0.         0.         0.         0.        ]
 [0.         0.         0.3134796  0.         0.12539186 9.52978
  0.         0.         0.12539186 0.        ]
 [0.18808778 0.06269593 0.3134796  0.06269593 0.         0.
  8.589341   0.6896552  0.12539186 0.        ]
 [0.12539186 0.         0.         0.5015674  0.         0.
  1.1285267  8.213166   0.         0.06269593]
 [0.         0.06269593 1.630094   0.18808778 0.         0.06269593
  0.         0.06269593 7.711599   0.        ]
 [0.3134796  0.         0.         0.         0.         0.
  0.         0.         0.         9.717868  ]]/home/sunrepe/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))

Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.

WARNING:tensorflow:From all_three_lstm.py:91: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:93: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.
WARNING:tensorflow:From all_three_lstm.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From all_three_lstm.py:94: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `keras.layers.RNN(cell)`, which is equivalent to this API
WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From all_three_lstm.py:96: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From all_three_lstm.py:97: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
loading data...
train: 6445 test: 1608
load data time: 197.02471780776978
Start train!
Training iter #400:   Batch Loss = 26.827190, Accuracy = 0.0925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 26.406808853149414, Accuracy = 0.1442786008119583
Training iter #2000:   Batch Loss = 25.328739, Accuracy = 0.2849999964237213
PERFORMANCE ON TEST SET: Batch Loss = 25.00161361694336, Accuracy = 0.34017413854599
Training iter #4000:   Batch Loss = 23.675880, Accuracy = 0.4124999940395355
PERFORMANCE ON TEST SET: Batch Loss = 23.219507217407227, Accuracy = 0.4514925479888916
Training iter #6000:   Batch Loss = 22.055393, Accuracy = 0.4099999964237213
PERFORMANCE ON TEST SET: Batch Loss = 21.530467987060547, Accuracy = 0.44340795278549194
Training iter #8000:   Batch Loss = 20.474375, Accuracy = 0.4675000011920929
PERFORMANCE ON TEST SET: Batch Loss = 20.161279678344727, Accuracy = 0.49191543459892273
Training iter #10000:   Batch Loss = 19.429258, Accuracy = 0.5400000214576721
PERFORMANCE ON TEST SET: Batch Loss = 19.026487350463867, Accuracy = 0.5758706331253052
Training iter #12000:   Batch Loss = 18.478979, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 17.960712432861328, Accuracy = 0.6026119589805603
Training iter #14000:   Batch Loss = 17.081480, Accuracy = 0.6200000047683716
PERFORMANCE ON TEST SET: Batch Loss = 17.26337432861328, Accuracy = 0.6106964945793152
Training iter #16000:   Batch Loss = 16.938629, Accuracy = 0.6050000190734863
PERFORMANCE ON TEST SET: Batch Loss = 16.780929565429688, Accuracy = 0.6461442708969116
Training iter #18000:   Batch Loss = 16.289764, Accuracy = 0.6575000286102295
PERFORMANCE ON TEST SET: Batch Loss = 16.270414352416992, Accuracy = 0.6623134613037109
Training iter #20000:   Batch Loss = 16.487284, Accuracy = 0.6324999928474426
PERFORMANCE ON TEST SET: Batch Loss = 16.219161987304688, Accuracy = 0.6629353165626526
Training iter #22000:   Batch Loss = 15.678730, Accuracy = 0.7124999761581421
PERFORMANCE ON TEST SET: Batch Loss = 15.748156547546387, Accuracy = 0.6728855967521667
Training iter #24000:   Batch Loss = 15.615692, Accuracy = 0.6924999952316284
PERFORMANCE ON TEST SET: Batch Loss = 15.246994972229004, Accuracy = 0.7070895433425903
Training iter #26000:   Batch Loss = 15.423689, Accuracy = 0.6875
PERFORMANCE ON TEST SET: Batch Loss = 14.955888748168945, Accuracy = 0.7195273637771606
Training iter #28000:   Batch Loss = 14.719398, Accuracy = 0.737500011920929
PERFORMANCE ON TEST SET: Batch Loss = 14.611255645751953, Accuracy = 0.7189054489135742
Training iter #30000:   Batch Loss = 14.353369, Accuracy = 0.7225000262260437
PERFORMANCE ON TEST SET: Batch Loss = 14.396581649780273, Accuracy = 0.7313432693481445
Training iter #32000:   Batch Loss = 14.096312, Accuracy = 0.7400000095367432
PERFORMANCE ON TEST SET: Batch Loss = 14.129919052124023, Accuracy = 0.7356964945793152
Training iter #34000:   Batch Loss = 14.665208, Accuracy = 0.7777777910232544
PERFORMANCE ON TEST SET: Batch Loss = 14.197263717651367, Accuracy = 0.7388059496879578
Training iter #36000:   Batch Loss = 13.743067, Accuracy = 0.7200000286102295
PERFORMANCE ON TEST SET: Batch Loss = 14.12975788116455, Accuracy = 0.7356964945793152
Training iter #38000:   Batch Loss = 13.610143, Accuracy = 0.7674999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.872213363647461, Accuracy = 0.7543532252311707
Training iter #40000:   Batch Loss = 13.470130, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.944172859191895, Accuracy = 0.7611940503120422
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-100
Training iter #42000:   Batch Loss = 13.325997, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.75330638885498, Accuracy = 0.7549751400947571
Training iter #44000:   Batch Loss = 12.958347, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 13.755151748657227, Accuracy = 0.7580845952033997
Training iter #46000:   Batch Loss = 13.406165, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 13.492532730102539, Accuracy = 0.7580845952033997
Training iter #48000:   Batch Loss = 13.198534, Accuracy = 0.7599999904632568
PERFORMANCE ON TEST SET: Batch Loss = 13.59164047241211, Accuracy = 0.7674129605293274
Training iter #50000:   Batch Loss = 12.645626, Accuracy = 0.7950000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.289519309997559, Accuracy = 0.763059675693512
Training iter #52000:   Batch Loss = 13.297779, Accuracy = 0.7549999952316284
PERFORMANCE ON TEST SET: Batch Loss = 13.298879623413086, Accuracy = 0.7748756408691406
Training iter #54000:   Batch Loss = 12.708731, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.972987174987793, Accuracy = 0.7810945510864258
Training iter #56000:   Batch Loss = 12.989660, Accuracy = 0.7649999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.87247085571289, Accuracy = 0.7711442708969116
Training iter #58000:   Batch Loss = 12.305800, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.583718299865723, Accuracy = 0.7916666865348816
Training iter #60000:   Batch Loss = 12.275511, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.253178596496582, Accuracy = 0.7891790866851807
Training iter #62000:   Batch Loss = 11.231694, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.51650333404541, Accuracy = 0.7916666865348816
Training iter #64000:   Batch Loss = 12.209317, Accuracy = 0.7850000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.190886497497559, Accuracy = 0.7860696315765381
Training iter #66000:   Batch Loss = 11.922953, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.208794593811035, Accuracy = 0.801616907119751
Training iter #68000:   Batch Loss = 10.973839, Accuracy = 0.8888888955116272
PERFORMANCE ON TEST SET: Batch Loss = 12.199617385864258, Accuracy = 0.7985074520111084
Training iter #70000:   Batch Loss = 11.878343, Accuracy = 0.7749999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.191506385803223, Accuracy = 0.8078358173370361
Training iter #72000:   Batch Loss = 11.983513, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.89681339263916, Accuracy = 0.8065920472145081
Training iter #74000:   Batch Loss = 11.063070, Accuracy = 0.8174999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.843637466430664, Accuracy = 0.7953979969024658
Training iter #76000:   Batch Loss = 11.042861, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.863456726074219, Accuracy = 0.8003731369972229
Training iter #78000:   Batch Loss = 11.481588, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.69849681854248, Accuracy = 0.8121890425682068
Training iter #80000:   Batch Loss = 11.714003, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.068264961242676, Accuracy = 0.802860677242279
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-200
Training iter #82000:   Batch Loss = 11.918707, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 13.14749813079834, Accuracy = 0.7723880410194397
Training iter #84000:   Batch Loss = 12.103045, Accuracy = 0.8199999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.49009895324707, Accuracy = 0.8009950518608093
Training iter #86000:   Batch Loss = 12.289003, Accuracy = 0.7925000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.072280883789062, Accuracy = 0.8047263622283936
Training iter #88000:   Batch Loss = 11.920092, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.234600067138672, Accuracy = 0.8184079527854919
Training iter #90000:   Batch Loss = 12.426309, Accuracy = 0.8324999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.842375755310059, Accuracy = 0.8177860975265503
Training iter #92000:   Batch Loss = 12.113850, Accuracy = 0.824999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.348806381225586, Accuracy = 0.8184079527854919
Training iter #94000:   Batch Loss = 11.501665, Accuracy = 0.8424999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.491216659545898, Accuracy = 0.8121890425682068
Training iter #96000:   Batch Loss = 11.770753, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.303352355957031, Accuracy = 0.80534827709198
Training iter #98000:   Batch Loss = 11.942239, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.261691093444824, Accuracy = 0.8090795874595642
Training iter #100000:   Batch Loss = 11.781761, Accuracy = 0.8075000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.075974464416504, Accuracy = 0.8121890425682068
Training iter #102000:   Batch Loss = 11.348390, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.211247444152832, Accuracy = 0.8121890425682068
Training iter #104000:   Batch Loss = 12.142462, Accuracy = 0.8349999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.408103942871094, Accuracy = 0.8128109574317932
Training iter #106000:   Batch Loss = 11.738096, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.418983459472656, Accuracy = 0.823383092880249
Training iter #108000:   Batch Loss = 12.771514, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.925060272216797, Accuracy = 0.8152984976768494
Training iter #110000:   Batch Loss = 11.991549, Accuracy = 0.8274999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.532387733459473, Accuracy = 0.8271144032478333
Training iter #112000:   Batch Loss = 11.720903, Accuracy = 0.8475000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.267382621765137, Accuracy = 0.8358209133148193
Training iter #114000:   Batch Loss = 11.830566, Accuracy = 0.7975000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.040840148925781, Accuracy = 0.823383092880249
Training iter #116000:   Batch Loss = 11.624438, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.168510437011719, Accuracy = 0.8240049481391907
Training iter #118000:   Batch Loss = 11.344960, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.028848648071289, Accuracy = 0.8208954930305481
Training iter #120000:   Batch Loss = 11.768332, Accuracy = 0.8149999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.952156066894531, Accuracy = 0.8296020030975342
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-300
Training iter #122000:   Batch Loss = 10.938386, Accuracy = 0.8399999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.569411277770996, Accuracy = 0.8320895433425903
Training iter #124000:   Batch Loss = 11.120317, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.698291778564453, Accuracy = 0.8302238583564758
Training iter #126000:   Batch Loss = 10.656441, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.7167329788208, Accuracy = 0.8327114582061768
Training iter #128000:   Batch Loss = 11.635895, Accuracy = 0.8100000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.838789939880371, Accuracy = 0.8277363181114197
Training iter #130000:   Batch Loss = 11.806623, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.450606346130371, Accuracy = 0.8258706331253052
Training iter #132000:   Batch Loss = 11.496251, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.197710037231445, Accuracy = 0.8320895433425903
Training iter #134000:   Batch Loss = 11.921476, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.163517951965332, Accuracy = 0.8432835936546326
Training iter #136000:   Batch Loss = 12.191206, Accuracy = 0.8444444537162781
PERFORMANCE ON TEST SET: Batch Loss = 12.000941276550293, Accuracy = 0.8439054489135742
Training iter #138000:   Batch Loss = 11.034832, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.176233291625977, Accuracy = 0.8327114582061768
Training iter #140000:   Batch Loss = 11.416866, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 12.141980171203613, Accuracy = 0.8296020030975342
Training iter #142000:   Batch Loss = 11.630789, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.082433700561523, Accuracy = 0.8202736377716064
Training iter #144000:   Batch Loss = 11.824062, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.473115921020508, Accuracy = 0.8184079527854919
Training iter #146000:   Batch Loss = 11.596642, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.13632869720459, Accuracy = 0.8264925479888916
Training iter #148000:   Batch Loss = 11.604354, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.029674530029297, Accuracy = 0.8215174078941345
Training iter #150000:   Batch Loss = 11.083419, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.871124267578125, Accuracy = 0.8258706331253052
Training iter #152000:   Batch Loss = 10.756394, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.76607608795166, Accuracy = 0.8420398235321045
Training iter #154000:   Batch Loss = 11.071890, Accuracy = 0.8550000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.68559741973877, Accuracy = 0.8308457732200623
Training iter #156000:   Batch Loss = 11.818455, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.21580696105957, Accuracy = 0.8420398235321045
Training iter #158000:   Batch Loss = 11.060823, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.903263092041016, Accuracy = 0.8451492786407471
Training iter #160000:   Batch Loss = 10.937302, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.7778959274292, Accuracy = 0.8432835936546326
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-400
Training iter #162000:   Batch Loss = 11.016378, Accuracy = 0.8525000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.606355667114258, Accuracy = 0.8383084535598755
Training iter #164000:   Batch Loss = 11.272978, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.011380195617676, Accuracy = 0.8252487778663635
Training iter #166000:   Batch Loss = 10.785048, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.70374870300293, Accuracy = 0.84017413854599
Training iter #168000:   Batch Loss = 10.687946, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.598658561706543, Accuracy = 0.8426616787910461
Training iter #170000:   Batch Loss = 10.442914, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 11.53629207611084, Accuracy = 0.8488805890083313
Training iter #172000:   Batch Loss = 11.070200, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.761066436767578, Accuracy = 0.8389303684234619
Training iter #174000:   Batch Loss = 11.229338, Accuracy = 0.8575000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.56906509399414, Accuracy = 0.8501243591308594
Training iter #176000:   Batch Loss = 10.607996, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.525914192199707, Accuracy = 0.8588308691978455
Training iter #178000:   Batch Loss = 11.067966, Accuracy = 0.8450000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.544363975524902, Accuracy = 0.8513681888580322
Training iter #180000:   Batch Loss = 10.583911, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.385315895080566, Accuracy = 0.8383084535598755
Training iter #182000:   Batch Loss = 10.454082, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.262004852294922, Accuracy = 0.8488805890083313
Training iter #184000:   Batch Loss = 10.679962, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 12.179865837097168, Accuracy = 0.8327114582061768
Training iter #186000:   Batch Loss = 11.336508, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.169612884521484, Accuracy = 0.8445273637771606
Training iter #188000:   Batch Loss = 11.255616, Accuracy = 0.862500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.78654956817627, Accuracy = 0.8538557291030884
Training iter #190000:   Batch Loss = 10.758233, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.108118057250977, Accuracy = 0.8526119589805603
Training iter #192000:   Batch Loss = 11.949891, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.72950553894043, Accuracy = 0.8470149040222168
Training iter #194000:   Batch Loss = 11.343087, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.378973007202148, Accuracy = 0.8476368188858032
Training iter #196000:   Batch Loss = 11.569815, Accuracy = 0.8725000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.162088394165039, Accuracy = 0.8563432693481445
Training iter #198000:   Batch Loss = 11.703471, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.101855278015137, Accuracy = 0.8550994992256165
Training iter #200000:   Batch Loss = 10.882772, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.832182884216309, Accuracy = 0.8495025038719177
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-500
Training iter #202000:   Batch Loss = 11.472353, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.747907638549805, Accuracy = 0.8557214140892029
Training iter #204000:   Batch Loss = 12.127144, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.061060905456543, Accuracy = 0.8383084535598755
Training iter #206000:   Batch Loss = 11.513733, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.188755989074707, Accuracy = 0.856965184211731
Training iter #208000:   Batch Loss = 11.123486, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.935914039611816, Accuracy = 0.8507462739944458
Training iter #210000:   Batch Loss = 11.178553, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.822336196899414, Accuracy = 0.858208954334259
Training iter #212000:   Batch Loss = 11.429356, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.804837226867676, Accuracy = 0.858208954334259
Training iter #214000:   Batch Loss = 10.884100, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.559730529785156, Accuracy = 0.8575870394706726
Training iter #216000:   Batch Loss = 10.827826, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.325614929199219, Accuracy = 0.8557214140892029
Training iter #218000:   Batch Loss = 10.687025, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.787347793579102, Accuracy = 0.8463930487632751
Training iter #220000:   Batch Loss = 10.879448, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.80721664428711, Accuracy = 0.8538557291030884
Training iter #222000:   Batch Loss = 10.728037, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.572502136230469, Accuracy = 0.8519900441169739
Training iter #224000:   Batch Loss = 10.829491, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.491281509399414, Accuracy = 0.861940324306488
Training iter #226000:   Batch Loss = 10.734891, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.529476165771484, Accuracy = 0.8588308691978455
Training iter #228000:   Batch Loss = 10.926600, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.406785011291504, Accuracy = 0.8625621795654297
Training iter #230000:   Batch Loss = 10.724922, Accuracy = 0.8799999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.273834228515625, Accuracy = 0.8756219148635864
Training iter #232000:   Batch Loss = 10.362030, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.673501014709473, Accuracy = 0.85447758436203
Training iter #234000:   Batch Loss = 10.643543, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.413414001464844, Accuracy = 0.861940324306488
Training iter #236000:   Batch Loss = 10.496993, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.291178703308105, Accuracy = 0.8681591749191284
Training iter #238000:   Batch Loss = 10.063763, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.61927604675293, Accuracy = 0.8519900441169739
Training iter #240000:   Batch Loss = 10.477878, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.3785400390625, Accuracy = 0.8526119589805603
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-600
Training iter #242000:   Batch Loss = 10.579949, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.287199020385742, Accuracy = 0.8631840944290161
Training iter #244000:   Batch Loss = 10.977375, Accuracy = 0.8700000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.290210723876953, Accuracy = 0.8588308691978455
Training iter #246000:   Batch Loss = 10.616270, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.439184188842773, Accuracy = 0.8644278645515442
Training iter #248000:   Batch Loss = 10.380242, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.53217601776123, Accuracy = 0.8575870394706726
Training iter #250000:   Batch Loss = 10.583373, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.344831466674805, Accuracy = 0.8563432693481445
Training iter #252000:   Batch Loss = 10.433053, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.525623321533203, Accuracy = 0.8513681888580322
Training iter #254000:   Batch Loss = 10.636359, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.20100212097168, Accuracy = 0.8650497794151306
Training iter #256000:   Batch Loss = 11.436584, Accuracy = 0.8500000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.031789779663086, Accuracy = 0.8613184094429016
Training iter #258000:   Batch Loss = 9.877807, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.903037071228027, Accuracy = 0.8737562298774719
Training iter #260000:   Batch Loss = 10.690103, Accuracy = 0.8600000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.070499420166016, Accuracy = 0.8644278645515442
Training iter #262000:   Batch Loss = 9.722120, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.907625198364258, Accuracy = 0.8731343150138855
Training iter #264000:   Batch Loss = 10.311082, Accuracy = 0.8774999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.788474082946777, Accuracy = 0.8669154047966003
Training iter #266000:   Batch Loss = 9.958927, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.854162216186523, Accuracy = 0.8694030046463013
Training iter #268000:   Batch Loss = 10.022358, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.388899803161621, Accuracy = 0.8762437701225281
Training iter #270000:   Batch Loss = 10.551877, Accuracy = 0.8849999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.758585929870605, Accuracy = 0.8638059496879578
Training iter #272000:   Batch Loss = 11.870095, Accuracy = 0.8444444537162781
PERFORMANCE ON TEST SET: Batch Loss = 11.622345924377441, Accuracy = 0.8606964945793152
Training iter #274000:   Batch Loss = 11.029544, Accuracy = 0.875
PERFORMANCE ON TEST SET: Batch Loss = 11.765619277954102, Accuracy = 0.8594527244567871
Training iter #276000:   Batch Loss = 10.892908, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.670623779296875, Accuracy = 0.8600746393203735
Training iter #278000:   Batch Loss = 10.683926, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.409846305847168, Accuracy = 0.8725124597549438
Training iter #280000:   Batch Loss = 11.038614, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.519021987915039, Accuracy = 0.8625621795654297
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-700
Training iter #282000:   Batch Loss = 10.519058, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.342891693115234, Accuracy = 0.8725124597549438
Training iter #284000:   Batch Loss = 10.461885, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.30045223236084, Accuracy = 0.8681591749191284
Training iter #286000:   Batch Loss = 9.702950, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.149195671081543, Accuracy = 0.8669154047966003
Training iter #288000:   Batch Loss = 10.466467, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.426312446594238, Accuracy = 0.878731369972229
Training iter #290000:   Batch Loss = 10.235904, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.115095138549805, Accuracy = 0.8743780851364136
Training iter #292000:   Batch Loss = 10.154303, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.027191162109375, Accuracy = 0.8700248599052429
Training iter #294000:   Batch Loss = 10.382236, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.125299453735352, Accuracy = 0.871268630027771
Training iter #296000:   Batch Loss = 9.921227, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.993025779724121, Accuracy = 0.8718905448913574
Training iter #298000:   Batch Loss = 10.151409, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.907546997070312, Accuracy = 0.8805969953536987
Training iter #300000:   Batch Loss = 10.632393, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.25549602508545, Accuracy = 0.875
Training iter #302000:   Batch Loss = 9.776564, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.009186744689941, Accuracy = 0.8737562298774719
Training iter #304000:   Batch Loss = 10.016225, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.802749633789062, Accuracy = 0.8756219148635864
Training iter #306000:   Batch Loss = 9.485123, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 10.937108993530273, Accuracy = 0.8799751400947571
Training iter #308000:   Batch Loss = 10.078527, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.994239807128906, Accuracy = 0.8731343150138855
Training iter #310000:   Batch Loss = 9.980391, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.12407398223877, Accuracy = 0.871268630027771
Training iter #312000:   Batch Loss = 9.802137, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.89063835144043, Accuracy = 0.8675373196601868
Training iter #314000:   Batch Loss = 10.583159, Accuracy = 0.8675000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.282962799072266, Accuracy = 0.8575870394706726
Training iter #316000:   Batch Loss = 10.094662, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.939854621887207, Accuracy = 0.8644278645515442
Training iter #318000:   Batch Loss = 9.896162, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.881185531616211, Accuracy = 0.8675373196601868
Training iter #320000:   Batch Loss = 9.876788, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.86044979095459, Accuracy = 0.8675373196601868
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-800
Training iter #322000:   Batch Loss = 9.553681, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.674846649169922, Accuracy = 0.8793532252311707
Training iter #324000:   Batch Loss = 10.375903, Accuracy = 0.8650000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.699270248413086, Accuracy = 0.8706467747688293
Training iter #326000:   Batch Loss = 9.966480, Accuracy = 0.8899999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.609167098999023, Accuracy = 0.8818408250808716
Training iter #328000:   Batch Loss = 9.583160, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.805295944213867, Accuracy = 0.8737562298774719
Training iter #330000:   Batch Loss = 9.786281, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.82659912109375, Accuracy = 0.875
Training iter #332000:   Batch Loss = 9.709738, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.011634826660156, Accuracy = 0.8762437701225281
Training iter #334000:   Batch Loss = 10.267473, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.997580528259277, Accuracy = 0.8743780851364136
Training iter #336000:   Batch Loss = 9.916443, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.08836841583252, Accuracy = 0.8743780851364136
Training iter #338000:   Batch Loss = 9.928318, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.124367713928223, Accuracy = 0.8706467747688293
Training iter #340000:   Batch Loss = 11.966732, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.047985076904297, Accuracy = 0.8681591749191284
Training iter #342000:   Batch Loss = 10.223609, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.346725463867188, Accuracy = 0.8606964945793152
Training iter #344000:   Batch Loss = 9.625382, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.091445922851562, Accuracy = 0.8613184094429016
Training iter #346000:   Batch Loss = 10.291492, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.992969512939453, Accuracy = 0.858208954334259
Training iter #348000:   Batch Loss = 10.128994, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.949102401733398, Accuracy = 0.8743780851364136
Training iter #350000:   Batch Loss = 9.989595, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.774375915527344, Accuracy = 0.871268630027771
Training iter #352000:   Batch Loss = 9.949291, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.76612663269043, Accuracy = 0.8650497794151306
Training iter #354000:   Batch Loss = 9.422944, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.688589096069336, Accuracy = 0.8756219148635864
Training iter #356000:   Batch Loss = 10.017096, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.089460372924805, Accuracy = 0.8774875402450562
Training iter #358000:   Batch Loss = 10.321918, Accuracy = 0.8974999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.954713821411133, Accuracy = 0.875
Training iter #360000:   Batch Loss = 9.422020, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.824079513549805, Accuracy = 0.8737562298774719
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-900
Training iter #362000:   Batch Loss = 10.065088, Accuracy = 0.8824999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.85352897644043, Accuracy = 0.8669154047966003
Training iter #364000:   Batch Loss = 9.491892, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.733540534973145, Accuracy = 0.8656716346740723
Training iter #366000:   Batch Loss = 9.713734, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.594764709472656, Accuracy = 0.8830845952033997
Training iter #368000:   Batch Loss = 10.038787, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.200139999389648, Accuracy = 0.8600746393203735
Training iter #370000:   Batch Loss = 9.941112, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.958498001098633, Accuracy = 0.8718905448913574
Training iter #372000:   Batch Loss = 9.906718, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.84563159942627, Accuracy = 0.8743780851364136
Training iter #374000:   Batch Loss = 9.992766, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 10.55949592590332, Accuracy = 0.8818408250808716
Training iter #376000:   Batch Loss = 9.967552, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.072078704833984, Accuracy = 0.8849502205848694
Training iter #378000:   Batch Loss = 9.898442, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.094791412353516, Accuracy = 0.8743780851364136
Training iter #380000:   Batch Loss = 9.564337, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.800379753112793, Accuracy = 0.878731369972229
Training iter #382000:   Batch Loss = 10.095369, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.150107383728027, Accuracy = 0.8681591749191284
Training iter #384000:   Batch Loss = 10.729832, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.359006881713867, Accuracy = 0.8762437701225281
Training iter #386000:   Batch Loss = 10.034731, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.028464317321777, Accuracy = 0.8818408250808716
Training iter #388000:   Batch Loss = 10.078150, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.191329002380371, Accuracy = 0.875
Training iter #390000:   Batch Loss = 10.592125, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.501445770263672, Accuracy = 0.8818408250808716
Training iter #392000:   Batch Loss = 10.786045, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.329187393188477, Accuracy = 0.8762437701225281
Training iter #394000:   Batch Loss = 10.633438, Accuracy = 0.8924999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.407803535461426, Accuracy = 0.8638059496879578
Training iter #396000:   Batch Loss = 10.127990, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.140045166015625, Accuracy = 0.8694030046463013
Training iter #398000:   Batch Loss = 10.202796, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.029215812683105, Accuracy = 0.8725124597549438
Training iter #400000:   Batch Loss = 9.810418, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.884926795959473, Accuracy = 0.8718905448913574
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1000
Training iter #402000:   Batch Loss = 9.958280, Accuracy = 0.887499988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.012197494506836, Accuracy = 0.8675373196601868
Training iter #404000:   Batch Loss = 10.124758, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.211048126220703, Accuracy = 0.8700248599052429
Training iter #406000:   Batch Loss = 10.229273, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.006779670715332, Accuracy = 0.8774875402450562
Training iter #408000:   Batch Loss = 10.996958, Accuracy = 0.8666666746139526
PERFORMANCE ON TEST SET: Batch Loss = 10.872666358947754, Accuracy = 0.8650497794151306
Training iter #410000:   Batch Loss = 10.226171, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.00049877166748, Accuracy = 0.8675373196601868
Training iter #412000:   Batch Loss = 9.559355, Accuracy = 0.9049999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.761107444763184, Accuracy = 0.8768656849861145
Training iter #414000:   Batch Loss = 9.431404, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.623920440673828, Accuracy = 0.8774875402450562
Training iter #416000:   Batch Loss = 9.563584, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.59560489654541, Accuracy = 0.8694030046463013
Training iter #418000:   Batch Loss = 9.851730, Accuracy = 0.8999999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.474191665649414, Accuracy = 0.8837064504623413
Training iter #420000:   Batch Loss = 9.234919, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.436758995056152, Accuracy = 0.8855721354484558
Training iter #422000:   Batch Loss = 9.519890, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.351285934448242, Accuracy = 0.8849502205848694
Training iter #424000:   Batch Loss = 9.098759, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.409423828125, Accuracy = 0.8893035054206848
Training iter #426000:   Batch Loss = 9.168163, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.448144912719727, Accuracy = 0.8812189102172852
Training iter #428000:   Batch Loss = 9.586767, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.49007797241211, Accuracy = 0.8793532252311707
Training iter #430000:   Batch Loss = 9.216417, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.635017395019531, Accuracy = 0.8731343150138855
Training iter #432000:   Batch Loss = 9.105914, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.484894752502441, Accuracy = 0.8743780851364136
Training iter #434000:   Batch Loss = 9.535385, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.648049354553223, Accuracy = 0.8837064504623413
Training iter #436000:   Batch Loss = 9.462768, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.684610366821289, Accuracy = 0.878731369972229
Training iter #438000:   Batch Loss = 9.741908, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.547452926635742, Accuracy = 0.8805969953536987
Training iter #440000:   Batch Loss = 9.819551, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.568263053894043, Accuracy = 0.8837064504623413
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1100
Training iter #442000:   Batch Loss = 9.234988, Accuracy = 0.8444444537162781
PERFORMANCE ON TEST SET: Batch Loss = 10.638468742370605, Accuracy = 0.8799751400947571
Training iter #444000:   Batch Loss = 9.244476, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.86092758178711, Accuracy = 0.8818408250808716
Training iter #446000:   Batch Loss = 9.479351, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.603473663330078, Accuracy = 0.8824626803398132
Training iter #448000:   Batch Loss = 10.077514, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.968361854553223, Accuracy = 0.8781094551086426
Training iter #450000:   Batch Loss = 9.396143, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 10.968779563903809, Accuracy = 0.8781094551086426
Training iter #452000:   Batch Loss = 9.758316, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 10.94508171081543, Accuracy = 0.8905472755432129
Training iter #454000:   Batch Loss = 10.026746, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 10.986297607421875, Accuracy = 0.8849502205848694
Training iter #456000:   Batch Loss = 9.890407, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.80069351196289, Accuracy = 0.8774875402450562
Training iter #458000:   Batch Loss = 10.082423, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.222880363464355, Accuracy = 0.888059675693512
Training iter #460000:   Batch Loss = 9.881640, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.951847076416016, Accuracy = 0.8774875402450562
Training iter #462000:   Batch Loss = 9.939999, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.884366989135742, Accuracy = 0.878731369972229
Training iter #464000:   Batch Loss = 10.102411, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.176827430725098, Accuracy = 0.8793532252311707
Training iter #466000:   Batch Loss = 9.382354, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.914713859558105, Accuracy = 0.8905472755432129
Training iter #468000:   Batch Loss = 9.298047, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.669570922851562, Accuracy = 0.8899253606796265
Training iter #470000:   Batch Loss = 9.222184, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.790253639221191, Accuracy = 0.8861940503120422
Training iter #472000:   Batch Loss = 9.568567, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.678962707519531, Accuracy = 0.8824626803398132
Training iter #474000:   Batch Loss = 9.567188, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.615530014038086, Accuracy = 0.8837064504623413
Training iter #476000:   Batch Loss = 9.105570, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 10.657581329345703, Accuracy = 0.893034815788269
Training iter #478000:   Batch Loss = 9.890814, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.649491310119629, Accuracy = 0.8893035054206848
Training iter #480000:   Batch Loss = 9.951921, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.897054672241211, Accuracy = 0.871268630027771
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1200
Training iter #482000:   Batch Loss = 10.177802, Accuracy = 0.9024999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.329060554504395, Accuracy = 0.8756219148635864
Training iter #484000:   Batch Loss = 9.954433, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.296110153198242, Accuracy = 0.8687810897827148
Training iter #486000:   Batch Loss = 9.755342, Accuracy = 0.9125000238418579WARNING:tensorflow:From /home/sunrepe/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to delete files with this prefix.

PERFORMANCE ON TEST SET: Batch Loss = 11.107185363769531, Accuracy = 0.871268630027771
Training iter #488000:   Batch Loss = 9.353294, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 10.86827564239502, Accuracy = 0.871268630027771
Training iter #490000:   Batch Loss = 12.346784, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 13.45592212677002, Accuracy = 0.8700248599052429
Training iter #492000:   Batch Loss = 12.061085, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.914631843566895, Accuracy = 0.8781094551086426
Training iter #494000:   Batch Loss = 11.426012, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.357440948486328, Accuracy = 0.8793532252311707
Training iter #496000:   Batch Loss = 11.481722, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.545652389526367, Accuracy = 0.8830845952033997
Training iter #498000:   Batch Loss = 11.432179, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.620851516723633, Accuracy = 0.8700248599052429
Training iter #500000:   Batch Loss = 11.268714, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.167699813842773, Accuracy = 0.8911691308021545
Training iter #502000:   Batch Loss = 11.125195, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.983670234680176, Accuracy = 0.8868159055709839
Training iter #504000:   Batch Loss = 11.058199, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 12.020095825195312, Accuracy = 0.8874378204345703
Training iter #506000:   Batch Loss = 10.854419, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.950382232666016, Accuracy = 0.8824626803398132
Training iter #508000:   Batch Loss = 10.489463, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.607837677001953, Accuracy = 0.89552241563797
Training iter #510000:   Batch Loss = 10.846092, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 11.714707374572754, Accuracy = 0.8868159055709839
Training iter #512000:   Batch Loss = 10.420961, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.812945365905762, Accuracy = 0.8805969953536987
Training iter #514000:   Batch Loss = 10.575564, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.720057487487793, Accuracy = 0.8899253606796265
Training iter #516000:   Batch Loss = 10.718948, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.589465141296387, Accuracy = 0.896766185760498
Training iter #518000:   Batch Loss = 10.708513, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.736507415771484, Accuracy = 0.8874378204345703
Training iter #520000:   Batch Loss = 10.623068, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.68196964263916, Accuracy = 0.8886815905570984
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1300
Training iter #522000:   Batch Loss = 10.536821, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.551243782043457, Accuracy = 0.8861940503120422
Training iter #524000:   Batch Loss = 10.574653, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.585769653320312, Accuracy = 0.8949005007743835
Training iter #526000:   Batch Loss = 10.482609, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.600732803344727, Accuracy = 0.8868159055709839
Training iter #528000:   Batch Loss = 10.628205, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.959135055541992, Accuracy = 0.8924129605293274
Training iter #530000:   Batch Loss = 10.323411, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.72288990020752, Accuracy = 0.8961442708969116
Training iter #532000:   Batch Loss = 11.210132, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.085607528686523, Accuracy = 0.8905472755432129
Training iter #534000:   Batch Loss = 10.401350, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.654693603515625, Accuracy = 0.8911691308021545
Training iter #536000:   Batch Loss = 10.450670, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.695245742797852, Accuracy = 0.8905472755432129
Training iter #538000:   Batch Loss = 10.727931, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.423707962036133, Accuracy = 0.8781094551086426
Training iter #540000:   Batch Loss = 11.686994, Accuracy = 0.9075000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.521248817443848, Accuracy = 0.8793532252311707
Training iter #542000:   Batch Loss = 11.045356, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.141963958740234, Accuracy = 0.8886815905570984
Training iter #544000:   Batch Loss = 10.239792, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 11.993165969848633, Accuracy = 0.8843283653259277
Training iter #546000:   Batch Loss = 10.589664, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.897052764892578, Accuracy = 0.8849502205848694
Training iter #548000:   Batch Loss = 10.765930, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.059782028198242, Accuracy = 0.8911691308021545
Training iter #550000:   Batch Loss = 11.038217, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.62092399597168, Accuracy = 0.8936567306518555
Training iter #552000:   Batch Loss = 10.141521, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.794676780700684, Accuracy = 0.8899253606796265
Training iter #554000:   Batch Loss = 10.988961, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 12.081073760986328, Accuracy = 0.8762437701225281
Training iter #556000:   Batch Loss = 11.257040, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.0472412109375, Accuracy = 0.8911691308021545
Training iter #558000:   Batch Loss = 10.878543, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.351318359375, Accuracy = 0.8855721354484558
Training iter #560000:   Batch Loss = 11.170324, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.992815971374512, Accuracy = 0.8980099558830261
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1400
Training iter #562000:   Batch Loss = 10.216644, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.839611053466797, Accuracy = 0.8874378204345703
Training iter #564000:   Batch Loss = 10.725493, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.894455909729004, Accuracy = 0.8706467747688293
Training iter #566000:   Batch Loss = 10.257690, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.807040214538574, Accuracy = 0.8861940503120422
Training iter #568000:   Batch Loss = 10.453971, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.615205764770508, Accuracy = 0.8936567306518555
Training iter #570000:   Batch Loss = 10.944385, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.511306762695312, Accuracy = 0.8961442708969116
Training iter #572000:   Batch Loss = 10.283027, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.443153381347656, Accuracy = 0.8911691308021545
Training iter #574000:   Batch Loss = 9.912660, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.391345977783203, Accuracy = 0.893034815788269
Training iter #576000:   Batch Loss = 10.643820, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.347023010253906, Accuracy = 0.8992537260055542
Training iter #578000:   Batch Loss = 10.747640, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.554146766662598, Accuracy = 0.8905472755432129
Training iter #580000:   Batch Loss = 10.358109, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.503345489501953, Accuracy = 0.8899253606796265
Training iter #582000:   Batch Loss = 10.126945, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.438817024230957, Accuracy = 0.8768656849861145
Training iter #584000:   Batch Loss = 9.781428, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.283735275268555, Accuracy = 0.8868159055709839
Training iter #586000:   Batch Loss = 10.194365, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.229838371276855, Accuracy = 0.8855721354484558
Training iter #588000:   Batch Loss = 9.739471, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.12796401977539, Accuracy = 0.8942785859107971
Training iter #590000:   Batch Loss = 10.333724, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.648056983947754, Accuracy = 0.8837064504623413
Training iter #592000:   Batch Loss = 10.807217, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.001420021057129, Accuracy = 0.8781094551086426
Training iter #594000:   Batch Loss = 10.151009, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.826345443725586, Accuracy = 0.8837064504623413
Training iter #596000:   Batch Loss = 10.702435, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.578882217407227, Accuracy = 0.8924129605293274
Training iter #598000:   Batch Loss = 10.202042, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.57571029663086, Accuracy = 0.8924129605293274
Training iter #600000:   Batch Loss = 10.334176, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.660989761352539, Accuracy = 0.9023631811141968
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1500
Training iter #602000:   Batch Loss = 10.573024, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.729246139526367, Accuracy = 0.893034815788269
Training iter #604000:   Batch Loss = 10.121346, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.32107162475586, Accuracy = 0.8893035054206848
Training iter #606000:   Batch Loss = 9.921757, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.562318801879883, Accuracy = 0.8830845952033997
Training iter #608000:   Batch Loss = 10.147240, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.322356224060059, Accuracy = 0.89552241563797
Training iter #610000:   Batch Loss = 9.871182, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.177717208862305, Accuracy = 0.8980099558830261
Training iter #612000:   Batch Loss = 9.575508, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.240346908569336, Accuracy = 0.891791045665741
Training iter #614000:   Batch Loss = 9.724757, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.160468101501465, Accuracy = 0.8942785859107971
Training iter #616000:   Batch Loss = 9.852868, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.964746475219727, Accuracy = 0.89552241563797
Training iter #618000:   Batch Loss = 10.211817, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.606802940368652, Accuracy = 0.9011194109916687
Training iter #620000:   Batch Loss = 10.045020, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.613642692565918, Accuracy = 0.8886815905570984
Training iter #622000:   Batch Loss = 10.021234, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.346202850341797, Accuracy = 0.8924129605293274
Training iter #624000:   Batch Loss = 9.874559, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.313161849975586, Accuracy = 0.89552241563797
Training iter #626000:   Batch Loss = 9.761867, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.09847640991211, Accuracy = 0.9023631811141968
Training iter #628000:   Batch Loss = 11.006294, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.508779525756836, Accuracy = 0.8911691308021545
Training iter #630000:   Batch Loss = 10.822390, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.236281394958496, Accuracy = 0.893034815788269
Training iter #632000:   Batch Loss = 10.936032, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.063589096069336, Accuracy = 0.9029850959777832
Training iter #634000:   Batch Loss = 10.806026, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.238664627075195, Accuracy = 0.8986318111419678
Training iter #636000:   Batch Loss = 10.334144, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.984145164489746, Accuracy = 0.893034815788269
Training iter #638000:   Batch Loss = 10.981243, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.958067893981934, Accuracy = 0.9048507213592529
Training iter #640000:   Batch Loss = 10.753817, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.045583724975586, Accuracy = 0.9004974961280823
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1600
Training iter #642000:   Batch Loss = 10.532706, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.846128463745117, Accuracy = 0.89552241563797
Training iter #644000:   Batch Loss = 10.560739, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.66490364074707, Accuracy = 0.8924129605293274
Training iter #646000:   Batch Loss = 12.844205, Accuracy = 0.8222222328186035
PERFORMANCE ON TEST SET: Batch Loss = 12.474868774414062, Accuracy = 0.8861940503120422
Training iter #648000:   Batch Loss = 11.241692, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.375931739807129, Accuracy = 0.8756219148635864
Training iter #650000:   Batch Loss = 11.304668, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.294971466064453, Accuracy = 0.9023631811141968
Training iter #652000:   Batch Loss = 11.036280, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.772134780883789, Accuracy = 0.8973880410194397
Training iter #654000:   Batch Loss = 10.668324, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.766478538513184, Accuracy = 0.8942785859107971
Training iter #656000:   Batch Loss = 10.191956, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.658563613891602, Accuracy = 0.9036069512367249
Training iter #658000:   Batch Loss = 10.567026, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.634603500366211, Accuracy = 0.9017412662506104
Training iter #660000:   Batch Loss = 9.918711, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.757068634033203, Accuracy = 0.8998756408691406
Training iter #662000:   Batch Loss = 10.333452, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.46713638305664, Accuracy = 0.9029850959777832
Training iter #664000:   Batch Loss = 9.971595, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.466094970703125, Accuracy = 0.9054726362228394
Training iter #666000:   Batch Loss = 10.010412, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.475775718688965, Accuracy = 0.9029850959777832
Training iter #668000:   Batch Loss = 10.104469, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.500361442565918, Accuracy = 0.8936567306518555
Training iter #670000:   Batch Loss = 9.975404, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.347648620605469, Accuracy = 0.9023631811141968
Training iter #672000:   Batch Loss = 9.939972, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.49402904510498, Accuracy = 0.8942785859107971
Training iter #674000:   Batch Loss = 10.210018, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.396949768066406, Accuracy = 0.89552241563797
Training iter #676000:   Batch Loss = 9.907199, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.442302703857422, Accuracy = 0.8949005007743835
Training iter #678000:   Batch Loss = 9.874722, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.27143669128418, Accuracy = 0.8973880410194397
Training iter #680000:   Batch Loss = 10.228689, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.379114151000977, Accuracy = 0.8961442708969116
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1700
Training iter #682000:   Batch Loss = 9.756587, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.380411148071289, Accuracy = 0.9060945510864258
Training iter #684000:   Batch Loss = 9.868688, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.20926284790039, Accuracy = 0.9011194109916687
Training iter #686000:   Batch Loss = 9.711727, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.0723295211792, Accuracy = 0.9060945510864258
Training iter #688000:   Batch Loss = 9.701296, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.254096031188965, Accuracy = 0.8949005007743835
Training iter #690000:   Batch Loss = 10.431802, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.078384399414062, Accuracy = 0.9023631811141968
Training iter #692000:   Batch Loss = 9.335817, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.908523559570312, Accuracy = 0.9042288661003113
Training iter #694000:   Batch Loss = 9.577089, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.02961540222168, Accuracy = 0.9073383212089539
Training iter #696000:   Batch Loss = 9.586660, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.055205345153809, Accuracy = 0.9004974961280823
Training iter #698000:   Batch Loss = 9.460194, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 10.968304634094238, Accuracy = 0.8961442708969116
Training iter #700000:   Batch Loss = 9.924276, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.226903915405273, Accuracy = 0.8998756408691406
Training iter #702000:   Batch Loss = 9.362177, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.268071174621582, Accuracy = 0.8936567306518555
Training iter #704000:   Batch Loss = 9.875125, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.08896255493164, Accuracy = 0.8973880410194397
Training iter #706000:   Batch Loss = 9.767496, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.922890663146973, Accuracy = 0.8998756408691406
Training iter #708000:   Batch Loss = 9.778400, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.088258743286133, Accuracy = 0.8861940503120422
Training iter #710000:   Batch Loss = 11.386230, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.491630554199219, Accuracy = 0.8936567306518555
Training iter #712000:   Batch Loss = 10.443520, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.678716659545898, Accuracy = 0.9042288661003113
Training iter #714000:   Batch Loss = 10.382680, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.662171363830566, Accuracy = 0.8936567306518555
Training iter #716000:   Batch Loss = 10.853416, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.293440818786621, Accuracy = 0.8805969953536987
Training iter #718000:   Batch Loss = 10.698270, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.500604629516602, Accuracy = 0.8893035054206848
Training iter #720000:   Batch Loss = 10.492273, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.0201997756958, Accuracy = 0.89552241563797
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1800
Training iter #722000:   Batch Loss = 10.237331, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.81728458404541, Accuracy = 0.891791045665741
Training iter #724000:   Batch Loss = 10.578547, Accuracy = 0.9175000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.59961986541748, Accuracy = 0.893034815788269
Training iter #726000:   Batch Loss = 10.315643, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.434735298156738, Accuracy = 0.9060945510864258
Training iter #728000:   Batch Loss = 10.455421, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.574495315551758, Accuracy = 0.9036069512367249
Training iter #730000:   Batch Loss = 9.829172, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.418298721313477, Accuracy = 0.8973880410194397
Training iter #732000:   Batch Loss = 9.479649, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.236722946166992, Accuracy = 0.9060945510864258
Training iter #734000:   Batch Loss = 9.690690, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.149551391601562, Accuracy = 0.9085820913314819
Training iter #736000:   Batch Loss = 9.906052, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.35060977935791, Accuracy = 0.9036069512367249
Training iter #738000:   Batch Loss = 9.915792, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.316978454589844, Accuracy = 0.9004974961280823
Training iter #740000:   Batch Loss = 10.049390, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.740277290344238, Accuracy = 0.9029850959777832
Training iter #742000:   Batch Loss = 11.141980, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.636375427246094, Accuracy = 0.8781094551086426
Training iter #744000:   Batch Loss = 10.776273, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.884848594665527, Accuracy = 0.8799751400947571
Training iter #746000:   Batch Loss = 10.719832, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.358430862426758, Accuracy = 0.8644278645515442
Training iter #748000:   Batch Loss = 11.245068, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 12.162693977355957, Accuracy = 0.8781094551086426
Training iter #750000:   Batch Loss = 10.700992, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.157926559448242, Accuracy = 0.8718905448913574
Training iter #752000:   Batch Loss = 10.684172, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 12.116250991821289, Accuracy = 0.8855721354484558
Training iter #754000:   Batch Loss = 10.864128, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.777737617492676, Accuracy = 0.8849502205848694
Training iter #756000:   Batch Loss = 10.232607, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.911404609680176, Accuracy = 0.8706467747688293
Training iter #758000:   Batch Loss = 10.159033, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.675971984863281, Accuracy = 0.8737562298774719
Training iter #760000:   Batch Loss = 10.175342, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.550840377807617, Accuracy = 0.8911691308021545
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-1900
Training iter #762000:   Batch Loss = 9.966473, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.427560806274414, Accuracy = 0.893034815788269
Training iter #764000:   Batch Loss = 10.598406, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.178914070129395, Accuracy = 0.8837064504623413
Training iter #766000:   Batch Loss = 10.703852, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.092833518981934, Accuracy = 0.8812189102172852
Training iter #768000:   Batch Loss = 10.536485, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.140785217285156, Accuracy = 0.8837064504623413
Training iter #770000:   Batch Loss = 11.057870, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.82763957977295, Accuracy = 0.8998756408691406
Training iter #772000:   Batch Loss = 10.439171, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.021559715270996, Accuracy = 0.8911691308021545
Training iter #774000:   Batch Loss = 10.415420, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.802353858947754, Accuracy = 0.888059675693512
Training iter #776000:   Batch Loss = 9.972315, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.956328392028809, Accuracy = 0.8905472755432129
Training iter #778000:   Batch Loss = 11.313103, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 12.592494010925293, Accuracy = 0.875
Training iter #780000:   Batch Loss = 11.332747, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.095465660095215, Accuracy = 0.8849502205848694
Training iter #782000:   Batch Loss = 11.469466, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 12.074609756469727, Accuracy = 0.8899253606796265
Training iter #784000:   Batch Loss = 10.601455, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.8794527053833, Accuracy = 0.8868159055709839
Training iter #786000:   Batch Loss = 10.887913, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.812178611755371, Accuracy = 0.8874378204345703
Training iter #788000:   Batch Loss = 10.280201, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.734333992004395, Accuracy = 0.8830845952033997
Training iter #790000:   Batch Loss = 10.592888, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.669808387756348, Accuracy = 0.8942785859107971
Training iter #792000:   Batch Loss = 10.554690, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.57766342163086, Accuracy = 0.8949005007743835
Training iter #794000:   Batch Loss = 10.241631, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.489888191223145, Accuracy = 0.8973880410194397
Training iter #796000:   Batch Loss = 10.109301, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.884201049804688, Accuracy = 0.8911691308021545
Training iter #798000:   Batch Loss = 10.088755, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.75804615020752, Accuracy = 0.8768656849861145
Training iter #800000:   Batch Loss = 10.896684, Accuracy = 0.9125000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.976055145263672, Accuracy = 0.8830845952033997
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2000
Training iter #802000:   Batch Loss = 10.311174, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.629018783569336, Accuracy = 0.8924129605293274
Training iter #804000:   Batch Loss = 10.256907, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.596234321594238, Accuracy = 0.891791045665741
Training iter #806000:   Batch Loss = 9.508621, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.340646743774414, Accuracy = 0.896766185760498
Training iter #808000:   Batch Loss = 9.911705, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.299454689025879, Accuracy = 0.8986318111419678
Training iter #810000:   Batch Loss = 9.842641, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.267809867858887, Accuracy = 0.8924129605293274
Training iter #812000:   Batch Loss = 9.429265, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.255970001220703, Accuracy = 0.896766185760498
Training iter #814000:   Batch Loss = 9.832970, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.236577987670898, Accuracy = 0.8855721354484558
Training iter #816000:   Batch Loss = 9.416395, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.25004768371582, Accuracy = 0.893034815788269
Training iter #818000:   Batch Loss = 10.144291, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.416414260864258, Accuracy = 0.8998756408691406
Training iter #820000:   Batch Loss = 9.945328, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.232025146484375, Accuracy = 0.8986318111419678
Training iter #822000:   Batch Loss = 9.903530, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.017742156982422, Accuracy = 0.9004974961280823
Training iter #824000:   Batch Loss = 10.141111, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.272781372070312, Accuracy = 0.8905472755432129
Training iter #826000:   Batch Loss = 9.708174, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.246253967285156, Accuracy = 0.8874378204345703
Training iter #828000:   Batch Loss = 10.391334, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.657445907592773, Accuracy = 0.8843283653259277
Training iter #830000:   Batch Loss = 10.341864, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.62393569946289, Accuracy = 0.8980099558830261
Training iter #832000:   Batch Loss = 9.986722, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.681602478027344, Accuracy = 0.893034815788269
Training iter #834000:   Batch Loss = 10.275116, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.631206512451172, Accuracy = 0.8973880410194397
Training iter #836000:   Batch Loss = 10.583298, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.490951538085938, Accuracy = 0.9023631811141968
Training iter #838000:   Batch Loss = 10.477948, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.60586166381836, Accuracy = 0.8998756408691406
Training iter #840000:   Batch Loss = 9.659393, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.392740249633789, Accuracy = 0.9004974961280823
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2100
Training iter #842000:   Batch Loss = 10.104315, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.304147720336914, Accuracy = 0.9004974961280823
Training iter #844000:   Batch Loss = 10.476040, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.168546676635742, Accuracy = 0.89552241563797
Training iter #846000:   Batch Loss = 9.986800, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.259628295898438, Accuracy = 0.896766185760498
Training iter #848000:   Batch Loss = 10.047276, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.102823257446289, Accuracy = 0.8986318111419678
Training iter #850000:   Batch Loss = 9.639259, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 11.093554496765137, Accuracy = 0.896766185760498
Training iter #852000:   Batch Loss = 10.559624, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.66236686706543, Accuracy = 0.8998756408691406
Training iter #854000:   Batch Loss = 9.610615, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.234102249145508, Accuracy = 0.8936567306518555
Training iter #856000:   Batch Loss = 10.316698, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.2229642868042, Accuracy = 0.8942785859107971
Training iter #858000:   Batch Loss = 10.216898, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.288127899169922, Accuracy = 0.8899253606796265
Training iter #860000:   Batch Loss = 9.777806, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.07788372039795, Accuracy = 0.8992537260055542
Training iter #862000:   Batch Loss = 9.915560, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.105903625488281, Accuracy = 0.8936567306518555
Training iter #864000:   Batch Loss = 9.390882, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.375082969665527, Accuracy = 0.893034815788269
Training iter #866000:   Batch Loss = 10.614904, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.893572807312012, Accuracy = 0.871268630027771
Training iter #868000:   Batch Loss = 10.043437, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.575981140136719, Accuracy = 0.8861940503120422
Training iter #870000:   Batch Loss = 10.438925, Accuracy = 0.9150000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.430499076843262, Accuracy = 0.8849502205848694
Training iter #872000:   Batch Loss = 10.171226, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.693750381469727, Accuracy = 0.8793532252311707
Training iter #874000:   Batch Loss = 10.205454, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.40053653717041, Accuracy = 0.8911691308021545
Training iter #876000:   Batch Loss = 10.108392, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.17049503326416, Accuracy = 0.8893035054206848
Training iter #878000:   Batch Loss = 9.550064, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.55557632446289, Accuracy = 0.8768656849861145
Training iter #880000:   Batch Loss = 9.905405, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.269134521484375, Accuracy = 0.8973880410194397
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2200
Training iter #882000:   Batch Loss = 10.058662, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.327703475952148, Accuracy = 0.888059675693512
Training iter #884000:   Batch Loss = 10.174150, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.168113708496094, Accuracy = 0.8942785859107971
Training iter #886000:   Batch Loss = 9.969461, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.354180335998535, Accuracy = 0.8886815905570984
Training iter #888000:   Batch Loss = 10.209089, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.107087135314941, Accuracy = 0.8949005007743835
Training iter #890000:   Batch Loss = 9.705848, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.257902145385742, Accuracy = 0.8949005007743835
Training iter #892000:   Batch Loss = 9.319208, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.284404754638672, Accuracy = 0.893034815788269
Training iter #894000:   Batch Loss = 9.710472, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.042717933654785, Accuracy = 0.9011194109916687
Training iter #896000:   Batch Loss = 9.541298, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.867952346801758, Accuracy = 0.9029850959777832
Training iter #898000:   Batch Loss = 8.916224, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.814604759216309, Accuracy = 0.8986318111419678
Training iter #900000:   Batch Loss = 9.856030, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.04394817352295, Accuracy = 0.8936567306518555
Training iter #902000:   Batch Loss = 9.498296, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.90203857421875, Accuracy = 0.9004974961280823
Training iter #904000:   Batch Loss = 9.713728, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.963215827941895, Accuracy = 0.9017412662506104
Training iter #906000:   Batch Loss = 9.631486, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.895774841308594, Accuracy = 0.9004974961280823
Training iter #908000:   Batch Loss = 10.056787, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.462385177612305, Accuracy = 0.8961442708969116
Training iter #910000:   Batch Loss = 10.057188, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.334444046020508, Accuracy = 0.9017412662506104
Training iter #912000:   Batch Loss = 10.020937, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.320484161376953, Accuracy = 0.8924129605293274
Training iter #914000:   Batch Loss = 9.651964, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.09823989868164, Accuracy = 0.9023631811141968
Training iter #916000:   Batch Loss = 9.664148, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.234417915344238, Accuracy = 0.8874378204345703
Training iter #918000:   Batch Loss = 11.269438, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 10.98615837097168, Accuracy = 0.89552241563797
Training iter #920000:   Batch Loss = 9.927021, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.098030090332031, Accuracy = 0.8911691308021545
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2300
Training iter #922000:   Batch Loss = 9.443404, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.932550430297852, Accuracy = 0.89552241563797
Training iter #924000:   Batch Loss = 9.541767, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.14647102355957, Accuracy = 0.8986318111419678
Training iter #926000:   Batch Loss = 9.741440, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.290580749511719, Accuracy = 0.8980099558830261
Training iter #928000:   Batch Loss = 10.224047, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.208306312561035, Accuracy = 0.893034815788269
Training iter #930000:   Batch Loss = 9.499283, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.912215232849121, Accuracy = 0.9017412662506104
Training iter #932000:   Batch Loss = 9.161007, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.006742477416992, Accuracy = 0.8992537260055542
Training iter #934000:   Batch Loss = 9.680517, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.100921630859375, Accuracy = 0.9029850959777832
Training iter #936000:   Batch Loss = 9.543079, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.167184829711914, Accuracy = 0.9023631811141968
Training iter #938000:   Batch Loss = 9.768349, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.060722351074219, Accuracy = 0.9036069512367249
Training iter #940000:   Batch Loss = 9.671087, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.158866882324219, Accuracy = 0.9042288661003113
Training iter #942000:   Batch Loss = 9.292320, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.927260398864746, Accuracy = 0.9073383212089539
Training iter #944000:   Batch Loss = 8.927681, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.819849967956543, Accuracy = 0.9054726362228394
Training iter #946000:   Batch Loss = 9.616316, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.82490062713623, Accuracy = 0.9092040061950684
Training iter #948000:   Batch Loss = 9.139412, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.869819641113281, Accuracy = 0.9004974961280823
Training iter #950000:   Batch Loss = 9.200111, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.77775764465332, Accuracy = 0.9042288661003113
Training iter #952000:   Batch Loss = 8.941783, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 10.663421630859375, Accuracy = 0.9104477763175964
Training iter #954000:   Batch Loss = 9.309196, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.884307861328125, Accuracy = 0.9004974961280823
Training iter #956000:   Batch Loss = 8.919991, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.703008651733398, Accuracy = 0.9017412662506104
Training iter #958000:   Batch Loss = 8.899364, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.753576278686523, Accuracy = 0.9060945510864258
Training iter #960000:   Batch Loss = 9.474051, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.686312675476074, Accuracy = 0.9048507213592529
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2400
Training iter #962000:   Batch Loss = 9.148718, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.727799415588379, Accuracy = 0.9017412662506104
Training iter #964000:   Batch Loss = 9.120800, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.719684600830078, Accuracy = 0.9092040061950684
Training iter #966000:   Batch Loss = 9.141717, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.920150756835938, Accuracy = 0.9054726362228394
Training iter #968000:   Batch Loss = 9.520219, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.919140815734863, Accuracy = 0.8998756408691406
Training iter #970000:   Batch Loss = 9.375788, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.936702728271484, Accuracy = 0.9048507213592529
Training iter #972000:   Batch Loss = 9.354338, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.4757661819458, Accuracy = 0.9060945510864258
Training iter #974000:   Batch Loss = 10.222406, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.112737655639648, Accuracy = 0.8980099558830261
Training iter #976000:   Batch Loss = 10.785501, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.04788589477539, Accuracy = 0.8936567306518555
Training iter #978000:   Batch Loss = 10.603555, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.74434757232666, Accuracy = 0.891791045665741
Training iter #980000:   Batch Loss = 10.201649, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.008176803588867, Accuracy = 0.8961442708969116
Training iter #982000:   Batch Loss = 10.479805, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.813055992126465, Accuracy = 0.8949005007743835
Training iter #984000:   Batch Loss = 10.398720, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.699957847595215, Accuracy = 0.8949005007743835
Training iter #986000:   Batch Loss = 8.994570, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.634872436523438, Accuracy = 0.8998756408691406
Training iter #988000:   Batch Loss = 9.893668, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.721142768859863, Accuracy = 0.9036069512367249
Training iter #990000:   Batch Loss = 10.181815, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.71728801727295, Accuracy = 0.9011194109916687
Training iter #992000:   Batch Loss = 10.509383, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.687786102294922, Accuracy = 0.9011194109916687
Training iter #994000:   Batch Loss = 10.041751, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.770018577575684, Accuracy = 0.8949005007743835
Training iter #996000:   Batch Loss = 10.254979, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.543548583984375, Accuracy = 0.893034815788269
Training iter #998000:   Batch Loss = 9.993976, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.387913703918457, Accuracy = 0.8949005007743835
Training iter #1000000:   Batch Loss = 9.673900, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.366182327270508, Accuracy = 0.8855721354484558
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2500
Training iter #1002000:   Batch Loss = 9.938044, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.178367614746094, Accuracy = 0.9017412662506104
Training iter #1004000:   Batch Loss = 9.590749, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.032977104187012, Accuracy = 0.89552241563797
Training iter #1006000:   Batch Loss = 9.341098, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.042745590209961, Accuracy = 0.8998756408691406
Training iter #1008000:   Batch Loss = 9.582167, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.264973640441895, Accuracy = 0.896766185760498
Training iter #1010000:   Batch Loss = 9.613360, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.23559284210205, Accuracy = 0.8992537260055542
Training iter #1012000:   Batch Loss = 9.474157, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.023409843444824, Accuracy = 0.9011194109916687
Training iter #1014000:   Batch Loss = 10.179741, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 12.018653869628906, Accuracy = 0.8942785859107971
Training iter #1016000:   Batch Loss = 10.972452, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.255614280700684, Accuracy = 0.8905472755432129
Training iter #1018000:   Batch Loss = 10.142738, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.793914794921875, Accuracy = 0.8837064504623413
Training iter #1020000:   Batch Loss = 10.666998, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 12.267827033996582, Accuracy = 0.8886815905570984
Training iter #1022000:   Batch Loss = 10.066403, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.880727767944336, Accuracy = 0.8986318111419678
Training iter #1024000:   Batch Loss = 10.718807, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.893963813781738, Accuracy = 0.9011194109916687
Training iter #1026000:   Batch Loss = 9.962262, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.56588363647461, Accuracy = 0.9110696315765381
Training iter #1028000:   Batch Loss = 9.976716, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.567863464355469, Accuracy = 0.8936567306518555
Training iter #1030000:   Batch Loss = 9.953325, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.413373947143555, Accuracy = 0.896766185760498
Training iter #1032000:   Batch Loss = 9.549918, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.23716926574707, Accuracy = 0.9011194109916687
Training iter #1034000:   Batch Loss = 9.676078, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.209053039550781, Accuracy = 0.9042288661003113
Training iter #1036000:   Batch Loss = 9.565140, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.174224853515625, Accuracy = 0.9092040061950684
Training iter #1038000:   Batch Loss = 10.341380, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.115324020385742, Accuracy = 0.9054726362228394
Training iter #1040000:   Batch Loss = 9.391793, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.027615547180176, Accuracy = 0.90982586145401
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2600
Training iter #1042000:   Batch Loss = 9.835821, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.24471664428711, Accuracy = 0.8986318111419678
Training iter #1044000:   Batch Loss = 9.803169, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.333606719970703, Accuracy = 0.8905472755432129
Training iter #1046000:   Batch Loss = 10.323069, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.922005653381348, Accuracy = 0.8861940503120422
Training iter #1048000:   Batch Loss = 9.854068, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.791984558105469, Accuracy = 0.891791045665741
Training iter #1050000:   Batch Loss = 10.011095, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.650398254394531, Accuracy = 0.896766185760498
Training iter #1052000:   Batch Loss = 9.716444, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.421185493469238, Accuracy = 0.8973880410194397
Training iter #1054000:   Batch Loss = 9.612049, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.277527809143066, Accuracy = 0.8998756408691406
Training iter #1056000:   Batch Loss = 9.355572, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.270990371704102, Accuracy = 0.8992537260055542
Training iter #1058000:   Batch Loss = 9.454941, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.429315567016602, Accuracy = 0.896766185760498
Training iter #1060000:   Batch Loss = 9.220411, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.265134811401367, Accuracy = 0.9017412662506104
Training iter #1062000:   Batch Loss = 9.928500, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.431180953979492, Accuracy = 0.9004974961280823
Training iter #1064000:   Batch Loss = 9.736959, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.379572868347168, Accuracy = 0.9011194109916687
Training iter #1066000:   Batch Loss = 9.287766, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.194000244140625, Accuracy = 0.9036069512367249
Training iter #1068000:   Batch Loss = 9.326060, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.19180679321289, Accuracy = 0.9029850959777832
Training iter #1070000:   Batch Loss = 9.572321, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.264665603637695, Accuracy = 0.90982586145401
Training iter #1072000:   Batch Loss = 9.529363, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.419815063476562, Accuracy = 0.9004974961280823
Training iter #1074000:   Batch Loss = 9.530479, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.316853523254395, Accuracy = 0.893034815788269
Training iter #1076000:   Batch Loss = 10.196662, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.740487098693848, Accuracy = 0.8961442708969116
Training iter #1078000:   Batch Loss = 9.969177, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.613350868225098, Accuracy = 0.8849502205848694
Training iter #1080000:   Batch Loss = 9.578115, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.416454315185547, Accuracy = 0.893034815788269
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2700
Training iter #1082000:   Batch Loss = 10.652762, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.091683387756348, Accuracy = 0.8700248599052429
Training iter #1084000:   Batch Loss = 10.513988, Accuracy = 0.9200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 12.033256530761719, Accuracy = 0.8824626803398132
Training iter #1086000:   Batch Loss = 9.680914, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.68273639678955, Accuracy = 0.9004974961280823
Training iter #1088000:   Batch Loss = 9.147204, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 11.576955795288086, Accuracy = 0.8986318111419678
Training iter #1090000:   Batch Loss = 9.712973, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.668416023254395, Accuracy = 0.8961442708969116
Training iter #1092000:   Batch Loss = 9.869641, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.4942626953125, Accuracy = 0.9054726362228394
Training iter #1094000:   Batch Loss = 9.533020, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.295769691467285, Accuracy = 0.8961442708969116
Training iter #1096000:   Batch Loss = 10.917888, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.147089958190918, Accuracy = 0.8961442708969116
Training iter #1098000:   Batch Loss = 10.570793, Accuracy = 0.925000011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.89272689819336, Accuracy = 0.8756219148635864
Training iter #1100000:   Batch Loss = 10.463761, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.591176986694336, Accuracy = 0.9042288661003113
Training iter #1102000:   Batch Loss = 9.852818, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.646804809570312, Accuracy = 0.8992537260055542
Training iter #1104000:   Batch Loss = 10.279142, Accuracy = 0.9275000095367432
PERFORMANCE ON TEST SET: Batch Loss = 11.591075897216797, Accuracy = 0.8980099558830261
Training iter #1106000:   Batch Loss = 9.790606, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.898560523986816, Accuracy = 0.8986318111419678
Training iter #1108000:   Batch Loss = 9.932150, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.542737007141113, Accuracy = 0.8949005007743835
Training iter #1110000:   Batch Loss = 10.296032, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.758584976196289, Accuracy = 0.8949005007743835
Training iter #1112000:   Batch Loss = 10.561742, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.564737319946289, Accuracy = 0.8949005007743835
Training iter #1114000:   Batch Loss = 9.930075, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.541333198547363, Accuracy = 0.8980099558830261
Training iter #1116000:   Batch Loss = 9.896024, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.411376953125, Accuracy = 0.8905472755432129
Training iter #1118000:   Batch Loss = 9.641058, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.330667495727539, Accuracy = 0.89552241563797
Training iter #1120000:   Batch Loss = 9.676037, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.278149604797363, Accuracy = 0.8861940503120422
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2800
Training iter #1122000:   Batch Loss = 10.581341, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.128597259521484, Accuracy = 0.8961442708969116
Training iter #1124000:   Batch Loss = 9.804486, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.284513473510742, Accuracy = 0.8924129605293274
Training iter #1126000:   Batch Loss = 9.866403, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.161057472229004, Accuracy = 0.8961442708969116
Training iter #1128000:   Batch Loss = 9.506312, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.922401428222656, Accuracy = 0.8961442708969116
Training iter #1130000:   Batch Loss = 9.837591, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.252592086791992, Accuracy = 0.9011194109916687
Training iter #1132000:   Batch Loss = 9.640751, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.126180648803711, Accuracy = 0.8942785859107971
Training iter #1134000:   Batch Loss = 9.892345, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.237367630004883, Accuracy = 0.888059675693512
Training iter #1136000:   Batch Loss = 9.480568, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.375751495361328, Accuracy = 0.8830845952033997
Training iter #1138000:   Batch Loss = 9.867270, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 11.244917869567871, Accuracy = 0.893034815788269
Training iter #1140000:   Batch Loss = 9.427328, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.25413703918457, Accuracy = 0.8886815905570984
Training iter #1142000:   Batch Loss = 9.445188, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.023868560791016, Accuracy = 0.8924129605293274
Training iter #1144000:   Batch Loss = 9.898006, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.16147232055664, Accuracy = 0.8905472755432129
Training iter #1146000:   Batch Loss = 9.705674, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.017305374145508, Accuracy = 0.9017412662506104
Training iter #1148000:   Batch Loss = 10.080275, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.155856132507324, Accuracy = 0.893034815788269
Training iter #1150000:   Batch Loss = 9.829641, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.376434326171875, Accuracy = 0.8973880410194397
Training iter #1152000:   Batch Loss = 9.711406, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.051361083984375, Accuracy = 0.8992537260055542
Training iter #1154000:   Batch Loss = 9.068291, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.90352725982666, Accuracy = 0.9042288661003113
Training iter #1156000:   Batch Loss = 9.797268, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 11.208335876464844, Accuracy = 0.9011194109916687
Training iter #1158000:   Batch Loss = 9.202196, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.865609169006348, Accuracy = 0.9048507213592529
Training iter #1160000:   Batch Loss = 9.376795, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.791789054870605, Accuracy = 0.9110696315765381
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-2900
Training iter #1162000:   Batch Loss = 9.335176, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.557623863220215, Accuracy = 0.9073383212089539
Training iter #1164000:   Batch Loss = 9.163435, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.9412841796875, Accuracy = 0.9067164063453674
Training iter #1166000:   Batch Loss = 9.472851, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.725013732910156, Accuracy = 0.9017412662506104
Training iter #1168000:   Batch Loss = 8.920691, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.709299087524414, Accuracy = 0.9060945510864258
Training iter #1170000:   Batch Loss = 9.313849, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.866012573242188, Accuracy = 0.9054726362228394
Training iter #1172000:   Batch Loss = 9.846847, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 10.929447174072266, Accuracy = 0.9036069512367249
Training iter #1174000:   Batch Loss = 9.266269, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.899103164672852, Accuracy = 0.8942785859107971
Training iter #1176000:   Batch Loss = 9.154970, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.77840805053711, Accuracy = 0.8992537260055542
Training iter #1178000:   Batch Loss = 10.203190, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.684215545654297, Accuracy = 0.8936567306518555
Training iter #1180000:   Batch Loss = 10.068633, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.643292427062988, Accuracy = 0.891791045665741
Training iter #1182000:   Batch Loss = 10.244062, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.503934860229492, Accuracy = 0.8980099558830261
Training iter #1184000:   Batch Loss = 9.704382, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.27649974822998, Accuracy = 0.9029850959777832
Training iter #1186000:   Batch Loss = 10.416468, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.144304275512695, Accuracy = 0.891791045665741
Training iter #1188000:   Batch Loss = 10.297743, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.93837833404541, Accuracy = 0.9036069512367249
Training iter #1190000:   Batch Loss = 9.753699, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.951738357543945, Accuracy = 0.8980099558830261
Training iter #1192000:   Batch Loss = 10.203498, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.841679573059082, Accuracy = 0.8973880410194397
Training iter #1194000:   Batch Loss = 9.921143, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.621660232543945, Accuracy = 0.8973880410194397
Training iter #1196000:   Batch Loss = 9.771181, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.495664596557617, Accuracy = 0.8961442708969116
Training iter #1198000:   Batch Loss = 9.696121, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.40915584564209, Accuracy = 0.8992537260055542
Training iter #1200000:   Batch Loss = 9.677643, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.185834884643555, Accuracy = 0.9079601764678955
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3000
Training iter #1202000:   Batch Loss = 9.131808, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.111114501953125, Accuracy = 0.90982586145401
Training iter #1204000:   Batch Loss = 9.337945, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.195597648620605, Accuracy = 0.9004974961280823
Training iter #1206000:   Batch Loss = 9.299664, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.1503324508667, Accuracy = 0.8986318111419678
Training iter #1208000:   Batch Loss = 9.254615, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.870269775390625, Accuracy = 0.9029850959777832
Training iter #1210000:   Batch Loss = 9.310668, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.946937561035156, Accuracy = 0.9004974961280823
Training iter #1212000:   Batch Loss = 9.074077, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.901098251342773, Accuracy = 0.9060945510864258
Training iter #1214000:   Batch Loss = 8.958740, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.755212783813477, Accuracy = 0.9067164063453674
Training iter #1216000:   Batch Loss = 9.398429, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.78088092803955, Accuracy = 0.9029850959777832
Training iter #1218000:   Batch Loss = 9.015119, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.01547622680664, Accuracy = 0.9004974961280823
Training iter #1220000:   Batch Loss = 8.766297, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.9732027053833, Accuracy = 0.9129353165626526
Training iter #1222000:   Batch Loss = 9.426830, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.868297576904297, Accuracy = 0.8992537260055542
Training iter #1224000:   Batch Loss = 8.457705, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 10.871971130371094, Accuracy = 0.9067164063453674
Training iter #1226000:   Batch Loss = 9.983263, Accuracy = 0.9350000023841858
PERFORMANCE ON TEST SET: Batch Loss = 10.865074157714844, Accuracy = 0.9036069512367249
Training iter #1228000:   Batch Loss = 9.266797, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.913980484008789, Accuracy = 0.89552241563797
Training iter #1230000:   Batch Loss = 9.283263, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 10.739145278930664, Accuracy = 0.9017412662506104
Training iter #1232000:   Batch Loss = 9.133791, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 10.732301712036133, Accuracy = 0.9042288661003113
Training iter #1234000:   Batch Loss = 9.093456, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.765432357788086, Accuracy = 0.9029850959777832
Training iter #1236000:   Batch Loss = 8.414204, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.496728897094727, Accuracy = 0.9116915464401245
Training iter #1238000:   Batch Loss = 9.196650, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.031403541564941, Accuracy = 0.9029850959777832
Training iter #1240000:   Batch Loss = 9.269592, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.000341415405273, Accuracy = 0.9048507213592529
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3100
Training iter #1242000:   Batch Loss = 9.266028, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.866219520568848, Accuracy = 0.9092040061950684
Training iter #1244000:   Batch Loss = 9.106446, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.791878700256348, Accuracy = 0.9060945510864258
Training iter #1246000:   Batch Loss = 9.556029, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.869368553161621, Accuracy = 0.8992537260055542
Training iter #1248000:   Batch Loss = 9.092343, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.688172340393066, Accuracy = 0.913557231426239
Training iter #1250000:   Batch Loss = 8.595613, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.582000732421875, Accuracy = 0.9123134613037109
Training iter #1252000:   Batch Loss = 9.534326, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.960880279541016, Accuracy = 0.9116915464401245
Training iter #1254000:   Batch Loss = 9.356058, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.222881317138672, Accuracy = 0.9011194109916687
Training iter #1256000:   Batch Loss = 9.660657, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.096071243286133, Accuracy = 0.8992537260055542
Training iter #1258000:   Batch Loss = 10.117014, Accuracy = 0.8666666746139526
PERFORMANCE ON TEST SET: Batch Loss = 10.882804870605469, Accuracy = 0.8986318111419678
Training iter #1260000:   Batch Loss = 9.252029, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.793861389160156, Accuracy = 0.8942785859107971
Training iter #1262000:   Batch Loss = 9.244691, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.725198745727539, Accuracy = 0.9060945510864258
Training iter #1264000:   Batch Loss = 9.476198, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.926767349243164, Accuracy = 0.9104477763175964
Training iter #1266000:   Batch Loss = 9.475883, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.084172248840332, Accuracy = 0.9023631811141968
Training iter #1268000:   Batch Loss = 9.403277, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.167214393615723, Accuracy = 0.9092040061950684
Training iter #1270000:   Batch Loss = 9.627056, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.691961288452148, Accuracy = 0.9073383212089539
Training iter #1272000:   Batch Loss = 8.830725, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.977880477905273, Accuracy = 0.8998756408691406
Training iter #1274000:   Batch Loss = 9.062866, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.795952796936035, Accuracy = 0.9054726362228394
Training iter #1276000:   Batch Loss = 8.993147, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.52496337890625, Accuracy = 0.9148010015487671
Training iter #1278000:   Batch Loss = 8.868022, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.685294151306152, Accuracy = 0.9141790866851807
Training iter #1280000:   Batch Loss = 8.901368, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.717981338500977, Accuracy = 0.9104477763175964
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3200
Training iter #1282000:   Batch Loss = 9.307366, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.90298080444336, Accuracy = 0.9054726362228394
Training iter #1284000:   Batch Loss = 9.494789, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.878511428833008, Accuracy = 0.9004974961280823
Training iter #1286000:   Batch Loss = 8.992800, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.83175277709961, Accuracy = 0.9029850959777832
Training iter #1288000:   Batch Loss = 8.564426, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.637523651123047, Accuracy = 0.9054726362228394
Training iter #1290000:   Batch Loss = 9.350247, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.529813766479492, Accuracy = 0.9067164063453674
Training iter #1292000:   Batch Loss = 9.374094, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 10.665925979614258, Accuracy = 0.8980099558830261
Training iter #1294000:   Batch Loss = 8.989965, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.467809677124023, Accuracy = 0.90982586145401
Training iter #1296000:   Batch Loss = 8.880136, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.386842727661133, Accuracy = 0.9123134613037109
Training iter #1298000:   Batch Loss = 8.538136, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.42852783203125, Accuracy = 0.9092040061950684
Training iter #1300000:   Batch Loss = 8.597075, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.555745124816895, Accuracy = 0.9203979969024658
Training iter #1302000:   Batch Loss = 8.971849, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.484079360961914, Accuracy = 0.9116915464401245
Training iter #1304000:   Batch Loss = 8.846544, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.359854698181152, Accuracy = 0.9172885417938232
Training iter #1306000:   Batch Loss = 8.738354, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.666330337524414, Accuracy = 0.9023631811141968
Training iter #1308000:   Batch Loss = 8.461308, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.492335319519043, Accuracy = 0.9067164063453674
Training iter #1310000:   Batch Loss = 8.648539, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 10.431878089904785, Accuracy = 0.9129353165626526
Training iter #1312000:   Batch Loss = 8.824074, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.443964004516602, Accuracy = 0.9042288661003113
Training iter #1314000:   Batch Loss = 8.954772, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.891336441040039, Accuracy = 0.9004974961280823
Training iter #1316000:   Batch Loss = 8.858983, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.68254566192627, Accuracy = 0.89552241563797
Training iter #1318000:   Batch Loss = 8.459496, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.611364364624023, Accuracy = 0.8986318111419678
Training iter #1320000:   Batch Loss = 9.074590, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.5499849319458, Accuracy = 0.891791045665741
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3300
Training iter #1322000:   Batch Loss = 8.752982, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.538119316101074, Accuracy = 0.9029850959777832
Training iter #1324000:   Batch Loss = 8.766092, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.47700309753418, Accuracy = 0.9029850959777832
Training iter #1326000:   Batch Loss = 7.637546, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 10.633503913879395, Accuracy = 0.8992537260055542
Training iter #1328000:   Batch Loss = 10.737657, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.582771301269531, Accuracy = 0.8799751400947571
Training iter #1330000:   Batch Loss = 10.663004, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.14992618560791, Accuracy = 0.8992537260055542
Training iter #1332000:   Batch Loss = 10.358156, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.790138244628906, Accuracy = 0.9042288661003113
Training iter #1334000:   Batch Loss = 9.912579, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.74128246307373, Accuracy = 0.9110696315765381
Training iter #1336000:   Batch Loss = 9.645349, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.682012557983398, Accuracy = 0.9054726362228394
Training iter #1338000:   Batch Loss = 9.794142, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.56765365600586, Accuracy = 0.9054726362228394
Training iter #1340000:   Batch Loss = 10.157852, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.814517974853516, Accuracy = 0.9004974961280823
Training iter #1342000:   Batch Loss = 9.990748, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.807304382324219, Accuracy = 0.9060945510864258
Training iter #1344000:   Batch Loss = 9.645624, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.82479476928711, Accuracy = 0.8998756408691406
Training iter #1346000:   Batch Loss = 10.176376, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.64841079711914, Accuracy = 0.9029850959777832
Training iter #1348000:   Batch Loss = 9.813181, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.586973190307617, Accuracy = 0.9004974961280823
Training iter #1350000:   Batch Loss = 9.946484, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.436838150024414, Accuracy = 0.9048507213592529
Training iter #1352000:   Batch Loss = 9.783340, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.499030113220215, Accuracy = 0.8992537260055542
Training iter #1354000:   Batch Loss = 9.685406, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.362961769104004, Accuracy = 0.9036069512367249
Training iter #1356000:   Batch Loss = 9.569616, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.318440437316895, Accuracy = 0.8973880410194397
Training iter #1358000:   Batch Loss = 9.726763, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.233328819274902, Accuracy = 0.9073383212089539
Training iter #1360000:   Batch Loss = 9.557124, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.237787246704102, Accuracy = 0.9054726362228394
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3400
Training iter #1362000:   Batch Loss = 9.270947, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.265998840332031, Accuracy = 0.8961442708969116
Training iter #1364000:   Batch Loss = 9.662453, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.11120891571045, Accuracy = 0.9067164063453674
Training iter #1366000:   Batch Loss = 9.608619, Accuracy = 0.9925000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.649374008178711, Accuracy = 0.9123134613037109
Training iter #1368000:   Batch Loss = 10.736467, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.913481712341309, Accuracy = 0.9011194109916687
Training iter #1370000:   Batch Loss = 10.504380, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.059850692749023, Accuracy = 0.9017412662506104
Training iter #1372000:   Batch Loss = 10.420825, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.661280632019043, Accuracy = 0.8986318111419678
Training iter #1374000:   Batch Loss = 10.267475, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.78406810760498, Accuracy = 0.896766185760498
Training iter #1376000:   Batch Loss = 10.537192, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.642590522766113, Accuracy = 0.8992537260055542
Training iter #1378000:   Batch Loss = 10.363785, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.409673690795898, Accuracy = 0.9073383212089539
Training iter #1380000:   Batch Loss = 10.025288, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.29702377319336, Accuracy = 0.9110696315765381
Training iter #1382000:   Batch Loss = 10.501976, Accuracy = 0.9225000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.59389591217041, Accuracy = 0.8980099558830261
Training iter #1384000:   Batch Loss = 9.663650, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.347173690795898, Accuracy = 0.9067164063453674
Training iter #1386000:   Batch Loss = 10.500169, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.293895721435547, Accuracy = 0.9073383212089539
Training iter #1388000:   Batch Loss = 10.227412, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.301603317260742, Accuracy = 0.9017412662506104
Training iter #1390000:   Batch Loss = 9.713699, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.387948989868164, Accuracy = 0.9036069512367249
Training iter #1392000:   Batch Loss = 9.932930, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.203638076782227, Accuracy = 0.9104477763175964
Training iter #1394000:   Batch Loss = 8.773971, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 11.337547302246094, Accuracy = 0.9085820913314819
Training iter #1396000:   Batch Loss = 10.081297, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.421249389648438, Accuracy = 0.9042288661003113
Training iter #1398000:   Batch Loss = 9.958677, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.445239067077637, Accuracy = 0.8980099558830261
Training iter #1400000:   Batch Loss = 9.739056, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.13949966430664, Accuracy = 0.9092040061950684
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3500
Training iter #1402000:   Batch Loss = 10.136363, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.179526329040527, Accuracy = 0.9029850959777832
Training iter #1404000:   Batch Loss = 9.842836, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.19981861114502, Accuracy = 0.9011194109916687
Training iter #1406000:   Batch Loss = 9.161225, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.040377616882324, Accuracy = 0.9079601764678955
Training iter #1408000:   Batch Loss = 9.674231, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.66468334197998, Accuracy = 0.8986318111419678
Training iter #1410000:   Batch Loss = 9.997816, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.18587589263916, Accuracy = 0.9004974961280823
Training iter #1412000:   Batch Loss = 9.960995, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.110834121704102, Accuracy = 0.8973880410194397
Training iter #1414000:   Batch Loss = 9.630335, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.937101364135742, Accuracy = 0.9073383212089539
Training iter #1416000:   Batch Loss = 9.737383, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.071830749511719, Accuracy = 0.90982586145401
Training iter #1418000:   Batch Loss = 9.130243, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.86631965637207, Accuracy = 0.9060945510864258
Training iter #1420000:   Batch Loss = 9.930120, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.080848693847656, Accuracy = 0.9073383212089539
Training iter #1422000:   Batch Loss = 9.974701, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.93606185913086, Accuracy = 0.9085820913314819
Training iter #1424000:   Batch Loss = 9.214783, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.248170852661133, Accuracy = 0.9011194109916687
Training iter #1426000:   Batch Loss = 10.225804, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 11.110519409179688, Accuracy = 0.9092040061950684
Training iter #1428000:   Batch Loss = 12.011353, Accuracy = 0.8888888955116272
PERFORMANCE ON TEST SET: Batch Loss = 11.832815170288086, Accuracy = 0.9048507213592529
Training iter #1430000:   Batch Loss = 10.120715, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.576451301574707, Accuracy = 0.9160447716712952
Training iter #1432000:   Batch Loss = 10.115835, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.522151947021484, Accuracy = 0.9048507213592529
Training iter #1434000:   Batch Loss = 9.485830, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.317512512207031, Accuracy = 0.9141790866851807
Training iter #1436000:   Batch Loss = 9.752844, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.293157577514648, Accuracy = 0.9104477763175964
Training iter #1438000:   Batch Loss = 9.565298, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.372075080871582, Accuracy = 0.9092040061950684
Training iter #1440000:   Batch Loss = 9.692434, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.169839859008789, Accuracy = 0.9116915464401245
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3600
Training iter #1442000:   Batch Loss = 9.731222, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.465256690979004, Accuracy = 0.9042288661003113
Training iter #1444000:   Batch Loss = 9.528540, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.386253356933594, Accuracy = 0.9073383212089539
Training iter #1446000:   Batch Loss = 10.458792, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 12.058778762817383, Accuracy = 0.9054726362228394
Training iter #1448000:   Batch Loss = 10.362892, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.744453430175781, Accuracy = 0.9079601764678955
Training iter #1450000:   Batch Loss = 10.384159, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.860626220703125, Accuracy = 0.9123134613037109
Training iter #1452000:   Batch Loss = 10.048247, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.691139221191406, Accuracy = 0.90982586145401
Training iter #1454000:   Batch Loss = 9.755424, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.566275596618652, Accuracy = 0.9048507213592529
Training iter #1456000:   Batch Loss = 9.209407, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.442886352539062, Accuracy = 0.9054726362228394
Training iter #1458000:   Batch Loss = 10.023443, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.452006340026855, Accuracy = 0.9079601764678955
Training iter #1460000:   Batch Loss = 10.053273, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.66635513305664, Accuracy = 0.9129353165626526
Training iter #1462000:   Batch Loss = 11.038774, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.8439302444458, Accuracy = 0.9011194109916687
Training iter #1464000:   Batch Loss = 10.606353, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.734922409057617, Accuracy = 0.8986318111419678
Training iter #1466000:   Batch Loss = 10.134976, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.575004577636719, Accuracy = 0.9054726362228394
Training iter #1468000:   Batch Loss = 9.875664, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.419332504272461, Accuracy = 0.9004974961280823
Training iter #1470000:   Batch Loss = 9.701286, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.59518814086914, Accuracy = 0.9073383212089539
Training iter #1472000:   Batch Loss = 10.354166, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.727073669433594, Accuracy = 0.9017412662506104
Training iter #1474000:   Batch Loss = 10.912252, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.55810546875, Accuracy = 0.9116915464401245
Training iter #1476000:   Batch Loss = 10.800832, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.597869873046875, Accuracy = 0.9060945510864258
Training iter #1478000:   Batch Loss = 10.588491, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.162389755249023, Accuracy = 0.9048507213592529
Training iter #1480000:   Batch Loss = 10.417606, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 12.041900634765625, Accuracy = 0.9017412662506104
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3700
Training iter #1482000:   Batch Loss = 10.331226, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.770750999450684, Accuracy = 0.9048507213592529
Training iter #1484000:   Batch Loss = 10.238431, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.785547256469727, Accuracy = 0.9042288661003113
Training iter #1486000:   Batch Loss = 9.694035, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.703287124633789, Accuracy = 0.9092040061950684
Training iter #1488000:   Batch Loss = 10.048617, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.565505027770996, Accuracy = 0.9123134613037109
Training iter #1490000:   Batch Loss = 9.696657, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.716094017028809, Accuracy = 0.90982586145401
Training iter #1492000:   Batch Loss = 10.003855, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.69828987121582, Accuracy = 0.9092040061950684
Training iter #1494000:   Batch Loss = 10.231080, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.54902458190918, Accuracy = 0.9191542267799377
Training iter #1496000:   Batch Loss = 9.799674, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.736519813537598, Accuracy = 0.9042288661003113
Training iter #1498000:   Batch Loss = 10.261147, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.823246955871582, Accuracy = 0.9104477763175964
Training iter #1500000:   Batch Loss = 9.918737, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.57668399810791, Accuracy = 0.9141790866851807
Training iter #1502000:   Batch Loss = 9.968646, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.525556564331055, Accuracy = 0.9110696315765381
Training iter #1504000:   Batch Loss = 10.174866, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.741509437561035, Accuracy = 0.9154228568077087
Training iter #1506000:   Batch Loss = 9.921874, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.652352333068848, Accuracy = 0.9085820913314819
Training iter #1508000:   Batch Loss = 10.702123, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 12.993385314941406, Accuracy = 0.9141790866851807
Training iter #1510000:   Batch Loss = 11.358799, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 13.186260223388672, Accuracy = 0.9011194109916687
Training iter #1512000:   Batch Loss = 11.970350, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 13.58010196685791, Accuracy = 0.9023631811141968
Training iter #1514000:   Batch Loss = 12.008677, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 13.146645545959473, Accuracy = 0.9048507213592529
Training iter #1516000:   Batch Loss = 11.118245, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.875601768493652, Accuracy = 0.9036069512367249
Training iter #1518000:   Batch Loss = 10.979038, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.861406326293945, Accuracy = 0.9067164063453674
Training iter #1520000:   Batch Loss = 11.583106, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 13.3302583694458, Accuracy = 0.9079601764678955
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3800
Training iter #1522000:   Batch Loss = 11.572454, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 12.931380271911621, Accuracy = 0.9123134613037109
Training iter #1524000:   Batch Loss = 11.535188, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 13.108622550964355, Accuracy = 0.9116915464401245
Training iter #1526000:   Batch Loss = 11.375687, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.872047424316406, Accuracy = 0.9067164063453674
Training iter #1528000:   Batch Loss = 11.524957, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 13.06419563293457, Accuracy = 0.9011194109916687
Training iter #1530000:   Batch Loss = 11.818403, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 12.953939437866211, Accuracy = 0.8973880410194397
Training iter #1532000:   Batch Loss = 11.663541, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 12.88636589050293, Accuracy = 0.8986318111419678
Training iter #1534000:   Batch Loss = 11.470673, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.769689559936523, Accuracy = 0.8973880410194397
Training iter #1536000:   Batch Loss = 11.237817, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.569153785705566, Accuracy = 0.8949005007743835
Training iter #1538000:   Batch Loss = 11.280966, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 12.68002700805664, Accuracy = 0.8986318111419678
Training iter #1540000:   Batch Loss = 11.260984, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.76230239868164, Accuracy = 0.8942785859107971
Training iter #1542000:   Batch Loss = 11.110857, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 12.439265251159668, Accuracy = 0.9110696315765381
Training iter #1544000:   Batch Loss = 11.039605, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.516563415527344, Accuracy = 0.9067164063453674
Training iter #1546000:   Batch Loss = 10.951681, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 12.383016586303711, Accuracy = 0.9073383212089539
Training iter #1548000:   Batch Loss = 10.934345, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 12.328502655029297, Accuracy = 0.9017412662506104
Training iter #1550000:   Batch Loss = 11.785109, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 13.3147611618042, Accuracy = 0.9004974961280823
Training iter #1552000:   Batch Loss = 11.747962, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.785323143005371, Accuracy = 0.9079601764678955
Training iter #1554000:   Batch Loss = 11.559210, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.724499702453613, Accuracy = 0.9073383212089539
Training iter #1556000:   Batch Loss = 11.075581, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.475214958190918, Accuracy = 0.9079601764678955
Training iter #1558000:   Batch Loss = 11.440086, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 12.47806167602539, Accuracy = 0.90982586145401
Training iter #1560000:   Batch Loss = 11.096972, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.306427001953125, Accuracy = 0.90982586145401
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-3900
Training iter #1562000:   Batch Loss = 10.714035, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.236649513244629, Accuracy = 0.9191542267799377
Training iter #1564000:   Batch Loss = 10.465112, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 12.142871856689453, Accuracy = 0.9148010015487671
Training iter #1566000:   Batch Loss = 10.549664, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.27132511138916, Accuracy = 0.9123134613037109
Training iter #1568000:   Batch Loss = 10.488735, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.170263290405273, Accuracy = 0.9073383212089539
Training iter #1570000:   Batch Loss = 10.627140, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.042821884155273, Accuracy = 0.9185323119163513
Training iter #1572000:   Batch Loss = 10.656100, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.142489433288574, Accuracy = 0.9048507213592529
Training iter #1574000:   Batch Loss = 10.760905, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 12.096940994262695, Accuracy = 0.9029850959777832
Training iter #1576000:   Batch Loss = 10.455348, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.933913230895996, Accuracy = 0.9160447716712952
Training iter #1578000:   Batch Loss = 10.313667, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.052423477172852, Accuracy = 0.9141790866851807
Training iter #1580000:   Batch Loss = 10.527159, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.982009887695312, Accuracy = 0.9141790866851807
Training iter #1582000:   Batch Loss = 10.170797, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.789785385131836, Accuracy = 0.90982586145401
Training iter #1584000:   Batch Loss = 10.458735, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.77861213684082, Accuracy = 0.9110696315765381
Training iter #1586000:   Batch Loss = 10.112876, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.888258934020996, Accuracy = 0.9073383212089539
Training iter #1588000:   Batch Loss = 9.865639, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.858896255493164, Accuracy = 0.9172885417938232
Training iter #1590000:   Batch Loss = 10.269251, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.616893768310547, Accuracy = 0.9253731369972229
Training iter #1592000:   Batch Loss = 10.426868, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 12.013984680175781, Accuracy = 0.9129353165626526
Training iter #1594000:   Batch Loss = 10.367732, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.909310340881348, Accuracy = 0.9067164063453674
Training iter #1596000:   Batch Loss = 10.379772, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.766894340515137, Accuracy = 0.9104477763175964
Training iter #1598000:   Batch Loss = 9.389159, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.572919845581055, Accuracy = 0.9179104566574097
Training iter #1600000:   Batch Loss = 9.801090, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.635721206665039, Accuracy = 0.9110696315765381
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4000
Training iter #1602000:   Batch Loss = 10.120270, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.66391658782959, Accuracy = 0.9141790866851807
Training iter #1604000:   Batch Loss = 9.635997, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.488974571228027, Accuracy = 0.9092040061950684
Training iter #1606000:   Batch Loss = 9.784676, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.53294849395752, Accuracy = 0.9141790866851807
Training iter #1608000:   Batch Loss = 9.861040, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.352091789245605, Accuracy = 0.9123134613037109
Training iter #1610000:   Batch Loss = 9.575633, Accuracy = 0.9825000166893005
PERFORMANCE ON TEST SET: Batch Loss = 11.398571014404297, Accuracy = 0.9129353165626526
Training iter #1612000:   Batch Loss = 9.446677, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.549772262573242, Accuracy = 0.9203979969024658
Training iter #1614000:   Batch Loss = 9.756377, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.435173034667969, Accuracy = 0.9179104566574097
Training iter #1616000:   Batch Loss = 9.600407, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.424810409545898, Accuracy = 0.9166666865348816
Training iter #1618000:   Batch Loss = 9.372084, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.321588516235352, Accuracy = 0.9191542267799377
Training iter #1620000:   Batch Loss = 9.391304, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.36361312866211, Accuracy = 0.9197761416435242
Training iter #1622000:   Batch Loss = 9.500923, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.358855247497559, Accuracy = 0.9197761416435242
Training iter #1624000:   Batch Loss = 9.638841, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.377542495727539, Accuracy = 0.913557231426239
Training iter #1626000:   Batch Loss = 9.658213, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.522682189941406, Accuracy = 0.9067164063453674
Training iter #1628000:   Batch Loss = 9.655275, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.41739273071289, Accuracy = 0.9166666865348816
Training iter #1630000:   Batch Loss = 9.300838, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 11.226275444030762, Accuracy = 0.9073383212089539
Training iter #1632000:   Batch Loss = 9.618415, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 11.313060760498047, Accuracy = 0.9216417670249939
Training iter #1634000:   Batch Loss = 10.374728, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.95551872253418, Accuracy = 0.9123134613037109
Training iter #1636000:   Batch Loss = 10.181906, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.900056838989258, Accuracy = 0.9067164063453674
Training iter #1638000:   Batch Loss = 10.388621, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.8080472946167, Accuracy = 0.9110696315765381
Training iter #1640000:   Batch Loss = 10.034563, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.00019359588623, Accuracy = 0.8998756408691406
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4100
Training iter #1642000:   Batch Loss = 10.302673, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.8019380569458, Accuracy = 0.9141790866851807
Training iter #1644000:   Batch Loss = 10.136108, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.623405456542969, Accuracy = 0.9203979969024658
Training iter #1646000:   Batch Loss = 9.895719, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.763908386230469, Accuracy = 0.9036069512367249
Training iter #1648000:   Batch Loss = 9.922503, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.512090682983398, Accuracy = 0.9060945510864258
Training iter #1650000:   Batch Loss = 9.705598, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.427267074584961, Accuracy = 0.9191542267799377
Training iter #1652000:   Batch Loss = 10.080702, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.537332534790039, Accuracy = 0.9104477763175964
Training iter #1654000:   Batch Loss = 10.832088, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 12.246723175048828, Accuracy = 0.9085820913314819
Training iter #1656000:   Batch Loss = 10.114328, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 12.030741691589355, Accuracy = 0.9110696315765381
Training iter #1658000:   Batch Loss = 10.207405, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 12.0313138961792, Accuracy = 0.9154228568077087
Training iter #1660000:   Batch Loss = 10.597636, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 12.207608222961426, Accuracy = 0.9085820913314819
Training iter #1662000:   Batch Loss = 10.145898, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.797239303588867, Accuracy = 0.913557231426239
Training iter #1664000:   Batch Loss = 9.966660, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.64984130859375, Accuracy = 0.9123134613037109
Training iter #1666000:   Batch Loss = 11.872946, Accuracy = 0.9111111164093018
PERFORMANCE ON TEST SET: Batch Loss = 11.474738121032715, Accuracy = 0.9141790866851807
Training iter #1668000:   Batch Loss = 10.099926, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.58396053314209, Accuracy = 0.90982586145401
Training iter #1670000:   Batch Loss = 9.673340, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.469161033630371, Accuracy = 0.9160447716712952
Training iter #1672000:   Batch Loss = 9.727478, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.531529426574707, Accuracy = 0.9092040061950684
Training iter #1674000:   Batch Loss = 9.529904, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.434196472167969, Accuracy = 0.9054726362228394
Training iter #1676000:   Batch Loss = 10.091919, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.343278884887695, Accuracy = 0.913557231426239
Training iter #1678000:   Batch Loss = 9.618760, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.163110733032227, Accuracy = 0.9185323119163513
Training iter #1680000:   Batch Loss = 9.599648, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 11.613924980163574, Accuracy = 0.9054726362228394
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4200
Training iter #1682000:   Batch Loss = 9.946980, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.626659393310547, Accuracy = 0.9129353165626526
Training iter #1684000:   Batch Loss = 10.235719, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.485610008239746, Accuracy = 0.9154228568077087
Training iter #1686000:   Batch Loss = 9.878204, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.169625282287598, Accuracy = 0.9085820913314819
Training iter #1688000:   Batch Loss = 9.333103, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.136556625366211, Accuracy = 0.9141790866851807
Training iter #1690000:   Batch Loss = 9.579714, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.259590148925781, Accuracy = 0.9172885417938232
Training iter #1692000:   Batch Loss = 9.849384, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.2403564453125, Accuracy = 0.9191542267799377
Training iter #1694000:   Batch Loss = 9.127458, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.200681686401367, Accuracy = 0.9191542267799377
Training iter #1696000:   Batch Loss = 9.557446, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.00401782989502, Accuracy = 0.9235074520111084
Training iter #1698000:   Batch Loss = 9.198947, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.039986610412598, Accuracy = 0.9160447716712952
Training iter #1700000:   Batch Loss = 9.817715, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.217324256896973, Accuracy = 0.9060945510864258
Training iter #1702000:   Batch Loss = 9.363587, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.178258895874023, Accuracy = 0.9042288661003113
Training iter #1704000:   Batch Loss = 9.248831, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.060254096984863, Accuracy = 0.9116915464401245
Training iter #1706000:   Batch Loss = 9.458369, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.179523468017578, Accuracy = 0.9104477763175964
Training iter #1708000:   Batch Loss = 9.836216, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.502943992614746, Accuracy = 0.8936567306518555
Training iter #1710000:   Batch Loss = 9.689765, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.266571998596191, Accuracy = 0.9160447716712952
Training iter #1712000:   Batch Loss = 9.269590, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.102603912353516, Accuracy = 0.9141790866851807
Training iter #1714000:   Batch Loss = 9.382352, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.241470336914062, Accuracy = 0.90982586145401
Training iter #1716000:   Batch Loss = 9.049013, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.133463859558105, Accuracy = 0.9104477763175964
Training iter #1718000:   Batch Loss = 9.514556, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.69251823425293, Accuracy = 0.9079601764678955
Training iter #1720000:   Batch Loss = 9.544989, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.407735824584961, Accuracy = 0.9123134613037109
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4300
Training iter #1722000:   Batch Loss = 10.185054, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 12.14051628112793, Accuracy = 0.9036069512367249
Training iter #1724000:   Batch Loss = 10.590824, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.83395004272461, Accuracy = 0.8961442708969116
Training iter #1726000:   Batch Loss = 10.325538, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.642536163330078, Accuracy = 0.9054726362228394
Training iter #1728000:   Batch Loss = 10.277476, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.77505111694336, Accuracy = 0.9042288661003113
Training iter #1730000:   Batch Loss = 10.618195, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.839941024780273, Accuracy = 0.9079601764678955
Training iter #1732000:   Batch Loss = 10.147943, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.652732849121094, Accuracy = 0.9110696315765381
Training iter #1734000:   Batch Loss = 10.217296, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 11.583888053894043, Accuracy = 0.9073383212089539
Training iter #1736000:   Batch Loss = 10.489956, Accuracy = 0.9325000047683716
PERFORMANCE ON TEST SET: Batch Loss = 11.77557373046875, Accuracy = 0.9079601764678955
Training iter #1738000:   Batch Loss = 10.552654, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.673480987548828, Accuracy = 0.90982586145401
Training iter #1740000:   Batch Loss = 9.979596, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.619681358337402, Accuracy = 0.8973880410194397
Training iter #1742000:   Batch Loss = 10.814254, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 12.11967658996582, Accuracy = 0.8998756408691406
Training iter #1744000:   Batch Loss = 10.470587, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.988388061523438, Accuracy = 0.8992537260055542
Training iter #1746000:   Batch Loss = 10.238869, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.728200912475586, Accuracy = 0.9017412662506104
Training iter #1748000:   Batch Loss = 10.433117, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.702640533447266, Accuracy = 0.9110696315765381
Training iter #1750000:   Batch Loss = 10.512304, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.65406608581543, Accuracy = 0.9004974961280823
Training iter #1752000:   Batch Loss = 10.807230, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.900324821472168, Accuracy = 0.9048507213592529
Training iter #1754000:   Batch Loss = 10.353006, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.752257347106934, Accuracy = 0.8980099558830261
Training iter #1756000:   Batch Loss = 10.430644, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.711587905883789, Accuracy = 0.8924129605293274
Training iter #1758000:   Batch Loss = 10.214425, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.607597351074219, Accuracy = 0.888059675693512
Training iter #1760000:   Batch Loss = 10.484989, Accuracy = 0.9375
PERFORMANCE ON TEST SET: Batch Loss = 11.915794372558594, Accuracy = 0.888059675693512
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4400
Training iter #1762000:   Batch Loss = 10.294175, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.68875789642334, Accuracy = 0.8911691308021545
Training iter #1764000:   Batch Loss = 9.872025, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.594830513000488, Accuracy = 0.8986318111419678
Training iter #1766000:   Batch Loss = 10.365284, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.460192680358887, Accuracy = 0.8992537260055542
Training iter #1768000:   Batch Loss = 10.971824, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 11.659904479980469, Accuracy = 0.9011194109916687
Training iter #1770000:   Batch Loss = 9.744185, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.733083724975586, Accuracy = 0.8992537260055542
Training iter #1772000:   Batch Loss = 10.115250, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.738180160522461, Accuracy = 0.9054726362228394
Training iter #1774000:   Batch Loss = 10.425448, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.612667083740234, Accuracy = 0.9017412662506104
Training iter #1776000:   Batch Loss = 9.629222, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.52297306060791, Accuracy = 0.8986318111419678
Training iter #1778000:   Batch Loss = 10.085344, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 11.42924690246582, Accuracy = 0.9011194109916687
Training iter #1780000:   Batch Loss = 9.927990, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.441825866699219, Accuracy = 0.8973880410194397
Training iter #1782000:   Batch Loss = 10.015389, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.219141006469727, Accuracy = 0.9073383212089539
Training iter #1784000:   Batch Loss = 9.679218, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.2622709274292, Accuracy = 0.8986318111419678
Training iter #1786000:   Batch Loss = 9.182509, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.065773963928223, Accuracy = 0.9110696315765381
Training iter #1788000:   Batch Loss = 9.234655, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.059621810913086, Accuracy = 0.9104477763175964
Training iter #1790000:   Batch Loss = 10.006168, Accuracy = 0.9449999928474426
PERFORMANCE ON TEST SET: Batch Loss = 11.238420486450195, Accuracy = 0.9104477763175964
Training iter #1792000:   Batch Loss = 9.947815, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 11.342187881469727, Accuracy = 0.9042288661003113
Training iter #1794000:   Batch Loss = 9.356032, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 11.310741424560547, Accuracy = 0.9148010015487671
Training iter #1796000:   Batch Loss = 9.779232, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.363178253173828, Accuracy = 0.9216417670249939
Training iter #1798000:   Batch Loss = 10.365144, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.35526180267334, Accuracy = 0.9123134613037109
Training iter #1800000:   Batch Loss = 10.309612, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.58745002746582, Accuracy = 0.9172885417938232
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4500
Training iter #1802000:   Batch Loss = 10.588931, Accuracy = 0.8888888955116272
PERFORMANCE ON TEST SET: Batch Loss = 11.258463859558105, Accuracy = 0.9154228568077087
Training iter #1804000:   Batch Loss = 9.568506, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.231644630432129, Accuracy = 0.9073383212089539
Training iter #1806000:   Batch Loss = 9.919098, Accuracy = 0.9424999952316284
PERFORMANCE ON TEST SET: Batch Loss = 11.148903846740723, Accuracy = 0.913557231426239
Training iter #1808000:   Batch Loss = 9.156738, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 11.009111404418945, Accuracy = 0.9073383212089539
Training iter #1810000:   Batch Loss = 9.260934, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.187278747558594, Accuracy = 0.9092040061950684
Training iter #1812000:   Batch Loss = 9.477607, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.198872566223145, Accuracy = 0.9116915464401245
Training iter #1814000:   Batch Loss = 9.306162, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 10.88180923461914, Accuracy = 0.9166666865348816
Training iter #1816000:   Batch Loss = 9.009135, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.86694622039795, Accuracy = 0.9185323119163513
Training iter #1818000:   Batch Loss = 8.895864, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.856038093566895, Accuracy = 0.9197761416435242
Training iter #1820000:   Batch Loss = 9.350895, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.815988540649414, Accuracy = 0.9141790866851807
Training iter #1822000:   Batch Loss = 8.956851, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.716048240661621, Accuracy = 0.913557231426239
Training iter #1824000:   Batch Loss = 8.772181, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.727527618408203, Accuracy = 0.9203979969024658
Training iter #1826000:   Batch Loss = 8.745422, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.651925086975098, Accuracy = 0.9253731369972229
Training iter #1828000:   Batch Loss = 9.337241, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.669059753417969, Accuracy = 0.9203979969024658
Training iter #1830000:   Batch Loss = 8.938527, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.658848762512207, Accuracy = 0.9210199117660522
Training iter #1832000:   Batch Loss = 9.106681, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.719314575195312, Accuracy = 0.9191542267799377
Training iter #1834000:   Batch Loss = 9.057575, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.625003814697266, Accuracy = 0.9172885417938232
Training iter #1836000:   Batch Loss = 8.463815, Accuracy = 0.9555555582046509
PERFORMANCE ON TEST SET: Batch Loss = 10.57579517364502, Accuracy = 0.9148010015487671
Training iter #1838000:   Batch Loss = 8.855343, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.662862777709961, Accuracy = 0.9092040061950684
Training iter #1840000:   Batch Loss = 9.326489, Accuracy = 0.949999988079071
PERFORMANCE ON TEST SET: Batch Loss = 10.729527473449707, Accuracy = 0.9185323119163513
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4600
Training iter #1842000:   Batch Loss = 8.997076, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.881193161010742, Accuracy = 0.927860677242279
Training iter #1844000:   Batch Loss = 9.494639, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 11.2755126953125, Accuracy = 0.9073383212089539
Training iter #1846000:   Batch Loss = 9.772226, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 11.040872573852539, Accuracy = 0.913557231426239
Training iter #1848000:   Batch Loss = 9.061323, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.947455406188965, Accuracy = 0.9154228568077087
Training iter #1850000:   Batch Loss = 9.044967, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.885932922363281, Accuracy = 0.913557231426239
Training iter #1852000:   Batch Loss = 8.808271, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 10.849048614501953, Accuracy = 0.9203979969024658
Training iter #1854000:   Batch Loss = 9.230543, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.797057151794434, Accuracy = 0.9116915464401245
Training iter #1856000:   Batch Loss = 9.110389, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.73973560333252, Accuracy = 0.9197761416435242
Training iter #1858000:   Batch Loss = 8.836817, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 10.972614288330078, Accuracy = 0.9216417670249939
Training iter #1860000:   Batch Loss = 8.852099, Accuracy = 0.9800000190734863
PERFORMANCE ON TEST SET: Batch Loss = 10.803592681884766, Accuracy = 0.9179104566574097
Training iter #1862000:   Batch Loss = 8.795775, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.617993354797363, Accuracy = 0.9191542267799377
Training iter #1864000:   Batch Loss = 9.259004, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.473310470581055, Accuracy = 0.9172885417938232
Training iter #1866000:   Batch Loss = 9.119183, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.828770637512207, Accuracy = 0.9197761416435242
Training iter #1868000:   Batch Loss = 9.059704, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 10.873359680175781, Accuracy = 0.9228855967521667
Training iter #1870000:   Batch Loss = 10.197584, Accuracy = 0.9333333373069763
PERFORMANCE ON TEST SET: Batch Loss = 10.61472225189209, Accuracy = 0.9172885417938232
Training iter #1872000:   Batch Loss = 9.623278, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 11.210957527160645, Accuracy = 0.9104477763175964
Training iter #1874000:   Batch Loss = 9.170448, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.907980918884277, Accuracy = 0.9154228568077087
Training iter #1876000:   Batch Loss = 8.955099, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.609375953674316, Accuracy = 0.9203979969024658
Training iter #1878000:   Batch Loss = 8.812228, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.62564754486084, Accuracy = 0.9222636818885803
Training iter #1880000:   Batch Loss = 8.784615, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.603466987609863, Accuracy = 0.9141790866851807
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4700
Training iter #1882000:   Batch Loss = 9.035988, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.640006065368652, Accuracy = 0.9210199117660522
Training iter #1884000:   Batch Loss = 8.696403, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.883042335510254, Accuracy = 0.9129353165626526
Training iter #1886000:   Batch Loss = 8.787884, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.760242462158203, Accuracy = 0.9241293668746948
Training iter #1888000:   Batch Loss = 8.330192, Accuracy = 0.9900000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.536284446716309, Accuracy = 0.9247512221336365
Training iter #1890000:   Batch Loss = 8.798504, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 10.64771556854248, Accuracy = 0.9166666865348816
Training iter #1892000:   Batch Loss = 8.878494, Accuracy = 0.9599999785423279
PERFORMANCE ON TEST SET: Batch Loss = 10.780555725097656, Accuracy = 0.9154228568077087
Training iter #1894000:   Batch Loss = 9.070541, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 10.641096115112305, Accuracy = 0.9247512221336365
Training iter #1896000:   Batch Loss = 8.837142, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.659794807434082, Accuracy = 0.9172885417938232
Training iter #1898000:   Batch Loss = 8.539911, Accuracy = 0.9900000095367432
PERFORMANCE ON TEST SET: Batch Loss = 10.559152603149414, Accuracy = 0.9222636818885803
Training iter #1900000:   Batch Loss = 8.852072, Accuracy = 0.9775000214576721
PERFORMANCE ON TEST SET: Batch Loss = 10.504806518554688, Accuracy = 0.9160447716712952
Training iter #1902000:   Batch Loss = 8.960874, Accuracy = 0.9725000262260437
PERFORMANCE ON TEST SET: Batch Loss = 10.545854568481445, Accuracy = 0.9160447716712952
Training iter #1904000:   Batch Loss = 8.417448, Accuracy = 0.9777777791023254
PERFORMANCE ON TEST SET: Batch Loss = 10.848437309265137, Accuracy = 0.9160447716712952
Training iter #1906000:   Batch Loss = 8.919675, Accuracy = 0.9474999904632568
PERFORMANCE ON TEST SET: Batch Loss = 10.624320983886719, Accuracy = 0.9129353165626526
Training iter #1908000:   Batch Loss = 8.713596, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 10.623419761657715, Accuracy = 0.9067164063453674
Training iter #1910000:   Batch Loss = 8.896102, Accuracy = 0.9524999856948853
PERFORMANCE ON TEST SET: Batch Loss = 10.516754150390625, Accuracy = 0.9110696315765381
Training iter #1912000:   Batch Loss = 9.011725, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 11.661870956420898, Accuracy = 0.9004974961280823
Training iter #1914000:   Batch Loss = 9.712048, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 11.693648338317871, Accuracy = 0.9179104566574097
Training iter #1916000:   Batch Loss = 9.389181, Accuracy = 0.987500011920929
PERFORMANCE ON TEST SET: Batch Loss = 11.459622383117676, Accuracy = 0.9110696315765381
Training iter #1918000:   Batch Loss = 9.355236, Accuracy = 0.9674999713897705
PERFORMANCE ON TEST SET: Batch Loss = 11.301586151123047, Accuracy = 0.9110696315765381
Training iter #1920000:   Batch Loss = 9.301296, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.21527099609375, Accuracy = 0.9104477763175964
Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-4800
Training iter #1922000:   Batch Loss = 9.809207, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.712075233459473, Accuracy = 0.9141790866851807
Training iter #1924000:   Batch Loss = 9.096138, Accuracy = 0.9750000238418579
PERFORMANCE ON TEST SET: Batch Loss = 11.245819091796875, Accuracy = 0.9210199117660522
Training iter #1926000:   Batch Loss = 9.824501, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 11.576981544494629, Accuracy = 0.9116915464401245
Training iter #1928000:   Batch Loss = 10.072738, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.521430969238281, Accuracy = 0.9141790866851807
Training iter #1930000:   Batch Loss = 10.071393, Accuracy = 0.9574999809265137
PERFORMANCE ON TEST SET: Batch Loss = 11.363645553588867, Accuracy = 0.9110696315765381
Training iter #1932000:   Batch Loss = 9.562222, Accuracy = 0.9624999761581421
PERFORMANCE ON TEST SET: Batch Loss = 11.476011276245117, Accuracy = 0.9110696315765381
Optimization Finished!
FINAL RESULT: Batch Loss = 11.61392593383789, Accuracy = 0.9048507213592529
All train time = 17102.12520813942
Final Model saved in file: ./lstm2/model_selftest_kfold2.ckpt-final
Precision: 90.59925342482543%
Recall: 90.48507462686567%
f1_score: 90.48797829928311%

Confusion Matrix:
[[156   0   0   0   0   4   0   0   1   0]
 [  2 149   0   0   1   0   1   1   7   0]
 [  0   0 133   0   0   2  11   1  13   0]
 [  2   0   0 150   4   0   0   5   0   0]
 [  2   0   0   0 158   0   0   0   0   0]
 [  1   0   1   0   2 155   0   0   4   0]
 [  3   0   1   5   0   0 136  15   1   0]
 [  7   0   2   7   0   0  13 131   0   1]
 [  0   0  20   1   0   0   0   2 134   0]
 [  7   2   0   1   0   0   0   0   0 153]]

Confusion matrix (normalised to % of total test data):
[[9.701492   0.         0.         0.         0.         0.24875621
  0.         0.         0.06218905 0.        ]
 [0.12437811 9.26617    0.         0.         0.06218905 0.
  0.06218905 0.06218905 0.43532336 0.        ]
 [0.         0.         8.271144   0.         0.         0.12437811
  0.68407965 0.06218905 0.80845773 0.        ]
 [0.12437811 0.         0.         9.328358   0.24875621 0.
  0.         0.31094527 0.         0.        ]
 [0.12437811 0.         0.         0.         9.8258705  0.
  0.         0.         0.         0.        ]
 [0.06218905 0.         0.06218905 0.         0.12437811 9.639303
  0.         0.         0.24875621 0.        ]
 [0.18656716 0.         0.06218905 0.31094527 0.         0.
  8.457711   0.9328358  0.06218905 0.        ]
 [0.43532336 0.         0.12437811 0.43532336 0.         0.
  0.80845773 8.146766   0.         0.06218905]
 [0.         0.         1.2437811  0.06218905 0.         0.
  0.         0.12437811 8.333334   0.        ]
 [0.43532336 0.12437811 0.         0.06218905 0.         0.
  0.         0.         0.         9.514926  ]]/home/sunrepe/anaconda3/lib/python3.7/site-packages/matplotlib/font_manager.py:1241: UserWarning: findfont: Font family ['Times New Roman'] not found. Falling back to DejaVu Sans.
  (prop.get_family(), self.defaultFamily[fontext]))

Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.
